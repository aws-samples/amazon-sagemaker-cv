{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMakerCV\n",
    "\n",
    "This notebook launches a SageMakerCV training job for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default region\n",
    "region = \"us-west-2\"\n",
    "os.environ['AWS_DEFAULT_REGION'] = region\n",
    "\n",
    "# Set a user ID - This is just used for naming your job, so can be anything you like.\n",
    "# The date_str is used for organizing your jobs in your S3 bucket\n",
    "# The time_str is used for keeping track of job names\n",
    "# The ecr_repo is the ECR repo that contains your SageMakerCV Docker image.\n",
    "# If you haven't created a SageMakerCV Docker image, see the instructions here.\n",
    "# The algo_name is the name of your Docker image in your ECR repo\n",
    "# The account_call is a subprocess command to get the AWS account associated with\n",
    "# your local AWS credentials. This is used to get the account asssociated with your\n",
    "# ECR repo.\n",
    "# instance_type is the type of sagemaker instance you want to use for training.\n",
    "# the config_file contains the model and training configuration in yaml format.\n",
    "# The s3_bucket is the bucket that contains your data, and will also be used for storing results\n",
    "user_id = \"jbsnyder\"\n",
    "date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "ecr_repo = \"jbsnyder\"\n",
    "algo_name = \"smcv-pt-1.8\"\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "nodes = 4\n",
    "config_file = \"configs/mrcnn_bs384_O4.yaml\"\n",
    "s3_bucket = \"s3://jbsnyder-sagemaker-pdx/\"\n",
    "\n",
    "account_call = f\"aws sts get-caller-identity --region {region} --endpoint-url https://sts.{region}.amazonaws.com --query Account --output text\"\n",
    "ecr_account = subprocess.check_output(account_call, shell=True).decode().strip()\n",
    "\n",
    "docker_image = \"{0}.dkr.ecr.{1}.amazonaws.com/{2}:{3}\".format(ecr_account,\n",
    "                                                              region,\n",
    "                                                              ecr_repo,\n",
    "                                                              algo_name)\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type_short = '-'.join(instance_type.split('.')[1:]).replace('large', '')\n",
    "processes_per_host = 8 if instance_type in ['ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p3.16xlarge'] \\\n",
    "                    else 4 if instance_type in ['ml.p3.8xlarge', 'ml.g4dn.12xlarge'] else 1\n",
    "config_info = config_file.split('/')[1].replace('.yaml', '').replace('_', '-')\n",
    "\n",
    "source_dir = \".\"\n",
    "\n",
    "if nodes>1 and instance_type in ['ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p3.16xlarge']:\n",
    "    distribution = { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } } \n",
    "    main_script = \"train.py\"\n",
    "else:\n",
    "    distribution = None\n",
    "    main_script = \"launch_torch.py\"\n",
    "\n",
    "job_name = f'{user_id}-{config_info}-{instance_type_short}-{time_str}'\n",
    "\n",
    "output_path = os.path.join(s3_bucket, \"sagemaker-output\", date_str, job_name)\n",
    "\n",
    "code_location = os.path.join(s3_bucket, \"sagemaker-code\", date_str, job_name)\n",
    "\n",
    "s3_data_dir = os.path.join(s3_bucket, \"data\")\n",
    "s3_coco_dir = \"coco/2017/archive/\"\n",
    "s3_weights_dir = \"weights/pytorch/resnet/\"\n",
    "\n",
    "channels = {\n",
    "    'coco': os.path.join(s3_data_dir, s3_coco_dir),\n",
    "    'weights': os.path.join(s3_data_dir, s3_weights_dir),\n",
    "}\n",
    "\n",
    "hyperparameters = {\"config\": config_file,\n",
    "                   \"unarchive\": '/opt/ml/input/data/coco/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "                entry_point=main_script, \n",
    "                source_dir=source_dir, \n",
    "                image_uri=docker_image, \n",
    "                role=role,\n",
    "                instance_count=nodes,\n",
    "                instance_type=instance_type,\n",
    "                distribution=distribution,\n",
    "                output_path=output_path,\n",
    "                checkpoint_s3_uri=output_path,\n",
    "                model_dir=output_path,\n",
    "                hyperparameters=hyperparameters,\n",
    "                volume_size=500,\n",
    "                disable_profiler=True,\n",
    "                debugger_hook_config=False,\n",
    "                code_location=code_location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(channels, wait=False, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
