{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download COCO dataset and weight, convert data to TFRecords and upload to S3\n",
    "# Guide for preparing data and weight https://github.com/HerringForks/DeepLearningExamples/tree/master/TensorFlow2/Segmentation/MaskRCNN#quick-start-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create FSx drive for your input data, which can save 3 mins of data download time at training start up and yield similar throughput\n",
    "# 1. Download and prepare your training dataset on S3.\n",
    "# 2. Follow the steps listed here to create a FSx linked with your S3 bucket with training data - https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-fs-linked-data-repo.html. Make sure to add an endpoint to your VPC allowing S3 access.\n",
    "# 3. Follow the steps listed here to configure your SageMaker training job to use FSx https://aws.amazon.com/blogs/machine-learning/speed-up-training-on-amazon-sagemaker-using-amazon-efs-or-amazon-fsx-for-lustre-file-systems/\n",
    "#\n",
    "# Important Caveats\n",
    "# 1. You need use the same subnet and vpc and security group used with FSx when launching the SageMaker notebook instance. The same configurations will be used by your SageMaker training job.\n",
    "# 2. Make sure you set appropriate inbound/output rules in the security group. Specically, opening up these ports is necessary for SageMaker to access the FSx filesystem in the training job. https://docs.aws.amazon.com/fsx/latest/LustreGuide/limit-access-security-groups.html\n",
    "# 3. Make sure SageMaker IAM Role used to launch this SageMaker training job has access to AmazonFSx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import FileSystemInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default region\n",
    "region = \"us-west-2\"\n",
    "os.environ['AWS_DEFAULT_REGION'] = region\n",
    "role = get_execution_role()\n",
    "\n",
    "user_id = \"johndoe\"\n",
    "time_str = datetime.now().strftime(\"%H-%M-%S\")\n",
    "\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "# launch single node training with total batch size of 64\n",
    "# parameters are only for demo purpose, user may need to tune to fit their workload\n",
    "config_file = \"configs/mrcnn_bs64.yaml\"\n",
    "hyperparameters = {\"config\": config_file}\n",
    "\n",
    "# Enable SMDDP\n",
    "distribution = {\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}}\n",
    "\n",
    "job_name = f'{user_id}-maskrcnn-keras-p4d-{instance_count}-{time_str}'\n",
    "\n",
    "source_dir = \".\"\n",
    "entry_point = \"train_keras.py\"\n",
    "\n",
    "# The Shared SMDDP Keras docker image\n",
    "docker_image = \"570106654206.dkr.ecr.us-west-2.amazonaws.com/smddp-keras-preview:tf251-maskrcnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnets = ['<subnet-id-of-fsx>']\n",
    "security_group_ids=['<security-group-id-of-fsx>']\n",
    "file_system_id = '<fsx-id>'\n",
    "file_system_directory_path = '/fsx/<path-to-dataset-under-imported-s3-bucket>'\n",
    "\n",
    "estimator = TensorFlow(\n",
    "            entry_point=entry_point,\n",
    "            role=role,\n",
    "            image_uri=docker_image,\n",
    "            source_dir=source_dir,\n",
    "            instance_count=instance_count,\n",
    "            instance_type=instance_type,\n",
    "            hyperparameters=hyperparameters,\n",
    "            subnets=subnets,\n",
    "            security_group_ids=security_group_ids,\n",
    "            disable_profiler=True,\n",
    "            debugger_hook_config=False,\n",
    "            distribution=distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fs = FileSystemInput(\n",
    "            file_system_id=file_system_id,\n",
    "            file_system_type='FSxLustre',\n",
    "            directory_path=file_system_directory_path,\n",
    "            file_system_access_mode='ro')\n",
    "\n",
    "data = {\"train\": train_fs}\n",
    "estimator.fit(inputs=data, job_name=job_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
