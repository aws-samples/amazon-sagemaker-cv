{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMakerCV TensorFlow Tutorial\n",
    "\n",
    "SageMakerCV is a collection of computer vision tools developed to take full advantage of Amazon SageMaker by providing state of the art model accuracy, training speed, and training cost reductions. SageMakerCV is based on the lessons we learned from developing the record breaking computer vision models we announced at Re:Invent in 2019 and 2020, along with talking to our customers and understanding the challenges they faced in training their own computer vision models.\n",
    "\n",
    "The tutorial in this notebook walks through using SageMakerCV to train Mask RCNN on the COCO dataset. The only prerequisite is to setup SageMaker studio, the instructions for which can be found in [Onboard to Amazon SageMaker Studio Using Quick Start](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html). Everything else, from getting the COCO data to launching a distributed training cluster, is included here.\n",
    "\n",
    "## Setup and Roadmap\n",
    "\n",
    "Before diving into the tutorial itself, let's take a minute to discuss the various tools we'll be using.\n",
    "\n",
    "#### SageMaker Studio\n",
    "[SageMaker Studio](https://aws.amazon.com/sagemaker/studio/) is a machine learning focused IDE where you can interactively develop models and launch SageMaker training jobs all in one place. SageMaker Studio provides a Jupyter Lab like environment, but with a number of enhancements. We'll just scratch the surface here. See the [SageMaker Studio Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) for more details.\n",
    "\n",
    "For our purposes, the biggest difference from regular Jupyter Lab is that SageMaker Studio allows you to change your compute resources as needed, by connecting notebooks to Docker containers on different ML instances. This is a little confusing to just describe, so let's walk through an example.\n",
    "\n",
    "Once you've completed the setup on [Onboard to Amazon SageMaker Studio Using Quick Start](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html), go to the [SageMaker Console](https://us-west-2.console.aws.amazon.com/sagemaker) and click `Open SageMaker Studio` near the top right of the page.\n",
    "\n",
    "<img src=\"../assets/SageMaker_console.png\" style=\"width: 600px\">\n",
    "\n",
    "If you haven't yet created a user, do so via the link at the top left of the page. Give it any name you like. For execution role, you can either use an existing SageMaker role, or create a new one. If you're unsure, create a new role. On the `Create IAM Role` window, make sure to select `Any S3 Bucket`. \n",
    "\n",
    "<img src=\"../assets/Create_IAM_role.png\" style=\"width: 600px\">\n",
    "\n",
    "Back on the SageMaker Studio page, select `Open Studio` next to the user you just created.\n",
    "\n",
    "<img src=\"../assets/Studio_domain.png\" style=\"width: 600px\">\n",
    "\n",
    "This will take a couple minutes to start up the first time. Once it starts, you'll have a Jupyter Lab like interface running on a small instance with an attached EBS volume. Let's start by taking a look at the `Launcher` tab.\n",
    "\n",
    "<img src=\"../assets/Studio_launcher.png\" style=\"width: 750px\">\n",
    "\n",
    "If you don't see the `Launcher`, you can bring one up by clicking the `+` on the menu bar in the upper left corner.\n",
    "\n",
    "<img src=\"../assets/Studio_menu_bar.png\" style=\"width: 600px\">\n",
    "\n",
    "The `Launcher` gives you access to all kinds of tools. This is where you can create new notebooks, text files, or get a terminal for your instance. Try the `System Terminal`. This gives you a new terminal tab for your Studio instance. It's useful for things like downloading data or cloning github repos into studio. For example, you can run `aws s3 ls` to browse your current S3 buckets. Go ahead and clone this repo onto Studio with \n",
    "\n",
    "`git clone https://github.com/aws-samples/amazon-sagemaker-cv`\n",
    "\n",
    "Let's look at the launcher one more time. Bring another one up with the `+`. Notice you have an option for `Select a SageMaker image` above the button to launch a notebook. This allows you to select a Docker image that will launch on a new instance. The notebook you create will be attached to that new instance, along with the EBS volume on your Studio instance. Let's try it out. On the `Launcher` page, click the drop down menu next to `Select a SageMaker Image` and select `TensorFlow 2.3 Python 3.7 (Optimzed for GPU)`, then click the `Notebook` button below the dropdown.\n",
    "\n",
    "<img src=\"../assets/Select_tensorflow_image.png\" style=\"width: 600px\">\n",
    "\n",
    "Take a look at the upper righthand corner of the notebook. \n",
    "\n",
    "<img src=\"../assets/notebook_tensorflow_kernel.png\" style=\"width: 600px\">\n",
    "\n",
    "The `Ptyhon 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)` refers to the kernel associated with this notebook. The `Unknown` refers to the current instance type. Click `Unknown` and select `ml.g4dn.xlarge`.\n",
    "\n",
    "<img src=\"../assets/instance_types.png\" style=\"width: 600px\">\n",
    "\n",
    "This will launch a `ml.g4dn.xlarge` instance and attach this notebook to it. This will take a couple of minutes, because Studio needs to download the PyTorch Docker image to the new instance. Once an instance has started, launching new notebooks with the same instance type and kernel is immediate. You'll also see the `Unknown` replaced with and instance description `4 vCPU + 16 GiB + 1 GPU`. You can also change instance as needed. Say you want to run your notebook on a `ml.p3dn.24xlarge` to get 8 GPUs. To change instances, just click the instance description. To get more instances in the menu, deselect `Fast launch only`.\n",
    "\n",
    "Once your notebook is up and running, you can also get a terminal into your new instance.\n",
    "\n",
    "<img src=\"../assets/Launch_terminal.png\" style=\"width: 600px\">\n",
    "\n",
    "This can be useful for customizing your image with setup scripts, pip installing new packages, or using mpi to launch multi GPU training jobs. Click to get a terminal and run `ls`. Note that you have the same directories as your main Studio instance. Studio will attach the same EBS volume to all the instances you start, so all your files and data are shared across any notebooks you start. This means that you can prototype a model on a single GPU instance, then switch to a multi GPU instance while still having access to all of your data and scripts.\n",
    "\n",
    "Finally, when you want to shut down instances, click the circle with a square in it on the left hand side.\n",
    "\n",
    "<img src=\"../assets/running_instances.png\" style=\"width: 600px\">\n",
    "\n",
    "This shows your current running instances, and the Docker containers attached to those instances. To shut them down, just click the power button to their right.\n",
    "\n",
    "Now that we've explored studio a bit, let's get started with SageMakerCV. If you followed the instructions above to clone the repo, you should have `amazon-sagemaker-cv` in the file browser on the left. Navigate to `amazon-sagemaker-cv/pytorch/tutorial.ipynb` to open this notebook on your instance. If you still have a `g4dn` running, it should automatically attach to it.\n",
    "\n",
    "The rest of this notebook is broken into 4 sections.\n",
    "\n",
    "- Installing SageMakerCV and Downloading the COCO Data\n",
    "\n",
    "Since we're using the base AWS Deep Learning Container image, we need to add the SageMakerCV tools. Then we'll download the COCO dataset and upload it to S3.\n",
    "\n",
    "- Prototyping in Studio\n",
    "\n",
    "We'll walk through how to train a model on Studio, how SageMakerCV is structured, and how you can add your own models and features.\n",
    "\n",
    "- Launching a SageMaker Training Job\n",
    "\n",
    "There's lots of bells and whistles available to train your models fast, an on large datasets. We'll put a lot of those together to launch a high performance training job. Specifically, we'll create a training job with 4 P4d.24xlarge instances connected with 400 GB EFA, and streaming our training data from S3, so we don't have to load the dataset onto the instances before training. You could even use this same configuration to train on a dataset that wouldn't fit on the instances. If you'd rather only launch a smaller (or larger) training cluster, we'll discuss how to modify configuration.\n",
    "\n",
    "- Testing Our Model\n",
    "\n",
    "Finally, we'll take the output trained Mask RCNN model and visualize its performance in Studio.\n",
    "\n",
    "#### Installing SageMakerCV\n",
    "\n",
    "To install SageMakerCV on the PyTorch Studio Docker, just run `pip install -e .` in the `amazon-sagemaker-cv/tensorflow` directory. You can do this with either an image terminal, or by running the paragraph below. Note that we use the `-e` option. This will keep the SageMakerCV modules editable, so any changes you make will be launched on your training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///root/blog_post/amazon-sagemaker-cv/tensorflow\n",
      "Collecting pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl\n",
      "  Using cached https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (0.14.0)\n",
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (4.4.0)\n",
      "Requirement already satisfied: yacs in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (0.1.8)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (3.4.3)\n",
      "Requirement already satisfied: mpi4py in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (3.0.3)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (4.3.0.36)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/site-packages (from pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl->sagemakercv==0.1) (0.29.24)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/site-packages (from pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl->sagemakercv==0.1) (2.6.2)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/site-packages (from pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl->sagemakercv==0.1) (56.0.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (1.18.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->sagemakercv==0.1) (1.15.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/site-packages (from tensorflow_addons->sagemakercv==0.1) (2.13.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (3.15.8)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (4.62.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (20.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (2.24.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (1.1.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.3.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.18.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.10.0)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (2.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (3.7.4.3)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (5.4.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (1.4.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/site-packages (from importlib-resources->tensorflow_datasets->sagemakercv==0.1) (3.4.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow_datasets->sagemakercv==0.1) (1.53.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/site-packages (from yacs->sagemakercv==0.1) (5.4.1)\n",
      "Installing collected packages: sagemakercv\n",
      "  Attempting uninstall: sagemakercv\n",
      "    Found existing installation: sagemakercv 0.1\n",
      "    Uninstalling sagemakercv-0.1:\n",
      "      Successfully uninstalled sagemakercv-0.1\n",
      "  Running setup.py develop for sagemakercv\n",
      "Successfully installed sagemakercv-0.1\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Setup on S3 and Download COCO data\n",
    "\n",
    "Next we need to setup an S3 bucket for all our data and results. Enter a name for your S3 bucket below. You can either create a new bucket, or use an existing bucket. If you use an existing bucket, make sure it's in the same region where you plan to run training. For new buckets, we'll specify that it needs to be in the current SageMaker region. By default we'll put everything in an S3 location on your bucket named `smcv-tutorial`, and locally in `/root/smcv-tutorial`, but you can change these locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 'sagemaker-smcv-tutorial' # Don't include s3:// in your bucket name\n",
    "S3_DIR = 'smcv-tensorflow-tutorial'\n",
    "LOCAL_DATA_DIR = '/root/smcv-tensorflow-tutorial' # For reasons detailed in Distributed Training, do not put this dir in the SageMakerCV dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from s3fs import S3FileSystem\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import boto3\n",
    "from botocore.client import ClientError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Bucket sagemaker-smcv-tutorial Exists\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "# Check if bucket exists. If it doesn't, create it.\n",
    "\n",
    "try:\n",
    "    bucket = s3.meta.client.head_bucket(Bucket=S3_BUCKET)\n",
    "    print(f\"S3 Bucket {S3_BUCKET} Exists\")\n",
    "except ClientError:\n",
    "    print(f\"Creating Bucket {S3_BUCKET}\")\n",
    "    bucket = s3.create_bucket(Bucket=S3_BUCKET, CreateBucketConfiguration={'LocationConstraint': region})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Next we'll download the COCO data to Studio, unzip the files, create TFRecords, and upload to S3. The reason we want the data in two places is that it's convenient to have the data locally on Studio for prototyping. We also want to unarchive the data before moving it to S3 so that we can stream it to our training instances instead of downloading it all at once.\n",
    "\n",
    "Once this is finished, you'll have copies of the COCO data on your Studio instance, and in S3. Be careful not to open the `data/coco/train2017` dir in the Studio file browser. It contains 118287 images, and can cause your web browser to crash. If you need to browse these files, use the terminal.\n",
    "\n",
    "This only needs to be done once, and only if you don't already have the data. The COCO 2017 dataset is about 20GB, so this step takes around 30 minutes to complete. The next paragraph sets up all the file directories we'll use for downloading, and later in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_URL=\"http://images.cocodataset.org\"\n",
    "ANNOTATIONS_ZIP=\"annotations_trainval2017.zip\"\n",
    "TRAIN_ZIP=\"train2017.zip\"\n",
    "VAL_ZIP=\"val2017.zip\"\n",
    "COCO_DIR=os.path.join(LOCAL_DATA_DIR, 'data', 'coco')\n",
    "TF_RECORD_DIR=os.path.join(LOCAL_DATA_DIR, 'data', 'coco', 'tfrecord')\n",
    "os.makedirs(COCO_DIR, exist_ok=True)\n",
    "os.makedirs(TF_RECORD_DIR, exist_ok=True)\n",
    "S3_DATA_LOCATION=os.path.join(\"s3://\", S3_BUCKET, S3_DIR, \"data\", \"coco\")\n",
    "S3_WEIGHTS_LOCATION=os.path.join(\"s3://\", S3_BUCKET, S3_DIR, \"data\", \"weights\", \"resnet\")\n",
    "WEIGHTS_DIR=os.path.join(LOCAL_DATA_DIR, 'data', 'weights')\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "R50_WEIGHTS_SRC=\"https://sagemakercv.s3.us-west-2.amazonaws.com/weights/tensorflow\"\n",
    "R50_WEIGHTS_TAR=\"tensorflow_resnet50.tar\"\n",
    "R50_WEIGHTS=\"tensorflow_resnet50\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "This paragraph will download everything. It takes around 30 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading annotations\n",
      "--2021-11-11 00:51:07--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.248.220\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.248.220|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252907541 (241M) [application/zip]\n",
      "Saving to: ‘/root/smcv-tensorflow-tutorial/data/coco/annotations_trainval2017.zip’\n",
      "\n",
      "/root/smcv-tensorfl 100%[===================>] 241.19M  25.3MB/s    in 8.8s    \n",
      "\n",
      "2021-11-11 00:51:16 (27.3 MB/s) - ‘/root/smcv-tensorflow-tutorial/data/coco/annotations_trainval2017.zip’ saved [252907541/252907541]\n",
      "\n",
      "Archive:  /root/smcv-tensorflow-tutorial/data/coco/annotations_trainval2017.zip\n",
      "  inflating: /root/smcv-tensorflow-tutorial/data/coco/annotations/instances_train2017.json  \n",
      "  inflating: /root/smcv-tensorflow-tutorial/data/coco/annotations/instances_val2017.json  \n",
      "  inflating: /root/smcv-tensorflow-tutorial/data/coco/annotations/captions_train2017.json  \n",
      "  inflating: /root/smcv-tensorflow-tutorial/data/coco/annotations/captions_val2017.json  \n",
      "  inflating: /root/smcv-tensorflow-tutorial/data/coco/annotations/person_keypoints_train2017.json  \n",
      "  inflating: /root/smcv-tensorflow-tutorial/data/coco/annotations/person_keypoints_val2017.json  \n",
      "upload: ../../../smcv-tensorflow-tutorial/data/coco/annotations/captions_val2017.json to s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/data/coco/annotations/captions_val2017.json\n",
      "upload: ../../../smcv-tensorflow-tutorial/data/coco/annotations/person_keypoints_val2017.json to s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/data/coco/annotations/person_keypoints_val2017.json\n",
      "upload: ../../../smcv-tensorflow-tutorial/data/coco/annotations/instances_val2017.json to s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/data/coco/annotations/instances_val2017.json\n",
      "upload: ../../../smcv-tensorflow-tutorial/data/coco/annotations/captions_train2017.json to s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/data/coco/annotations/captions_train2017.json\n",
      "upload: ../../../smcv-tensorflow-tutorial/data/coco/annotations/person_keypoints_train2017.json to s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/data/coco/annotations/person_keypoints_train2017.json\n",
      "upload: ../../../smcv-tensorflow-tutorial/data/coco/annotations/instances_train2017.json to s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/data/coco/annotations/instances_train2017.json\n",
      "Downloading COCO training data\n",
      "--2021-11-11 00:51:39--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.194.41\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.194.41|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘/root/smcv-tensorflow-tutorial/data/coco/train2017.zip’\n",
      "\n",
      "/root/smcv-tensorfl 100%[===================>]  18.01G  18.0MB/s    in 12m 17s \n",
      "\n",
      "2021-11-11 01:03:57 (25.0 MB/s) - ‘/root/smcv-tensorflow-tutorial/data/coco/train2017.zip’ saved [19336861798/19336861798]\n",
      "\n",
      "Unzipping COCO training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118287/118287 [11:15<00:00, 175.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading COCO validation data\n",
      "--2021-11-11 01:15:16--  http://images.cocodataset.org/zips/val2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.204.193\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.204.193|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 815585330 (778M) [application/zip]\n",
      "Saving to: ‘/root/smcv-tensorflow-tutorial/data/coco/val2017.zip’\n",
      "\n",
      "/root/smcv-tensorfl 100%[===================>] 777.80M  42.1MB/s    in 21s     \n",
      "\n",
      "2021-11-11 01:15:37 (36.2 MB/s) - ‘/root/smcv-tensorflow-tutorial/data/coco/val2017.zip’ saved [815585330/815585330]\n",
      "\n",
      "Get:1 file:/var/nvinfer-runtime-trt-repo-5.0.2-ga-cuda10.0  InRelease\n",
      "Ign:1 file:/var/nvinfer-runtime-trt-repo-5.0.2-ga-cuda10.0  InRelease\n",
      "Get:2 file:/var/nvinfer-runtime-trt-repo-5.0.2-ga-cuda10.0  Release [574 B]\n",
      "Get:2 file:/var/nvinfer-runtime-trt-repo-5.0.2-ga-cuda10.0  Release [574 B]\n",
      "Get:3 file:/var/nvinfer-runtime-trt-repo-5.0.2-ga-cuda10.0  Release.gpg [801 B]\n",
      "Get:3 file:/var/nvinfer-runtime-trt-repo-5.0.2-ga-cuda10.0  Release.gpg [801 B]\n",
      "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:5 file:/var/nvinfer-runtime-trt-repo-5.0.2-ga-cuda10.0  Packages [764 B]   \n",
      "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
      "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
      "Get:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
      "Get:11 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]   \n",
      "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [808 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]              \n",
      "Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [73.8 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [666 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]     \n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]   \n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1439 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2430 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [699 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2868 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2220 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]\n",
      "Fetched 24.7 MB in 4s (6397 kB/s)                             \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libprotobuf10 libprotoc10\n",
      "The following NEW packages will be installed:\n",
      "  libprotobuf10 libprotoc10 protobuf-compiler\n",
      "0 upgraded, 3 newly installed, 0 to remove and 90 not upgraded.\n",
      "Need to get 1242 kB of archives.\n",
      "After this operation, 4942 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf10 amd64 3.0.0-9.1ubuntu1 [651 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotoc10 amd64 3.0.0-9.1ubuntu1 [566 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 protobuf-compiler amd64 3.0.0-9.1ubuntu1 [24.5 kB]\n",
      "Fetched 1242 kB in 1s (1029 kB/s)         \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libprotobuf10:amd64.\n",
      "(Reading database ... 43265 files and directories currently installed.)\n",
      "Preparing to unpack .../libprotobuf10_3.0.0-9.1ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libprotobuf10:amd64 (3.0.0-9.1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libprotoc10:amd64.\n",
      "Preparing to unpack .../libprotoc10_3.0.0-9.1ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libprotoc10:amd64 (3.0.0-9.1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package protobuf-compiler.\n",
      "Preparing to unpack .../protobuf-compiler_3.0.0-9.1ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking protobuf-compiler (3.0.0-9.1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libprotobuf10:amd64 (3.0.0-9.1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libprotoc10:amd64 (3.0.0-9.1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up protobuf-compiler (3.0.0-9.1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J+ [ -z /root/smcv-tensorflow-tutorial/data/coco/tfrecord ]\n",
      "+ echo Cloning Tensorflow models directory (for conversion utilities)\n",
      "Cloning Tensorflow models directory (for conversion utilities)\n",
      "+ [ ! -e tf-models ]\n",
      "+ git clone http://github.com/tensorflow/models tf-models\n",
      "Cloning into 'tf-models'...\n",
      "warning: redirecting to https://github.com/tensorflow/models/\n",
      "remote: Enumerating objects: 66521, done.\u001b[K\n",
      "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
      "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
      "remote: Total 66521 (delta 115), reused 186 (delta 102), pack-reused 66289\u001b[K\n",
      "Receiving objects: 100% (66521/66521), 575.96 MiB | 31.20 MiB/s, done.\n",
      "Resolving deltas: 100% (46531/46531), done.\n",
      "Checking out files: 100% (2815/2815), done.\n",
      "+ cd tf-models/research\n",
      "+ protoc object_detection/protos/anchor_generator.proto object_detection/protos/argmax_matcher.proto object_detection/protos/bipartite_matcher.proto object_detection/protos/box_coder.proto object_detection/protos/box_predictor.proto object_detection/protos/calibration.proto object_detection/protos/center_net.proto object_detection/protos/eval.proto object_detection/protos/faster_rcnn.proto object_detection/protos/faster_rcnn_box_coder.proto object_detection/protos/flexible_grid_anchor_generator.proto object_detection/protos/fpn.proto object_detection/protos/graph_rewriter.proto object_detection/protos/grid_anchor_generator.proto object_detection/protos/hyperparams.proto object_detection/protos/image_resizer.proto object_detection/protos/input_reader.proto object_detection/protos/keypoint_box_coder.proto object_detection/protos/losses.proto object_detection/protos/matcher.proto object_detection/protos/mean_stddev_box_coder.proto object_detection/protos/model.proto object_detection/protos/multiscale_anchor_generator.proto object_detection/protos/optimizer.proto object_detection/protos/pipeline.proto object_detection/protos/post_processing.proto object_detection/protos/preprocessor.proto object_detection/protos/region_similarity_calculator.proto object_detection/protos/square_box_coder.proto object_detection/protos/ssd.proto object_detection/protos/ssd_anchor_generator.proto object_detection/protos/string_int_label_map.proto object_detection/protos/target_assigner.proto object_detection/protos/train.proto --python_out=.\n",
      "+ mv tf-models/research/object_detection .\n",
      "+ rm -rf tf-models\n",
      "+ COCO_DIR=/root/smcv-tensorflow-tutorial/data/coco\n",
      "+ OUTPUT_DIR=/root/smcv-tensorflow-tutorial/data/coco/tfrecord\n",
      "+ TRAIN_IMAGE_DIR=/root/smcv-tensorflow-tutorial/data/coco/train2017\n",
      "+ VAL_IMAGE_DIR=/root/smcv-tensorflow-tutorial/data/coco/val2017\n",
      "+ TRAIN_OBJECT_ANNOTATIONS_FILE=/root/smcv-tensorflow-tutorial/data/coco/annotations/instances_train2017.json\n",
      "+ VAL_OBJECT_ANNOTATIONS_FILE=/root/smcv-tensorflow-tutorial/data/coco/annotations/instances_val2017.json\n",
      "+ TRAIN_CAPTION_FILE=/root/smcv-tensorflow-tutorial/data/coco/annotations/captions_train2017.json\n",
      "+ VAL_CAPTION_FILE=/root/smcv-tensorflow-tutorial/data/coco/annotations/captions_val2017.json\n",
      "+ mkdir -p /root/smcv-tensorflow-tutorial/data/coco/tfrecord\n",
      "+ python create_coco_tf_record.py --logtostderr --include_masks --train_image_dir=/root/smcv-tensorflow-tutorial/data/coco/train2017 --val_image_dir=/root/smcv-tensorflow-tutorial/data/coco/val2017 --train_object_annotations_file=/root/smcv-tensorflow-tutorial/data/coco/annotations/instances_train2017.json --val_object_annotations_file=/root/smcv-tensorflow-tutorial/data/coco/annotations/instances_val2017.json --train_caption_annotations_file=/root/smcv-tensorflow-tutorial/data/coco/annotations/captions_train2017.json --val_caption_annotations_file=/root/smcv-tensorflow-tutorial/data/coco/annotations/captions_val2017.json --output_dir=/root/smcv-tensorflow-tutorial/data/coco/tfrecord\n",
      "2021-11-11 01:20:09.006685: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2021-11-11 01:20:09.011820: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2021-11-11 01:20:09.349228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-11 01:20:09.460424: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "INFO:tensorflow:writing to output path: /root/smcv-tensorflow-tutorial/data/coco/tfrecord/train\n",
      "I1111 01:20:12.432540 140220114069312 create_coco_tf_record.py:269] writing to output path: /root/smcv-tensorflow-tutorial/data/coco/tfrecord/train\n",
      "INFO:tensorflow:Building bounding box index.\n",
      "I1111 01:20:33.573320 140220114069312 create_coco_tf_record.py:215] Building bounding box index.\n",
      "INFO:tensorflow:1021 images are missing bboxes.\n",
      "I1111 01:20:34.155410 140220114069312 create_coco_tf_record.py:226] 1021 images are missing bboxes.\n",
      "INFO:tensorflow:Building caption index.\n",
      "I1111 01:20:38.663452 140220114069312 create_coco_tf_record.py:236] Building caption index.\n",
      "INFO:tensorflow:0 images are missing captions.\n",
      "I1111 01:20:38.900010 140220114069312 create_coco_tf_record.py:248] 0 images are missing captions.\n",
      "INFO:tensorflow:On image 0 of 118287\n",
      "I1111 01:20:39.773059 140220114069312 create_coco_tf_record.py:292] On image 0 of 118287\n",
      "INFO:tensorflow:On image 1000 of 118287\n",
      "I1111 01:20:54.981576 140220114069312 create_coco_tf_record.py:292] On image 1000 of 118287\n",
      "INFO:tensorflow:On image 2000 of 118287\n",
      "I1111 01:21:11.595414 140220114069312 create_coco_tf_record.py:292] On image 2000 of 118287\n",
      "INFO:tensorflow:On image 3000 of 118287\n",
      "I1111 01:21:31.687280 140220114069312 create_coco_tf_record.py:292] On image 3000 of 118287\n",
      "INFO:tensorflow:On image 4000 of 118287\n",
      "I1111 01:21:50.972393 140220114069312 create_coco_tf_record.py:292] On image 4000 of 118287\n",
      "INFO:tensorflow:On image 5000 of 118287\n",
      "I1111 01:22:10.458924 140220114069312 create_coco_tf_record.py:292] On image 5000 of 118287\n",
      "INFO:tensorflow:On image 6000 of 118287\n",
      "I1111 01:22:27.614581 140220114069312 create_coco_tf_record.py:292] On image 6000 of 118287\n",
      "INFO:tensorflow:On image 7000 of 118287\n",
      "I1111 01:22:45.437959 140220114069312 create_coco_tf_record.py:292] On image 7000 of 118287\n",
      "INFO:tensorflow:On image 8000 of 118287\n",
      "I1111 01:23:04.234357 140220114069312 create_coco_tf_record.py:292] On image 8000 of 118287\n",
      "INFO:tensorflow:On image 9000 of 118287\n",
      "I1111 01:23:19.708897 140220114069312 create_coco_tf_record.py:292] On image 9000 of 118287\n",
      "INFO:tensorflow:On image 10000 of 118287\n",
      "I1111 01:23:35.999587 140220114069312 create_coco_tf_record.py:292] On image 10000 of 118287\n",
      "INFO:tensorflow:On image 11000 of 118287\n",
      "I1111 01:23:51.081951 140220114069312 create_coco_tf_record.py:292] On image 11000 of 118287\n",
      "INFO:tensorflow:On image 12000 of 118287\n",
      "I1111 01:24:09.286301 140220114069312 create_coco_tf_record.py:292] On image 12000 of 118287\n",
      "INFO:tensorflow:On image 13000 of 118287\n",
      "I1111 01:24:24.645329 140220114069312 create_coco_tf_record.py:292] On image 13000 of 118287\n",
      "INFO:tensorflow:On image 14000 of 118287\n",
      "I1111 01:24:41.684176 140220114069312 create_coco_tf_record.py:292] On image 14000 of 118287\n",
      "INFO:tensorflow:On image 15000 of 118287\n",
      "I1111 01:24:58.251348 140220114069312 create_coco_tf_record.py:292] On image 15000 of 118287\n",
      "INFO:tensorflow:On image 16000 of 118287\n",
      "I1111 01:25:16.991275 140220114069312 create_coco_tf_record.py:292] On image 16000 of 118287\n",
      "INFO:tensorflow:On image 17000 of 118287\n",
      "I1111 01:25:33.053377 140220114069312 create_coco_tf_record.py:292] On image 17000 of 118287\n",
      "INFO:tensorflow:On image 18000 of 118287\n",
      "I1111 01:25:51.829274 140220114069312 create_coco_tf_record.py:292] On image 18000 of 118287\n",
      "INFO:tensorflow:On image 19000 of 118287\n",
      "I1111 01:26:06.963951 140220114069312 create_coco_tf_record.py:292] On image 19000 of 118287\n",
      "INFO:tensorflow:On image 20000 of 118287\n",
      "I1111 01:26:24.437197 140220114069312 create_coco_tf_record.py:292] On image 20000 of 118287\n",
      "INFO:tensorflow:On image 21000 of 118287\n",
      "I1111 01:26:39.999080 140220114069312 create_coco_tf_record.py:292] On image 21000 of 118287\n",
      "INFO:tensorflow:On image 22000 of 118287\n",
      "I1111 01:26:56.014717 140220114069312 create_coco_tf_record.py:292] On image 22000 of 118287\n",
      "INFO:tensorflow:On image 23000 of 118287\n",
      "I1111 01:27:13.009616 140220114069312 create_coco_tf_record.py:292] On image 23000 of 118287\n",
      "INFO:tensorflow:On image 24000 of 118287\n",
      "I1111 01:27:28.175217 140220114069312 create_coco_tf_record.py:292] On image 24000 of 118287\n",
      "INFO:tensorflow:On image 25000 of 118287\n",
      "I1111 01:27:46.050320 140220114069312 create_coco_tf_record.py:292] On image 25000 of 118287\n",
      "INFO:tensorflow:On image 26000 of 118287\n",
      "I1111 01:28:01.758128 140220114069312 create_coco_tf_record.py:292] On image 26000 of 118287\n",
      "INFO:tensorflow:On image 27000 of 118287\n",
      "I1111 01:28:16.261619 140220114069312 create_coco_tf_record.py:292] On image 27000 of 118287\n",
      "INFO:tensorflow:On image 28000 of 118287\n",
      "I1111 01:28:34.969502 140220114069312 create_coco_tf_record.py:292] On image 28000 of 118287\n",
      "INFO:tensorflow:On image 29000 of 118287\n",
      "I1111 01:28:50.337921 140220114069312 create_coco_tf_record.py:292] On image 29000 of 118287\n",
      "INFO:tensorflow:On image 30000 of 118287\n",
      "I1111 01:29:08.701431 140220114069312 create_coco_tf_record.py:292] On image 30000 of 118287\n",
      "INFO:tensorflow:On image 31000 of 118287\n",
      "I1111 01:29:23.153669 140220114069312 create_coco_tf_record.py:292] On image 31000 of 118287\n",
      "INFO:tensorflow:On image 32000 of 118287\n",
      "I1111 01:29:40.747274 140220114069312 create_coco_tf_record.py:292] On image 32000 of 118287\n",
      "INFO:tensorflow:On image 33000 of 118287\n",
      "I1111 01:29:55.800680 140220114069312 create_coco_tf_record.py:292] On image 33000 of 118287\n",
      "INFO:tensorflow:On image 34000 of 118287\n",
      "I1111 01:30:10.471838 140220114069312 create_coco_tf_record.py:292] On image 34000 of 118287\n",
      "INFO:tensorflow:On image 35000 of 118287\n",
      "I1111 01:30:28.572749 140220114069312 create_coco_tf_record.py:292] On image 35000 of 118287\n",
      "INFO:tensorflow:On image 36000 of 118287\n",
      "I1111 01:30:43.586116 140220114069312 create_coco_tf_record.py:292] On image 36000 of 118287\n",
      "INFO:tensorflow:On image 37000 of 118287\n",
      "I1111 01:31:01.618263 140220114069312 create_coco_tf_record.py:292] On image 37000 of 118287\n",
      "INFO:tensorflow:On image 38000 of 118287\n",
      "I1111 01:31:16.409474 140220114069312 create_coco_tf_record.py:292] On image 38000 of 118287\n",
      "INFO:tensorflow:On image 39000 of 118287\n",
      "I1111 01:31:35.185150 140220114069312 create_coco_tf_record.py:292] On image 39000 of 118287\n",
      "INFO:tensorflow:On image 40000 of 118287\n",
      "I1111 01:31:52.498108 140220114069312 create_coco_tf_record.py:292] On image 40000 of 118287\n",
      "INFO:tensorflow:On image 41000 of 118287\n",
      "I1111 01:32:08.069560 140220114069312 create_coco_tf_record.py:292] On image 41000 of 118287\n",
      "INFO:tensorflow:On image 42000 of 118287\n",
      "I1111 01:32:25.969803 140220114069312 create_coco_tf_record.py:292] On image 42000 of 118287\n",
      "INFO:tensorflow:On image 43000 of 118287\n",
      "I1111 01:32:41.040912 140220114069312 create_coco_tf_record.py:292] On image 43000 of 118287\n",
      "INFO:tensorflow:On image 44000 of 118287\n",
      "I1111 01:32:57.326756 140220114069312 create_coco_tf_record.py:292] On image 44000 of 118287\n",
      "INFO:tensorflow:On image 45000 of 118287\n",
      "I1111 01:33:13.364424 140220114069312 create_coco_tf_record.py:292] On image 45000 of 118287\n",
      "INFO:tensorflow:On image 46000 of 118287\n",
      "I1111 01:33:27.638669 140220114069312 create_coco_tf_record.py:292] On image 46000 of 118287\n",
      "INFO:tensorflow:On image 47000 of 118287\n",
      "I1111 01:33:45.361796 140220114069312 create_coco_tf_record.py:292] On image 47000 of 118287\n",
      "INFO:tensorflow:On image 48000 of 118287\n",
      "I1111 01:33:59.933599 140220114069312 create_coco_tf_record.py:292] On image 48000 of 118287\n",
      "INFO:tensorflow:On image 49000 of 118287\n",
      "I1111 01:34:14.700538 140220114069312 create_coco_tf_record.py:292] On image 49000 of 118287\n",
      "INFO:tensorflow:On image 50000 of 118287\n",
      "I1111 01:34:33.104259 140220114069312 create_coco_tf_record.py:292] On image 50000 of 118287\n",
      "INFO:tensorflow:On image 51000 of 118287\n",
      "I1111 01:34:48.908864 140220114069312 create_coco_tf_record.py:292] On image 51000 of 118287\n",
      "INFO:tensorflow:On image 52000 of 118287\n",
      "I1111 01:35:05.273515 140220114069312 create_coco_tf_record.py:292] On image 52000 of 118287\n",
      "INFO:tensorflow:On image 53000 of 118287\n",
      "I1111 01:35:21.758003 140220114069312 create_coco_tf_record.py:292] On image 53000 of 118287\n",
      "INFO:tensorflow:On image 54000 of 118287\n",
      "I1111 01:35:40.006412 140220114069312 create_coco_tf_record.py:292] On image 54000 of 118287\n",
      "INFO:tensorflow:On image 55000 of 118287\n",
      "I1111 01:35:56.298038 140220114069312 create_coco_tf_record.py:292] On image 55000 of 118287\n",
      "INFO:tensorflow:On image 56000 of 118287\n",
      "I1111 01:36:11.344453 140220114069312 create_coco_tf_record.py:292] On image 56000 of 118287\n",
      "INFO:tensorflow:On image 57000 of 118287\n",
      "I1111 01:36:28.999821 140220114069312 create_coco_tf_record.py:292] On image 57000 of 118287\n",
      "INFO:tensorflow:On image 58000 of 118287\n",
      "I1111 01:36:43.942407 140220114069312 create_coco_tf_record.py:292] On image 58000 of 118287\n",
      "INFO:tensorflow:On image 59000 of 118287\n",
      "I1111 01:37:00.293119 140220114069312 create_coco_tf_record.py:292] On image 59000 of 118287\n",
      "INFO:tensorflow:On image 60000 of 118287\n",
      "I1111 01:37:15.483348 140220114069312 create_coco_tf_record.py:292] On image 60000 of 118287\n",
      "INFO:tensorflow:On image 61000 of 118287\n",
      "I1111 01:37:32.811188 140220114069312 create_coco_tf_record.py:292] On image 61000 of 118287\n",
      "INFO:tensorflow:On image 62000 of 118287\n",
      "I1111 01:37:48.741798 140220114069312 create_coco_tf_record.py:292] On image 62000 of 118287\n",
      "INFO:tensorflow:On image 63000 of 118287\n",
      "I1111 01:38:03.492283 140220114069312 create_coco_tf_record.py:292] On image 63000 of 118287\n",
      "INFO:tensorflow:On image 64000 of 118287\n",
      "I1111 01:38:22.702586 140220114069312 create_coco_tf_record.py:292] On image 64000 of 118287\n",
      "INFO:tensorflow:On image 65000 of 118287\n",
      "I1111 01:38:38.056153 140220114069312 create_coco_tf_record.py:292] On image 65000 of 118287\n",
      "INFO:tensorflow:On image 66000 of 118287\n",
      "I1111 01:38:54.040433 140220114069312 create_coco_tf_record.py:292] On image 66000 of 118287\n",
      "INFO:tensorflow:On image 67000 of 118287\n",
      "I1111 01:39:10.706553 140220114069312 create_coco_tf_record.py:292] On image 67000 of 118287\n",
      "INFO:tensorflow:On image 68000 of 118287\n",
      "I1111 01:39:29.381331 140220114069312 create_coco_tf_record.py:292] On image 68000 of 118287\n",
      "INFO:tensorflow:On image 69000 of 118287\n",
      "I1111 01:39:44.552846 140220114069312 create_coco_tf_record.py:292] On image 69000 of 118287\n",
      "INFO:tensorflow:On image 70000 of 118287\n",
      "I1111 01:40:00.254701 140220114069312 create_coco_tf_record.py:292] On image 70000 of 118287\n",
      "INFO:tensorflow:On image 71000 of 118287\n",
      "I1111 01:40:19.554858 140220114069312 create_coco_tf_record.py:292] On image 71000 of 118287\n",
      "INFO:tensorflow:On image 72000 of 118287\n",
      "I1111 01:40:34.998768 140220114069312 create_coco_tf_record.py:292] On image 72000 of 118287\n",
      "INFO:tensorflow:On image 73000 of 118287\n",
      "I1111 01:40:52.433838 140220114069312 create_coco_tf_record.py:292] On image 73000 of 118287\n",
      "INFO:tensorflow:On image 74000 of 118287\n",
      "I1111 01:41:08.709096 140220114069312 create_coco_tf_record.py:292] On image 74000 of 118287\n",
      "INFO:tensorflow:On image 75000 of 118287\n",
      "I1111 01:41:24.498158 140220114069312 create_coco_tf_record.py:292] On image 75000 of 118287\n",
      "INFO:tensorflow:On image 76000 of 118287\n",
      "I1111 01:41:42.529680 140220114069312 create_coco_tf_record.py:292] On image 76000 of 118287\n",
      "INFO:tensorflow:On image 77000 of 118287\n",
      "I1111 01:41:57.862277 140220114069312 create_coco_tf_record.py:292] On image 77000 of 118287\n",
      "INFO:tensorflow:On image 78000 of 118287\n",
      "I1111 01:42:14.747300 140220114069312 create_coco_tf_record.py:292] On image 78000 of 118287\n",
      "INFO:tensorflow:On image 79000 of 118287\n",
      "I1111 01:42:29.536972 140220114069312 create_coco_tf_record.py:292] On image 79000 of 118287\n",
      "INFO:tensorflow:On image 80000 of 118287\n",
      "I1111 01:42:46.024455 140220114069312 create_coco_tf_record.py:292] On image 80000 of 118287\n",
      "INFO:tensorflow:On image 81000 of 118287\n",
      "I1111 01:43:01.479185 140220114069312 create_coco_tf_record.py:292] On image 81000 of 118287\n",
      "INFO:tensorflow:On image 82000 of 118287\n",
      "I1111 01:43:17.762773 140220114069312 create_coco_tf_record.py:292] On image 82000 of 118287\n",
      "INFO:tensorflow:On image 83000 of 118287\n",
      "I1111 01:43:34.525576 140220114069312 create_coco_tf_record.py:292] On image 83000 of 118287\n",
      "INFO:tensorflow:On image 84000 of 118287\n",
      "I1111 01:43:51.057501 140220114069312 create_coco_tf_record.py:292] On image 84000 of 118287\n",
      "INFO:tensorflow:On image 85000 of 118287\n",
      "I1111 01:44:07.816141 140220114069312 create_coco_tf_record.py:292] On image 85000 of 118287\n",
      "INFO:tensorflow:On image 86000 of 118287\n",
      "I1111 01:44:26.820176 140220114069312 create_coco_tf_record.py:292] On image 86000 of 118287\n",
      "INFO:tensorflow:On image 87000 of 118287\n",
      "I1111 01:44:42.178291 140220114069312 create_coco_tf_record.py:292] On image 87000 of 118287\n",
      "INFO:tensorflow:On image 88000 of 118287\n",
      "I1111 01:44:57.780563 140220114069312 create_coco_tf_record.py:292] On image 88000 of 118287\n",
      "INFO:tensorflow:On image 89000 of 118287\n",
      "I1111 01:45:15.645044 140220114069312 create_coco_tf_record.py:292] On image 89000 of 118287\n",
      "INFO:tensorflow:On image 90000 of 118287\n",
      "I1111 01:45:31.548037 140220114069312 create_coco_tf_record.py:292] On image 90000 of 118287\n",
      "INFO:tensorflow:On image 91000 of 118287\n",
      "I1111 01:45:49.150346 140220114069312 create_coco_tf_record.py:292] On image 91000 of 118287\n",
      "INFO:tensorflow:On image 92000 of 118287\n",
      "I1111 01:46:05.586006 140220114069312 create_coco_tf_record.py:292] On image 92000 of 118287\n",
      "INFO:tensorflow:On image 93000 of 118287\n",
      "I1111 01:46:22.516624 140220114069312 create_coco_tf_record.py:292] On image 93000 of 118287\n",
      "INFO:tensorflow:On image 94000 of 118287\n",
      "I1111 01:46:38.085733 140220114069312 create_coco_tf_record.py:292] On image 94000 of 118287\n",
      "INFO:tensorflow:On image 95000 of 118287\n",
      "I1111 01:46:56.151065 140220114069312 create_coco_tf_record.py:292] On image 95000 of 118287\n",
      "INFO:tensorflow:On image 96000 of 118287\n",
      "I1111 01:47:12.367116 140220114069312 create_coco_tf_record.py:292] On image 96000 of 118287\n",
      "INFO:tensorflow:On image 97000 of 118287\n",
      "I1111 01:47:31.276188 140220114069312 create_coco_tf_record.py:292] On image 97000 of 118287\n",
      "INFO:tensorflow:On image 98000 of 118287\n",
      "I1111 01:47:46.959761 140220114069312 create_coco_tf_record.py:292] On image 98000 of 118287\n",
      "INFO:tensorflow:On image 99000 of 118287\n",
      "I1111 01:48:03.156551 140220114069312 create_coco_tf_record.py:292] On image 99000 of 118287\n",
      "INFO:tensorflow:On image 100000 of 118287\n",
      "I1111 01:48:20.116206 140220114069312 create_coco_tf_record.py:292] On image 100000 of 118287\n",
      "INFO:tensorflow:On image 101000 of 118287\n",
      "I1111 01:48:36.655741 140220114069312 create_coco_tf_record.py:292] On image 101000 of 118287\n",
      "INFO:tensorflow:On image 102000 of 118287\n",
      "I1111 01:48:54.034619 140220114069312 create_coco_tf_record.py:292] On image 102000 of 118287\n",
      "INFO:tensorflow:On image 103000 of 118287\n",
      "I1111 01:49:09.223153 140220114069312 create_coco_tf_record.py:292] On image 103000 of 118287\n",
      "INFO:tensorflow:On image 104000 of 118287\n",
      "I1111 01:49:28.575493 140220114069312 create_coco_tf_record.py:292] On image 104000 of 118287\n",
      "INFO:tensorflow:On image 105000 of 118287\n",
      "I1111 01:49:43.954661 140220114069312 create_coco_tf_record.py:292] On image 105000 of 118287\n",
      "INFO:tensorflow:On image 106000 of 118287\n",
      "I1111 01:49:59.986290 140220114069312 create_coco_tf_record.py:292] On image 106000 of 118287\n",
      "INFO:tensorflow:On image 107000 of 118287\n",
      "I1111 01:50:17.129183 140220114069312 create_coco_tf_record.py:292] On image 107000 of 118287\n",
      "INFO:tensorflow:On image 108000 of 118287\n",
      "I1111 01:50:32.763884 140220114069312 create_coco_tf_record.py:292] On image 108000 of 118287\n",
      "INFO:tensorflow:On image 109000 of 118287\n",
      "I1111 01:50:48.770056 140220114069312 create_coco_tf_record.py:292] On image 109000 of 118287\n",
      "INFO:tensorflow:On image 110000 of 118287\n",
      "I1111 01:51:07.089560 140220114069312 create_coco_tf_record.py:292] On image 110000 of 118287\n",
      "INFO:tensorflow:On image 111000 of 118287\n",
      "I1111 01:51:24.635941 140220114069312 create_coco_tf_record.py:292] On image 111000 of 118287\n",
      "INFO:tensorflow:On image 112000 of 118287\n",
      "I1111 01:51:38.937774 140220114069312 create_coco_tf_record.py:292] On image 112000 of 118287\n",
      "INFO:tensorflow:On image 113000 of 118287\n",
      "I1111 01:51:54.497702 140220114069312 create_coco_tf_record.py:292] On image 113000 of 118287\n",
      "INFO:tensorflow:On image 114000 of 118287\n",
      "I1111 01:52:13.850819 140220114069312 create_coco_tf_record.py:292] On image 114000 of 118287\n",
      "INFO:tensorflow:On image 115000 of 118287\n",
      "I1111 01:52:32.560198 140220114069312 create_coco_tf_record.py:292] On image 115000 of 118287\n",
      "INFO:tensorflow:On image 116000 of 118287\n",
      "I1111 01:52:54.487525 140220114069312 create_coco_tf_record.py:292] On image 116000 of 118287\n",
      "INFO:tensorflow:On image 117000 of 118287\n",
      "I1111 01:53:13.790646 140220114069312 create_coco_tf_record.py:292] On image 117000 of 118287\n",
      "INFO:tensorflow:On image 118000 of 118287\n",
      "I1111 01:53:34.716281 140220114069312 create_coco_tf_record.py:292] On image 118000 of 118287\n",
      "INFO:tensorflow:Finished writing, skipped 2 annotations.\n",
      "I1111 01:54:30.358932 140220114069312 create_coco_tf_record.py:304] Finished writing, skipped 2 annotations.\n",
      "INFO:tensorflow:writing to output path: /root/smcv-tensorflow-tutorial/data/coco/tfrecord/val\n",
      "I1111 01:54:33.007668 140220114069312 create_coco_tf_record.py:269] writing to output path: /root/smcv-tensorflow-tutorial/data/coco/tfrecord/val\n",
      "INFO:tensorflow:Building bounding box index.\n",
      "I1111 01:54:38.421806 140220114069312 create_coco_tf_record.py:215] Building bounding box index.\n",
      "INFO:tensorflow:48 images are missing bboxes.\n",
      "I1111 01:54:38.438684 140220114069312 create_coco_tf_record.py:226] 48 images are missing bboxes.\n",
      "INFO:tensorflow:Building caption index.\n",
      "I1111 01:54:38.923582 140220114069312 create_coco_tf_record.py:236] Building caption index.\n",
      "INFO:tensorflow:0 images are missing captions.\n",
      "I1111 01:54:38.930701 140220114069312 create_coco_tf_record.py:248] 0 images are missing captions.\n",
      "INFO:tensorflow:On image 0 of 5000\n",
      "I1111 01:54:39.171393 140220114069312 create_coco_tf_record.py:292] On image 0 of 5000\n",
      "INFO:tensorflow:On image 1000 of 5000\n",
      "I1111 01:55:00.642646 140220114069312 create_coco_tf_record.py:292] On image 1000 of 5000\n",
      "INFO:tensorflow:On image 2000 of 5000\n",
      "I1111 01:55:21.234986 140220114069312 create_coco_tf_record.py:292] On image 2000 of 5000\n",
      "INFO:tensorflow:On image 3000 of 5000\n",
      "I1111 01:55:38.875432 140220114069312 create_coco_tf_record.py:292] On image 3000 of 5000\n",
      "INFO:tensorflow:On image 4000 of 5000\n",
      "I1111 01:55:59.507260 140220114069312 create_coco_tf_record.py:292] On image 4000 of 5000\n",
      "INFO:tensorflow:Finished writing, skipped 0 annotations.\n",
      "I1111 01:56:38.469520 140220114069312 create_coco_tf_record.py:304] Finished writing, skipped 0 annotations.\n",
      "+ rm -rf object_detection\n",
      "Uploading training tfrecords to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [02:39<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading validation tfrecords to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:08<00:00, 63.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Resnet Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-11 01:59:32--  https://sagemakercv.s3.us-west-2.amazonaws.com/weights/tensorflow/tensorflow_resnet50.tar\n",
      "Resolving sagemakercv.s3.us-west-2.amazonaws.com (sagemakercv.s3.us-west-2.amazonaws.com)... 52.92.145.18\n",
      "Connecting to sagemakercv.s3.us-west-2.amazonaws.com (sagemakercv.s3.us-west-2.amazonaws.com)|52.92.145.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 212418560 (203M) [application/x-tar]\n",
      "Saving to: ‘/root/smcv-tensorflow-tutorial/data/weights/tensorflow_resnet50.tar’\n",
      "\n",
      "/root/smcv-tensorfl 100%[===================>] 202.58M  78.2MB/s    in 2.6s    \n",
      "\n",
      "2021-11-11 01:59:34 (78.2 MB/s) - ‘/root/smcv-tensorflow-tutorial/data/weights/tensorflow_resnet50.tar’ saved [212418560/212418560]\n",
      "\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading annotations\")\n",
    "!wget -O $COCO_DIR/$ANNOTATIONS_ZIP $COCO_URL/annotations/$ANNOTATIONS_ZIP\n",
    "!unzip $COCO_DIR/$ANNOTATIONS_ZIP -d $COCO_DIR\n",
    "!aws s3 cp --recursive $COCO_DIR/annotations $S3_DATA_LOCATION/annotations\n",
    "\n",
    "print(\"Downloading COCO training data\")\n",
    "!wget -O $COCO_DIR/$TRAIN_ZIP $COCO_URL/zips/$TRAIN_ZIP\n",
    "\n",
    "# train data has ~128000 images. Unzip is too slow, about 1.5 hours beceause of disk read and write speed on the EBS volume. \n",
    "# This technique is much faster because it grabs all the zip metadata at once, then uses threading to unzip multiple files at once.\n",
    "print(\"Unzipping COCO training data\")\n",
    "train_zip = zipfile.ZipFile(os.path.join(COCO_DIR, TRAIN_ZIP))\n",
    "jpeg_files = [image.filename for image in train_zip.filelist if image.filename.endswith('.jpg')]\n",
    "os.makedirs(os.path.join(COCO_DIR, 'train2017'))\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    threads = list(tqdm(executor.map(lambda x: train_zip.extract(x, COCO_DIR), jpeg_files), total=len(jpeg_files)))\n",
    "\n",
    "print(\"Downloading COCO validation data\")\n",
    "!wget -O $COCO_DIR/$VAL_ZIP $COCO_URL/zips/$VAL_ZIP\n",
    "# switch to also threading\n",
    "!unzip -q $COCO_DIR/$VAL_ZIP -d $COCO_DIR\n",
    "val_images = [i for i in Path(os.path.join(COCO_DIR, 'val2017')).glob(\"*.jpg\")]\n",
    "    \n",
    "!apt-get -y update && apt install -y protobuf-compiler\n",
    "!cd sagemakercv/data/coco && ./process_coco_tfrecord.sh $COCO_DIR $TF_RECORD_DIR\n",
    "\n",
    "\n",
    "tfrecord_train = list(Path(TF_RECORD_DIR).glob('train-*.tfrecord'))\n",
    "tfrecord_val = list(Path(TF_RECORD_DIR).glob('val-*.tfrecord'))\n",
    "s3fs = S3FileSystem()\n",
    "\n",
    "print(\"Uploading training tfrecords to S3\")\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    threads = list(tqdm(executor.map(lambda record: s3fs.put(record.as_posix(), \n",
    "                                     os.path.join(S3_DATA_LOCATION, 'tfrecord', 'train2017', record.name)), \n",
    "                                     tfrecord_train), total=len(tfrecord_train)))\n",
    "print(\"Uploading validation tfrecords to S3\")\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    threads = list(tqdm(executor.map(lambda record: s3fs.put(record.as_posix(), \n",
    "                                     os.path.join(S3_DATA_LOCATION, 'tfrecord', 'val2017', record.name)), \n",
    "                                     tfrecord_val), total=len(tfrecord_val)))\n",
    "\n",
    "print(\"Downloading Resnet Weights\")\n",
    "!wget -O $WEIGHTS_DIR/$R50_WEIGHTS_TAR $R50_WEIGHTS_SRC/$R50_WEIGHTS_TAR\n",
    "!tar -xf $WEIGHTS_DIR/$R50_WEIGHTS_TAR -C $WEIGHTS_DIR\n",
    "s3fs.put(os.path.join(WEIGHTS_DIR, R50_WEIGHTS), S3_WEIGHTS_LOCATION, recursive=True)\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Training on Studio\n",
    "\n",
    "Now that we have the data, we can get to training a Mask RCNN model to detect objects in the COCO dataset images. \n",
    "\n",
    "Since training on a single GPU can take days, we'll just train for a couple thousands steps, and run a single evaluation to make sure our model is at least starting to learn something. We'll train a full model on a larger cluster of GPUs in a SageMaker training job.\n",
    "\n",
    "The reason we first want to train in Studio is that we want to dig a bit into the SageMakerCV framework, and talk about the model architecture, since we expect many users will want to modify models for their own use cases.\n",
    "\n",
    "#### Mask RCNN\n",
    "\n",
    "First, just a very brief overview of Mask RCNN. If you would like a more in depth examination, we recommend taking a look at the [original paper](https://arxiv.org/abs/1703.06870), the [feature pyramid paper](https://arxiv.org/abs/1612.03144) which describes a popular architectural change we'll use in our model, and blog posts from [viso.ai](https://viso.ai/deep-learning/mask-r-cnn/), [tryo labs](https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/), [Jonathan Hui](https://jonathan-hui.medium.com/image-segmentation-with-mask-r-cnn-ebe6d793272), and [Lilian Weng](https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html).\n",
    "\n",
    "Mask RCNN is a two stage object detection model that locates objects in images by places bounding boxes around, and segmentation masks over, any object for which the model is trained to find. It also provides classifcations for each object.\n",
    "\n",
    "<img src=\"../assets/traffic.png\" style=\"width: 1200px\">\n",
    "\n",
    "Mask RCNN is called a two stage model because it performs detection in two steps. The first identifies any objects in the image, versus background. The second stage determines the specific class of each object, and applies the segmentation mask. Below is an architectural diagram of the model. Let's walk through each step.\n",
    "\n",
    "<img src=\"../assets/mask_rcnn_arch.jpeg\" style=\"width: 1200px\">\n",
    "Credit: Jonathan Hui\n",
    "\n",
    "The `Convolution Network` is often referred to as the model backbone. This is a pretrained image classification model, commonly ResNet, which has been trained on a large image classification dataset, like ImageNet. The classification layer is removed, and instead the backbone outputs a set of convolution feature maps. The idea is, the classification model learned to identify objects in the process of classifying images, and now we can use that information to build a more complex model that can find those objects in the image. We want to pretrain because training the backbone at the same time as training the object detector tends to be very unstable.\n",
    "\n",
    "One additional component that is sometimes added to the backbone is a `Fearure Pyramid Network`. This take the outputs of the backbone, and combines them to together into a new set of feature maps by perform both up and down convolutions. The idea is that the different sized feature maps will help the model detect images of different sizes. The feature pyramid also helps with this, by allowing the different feature maps to share information with each other.\n",
    "\n",
    "The outputs of the feature pyramid are then passed to the `Region Proposal Network` which is responsible for finding regions of the image that might contain an object (this is the first of the two stages). The RPN will output several hundred thousand regions, each with a probability of containing an object. We'll typically take the top few thousand most likely regions. Because these several thousand regions will usually have a lot of overlap, we perform [non-max supression](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c), which removed regions with large areas of overlap. This gives us a set of `regions of interest` regions of the image that we think might contain an image.\n",
    "\n",
    "Next, we use those regions to crop out the corresponding sections of the feature maps that came from the feature pyramid network using a technique called [ROI align](https://firiuza.medium.com/roi-pooling-vs-roi-align-65293ab741db).\n",
    "\n",
    "We pass our cropped feature maps to the `box head` which classifies each region into either a specific object category, or as background. It also refines the position of the bounding box. In Mask RCNN, we also pass the feature maps to a `mask head` which produces a segmentation mask over the object.\n",
    "\n",
    "#### SageMakerCV Internals\n",
    "\n",
    "An important feature of Mask RCNN is its multiple heads. One head constructs a bounding box, while another creates a mask. These are referred to as the `ROI heads`. It's common for users to extend this and other two stage models by adding their own ROI heads. For example, a keypoint head it common. Doing so means modifying SageMakerCV's internals, so let's talk about those for a second. \n",
    "\n",
    "The high level Mask RCNN model can be found in `amazon-sageamaker-cv/pytorch/sagemakercv/detection/detectors/two_stage_detector.py`. If you trace through the `call` function, you'll see that the model first passes an image through the backbone, neck, then the RPN. The RPN layer also contains the non-max supression step. The regions of interest are then passed to the roi heads, where the regions of interest are used to crop sections of the feature maps, which are then classified into object categories.\n",
    "\n",
    "Probably the most important feature to be aware of are the `build` imports at the top. Each section of the model has an associated build function `(build_backbone, build_neck, build_dense_head, build_roi_head)` which are implemented in the `build_two_stage_detector` at the bottom of the file. These functions simplify building the model by letting us pass in a single configuration file for building all the different pieces.  \n",
    "\n",
    "For example, if you open `amazon-sageamaker-cv/tensorflow/sagemakercv/detection/roi_heads/standard_roi_head.py`, you'll find the `build_standard_roi_head` function at the bottom. To add a new head, you would write a Tensorflow module with its own build function. The decorator at the top of the build function allows it to be called from the config file. The dectorator `@HEADS.register(\"StandardRoIHead\")` adds a dictionary entry so that when `StandardRoIHead` is in the config file, build_standard_roi_head gets called at the `build_roi_head`. If, for example, you specify `CascadeRoIHead` the associated builder for the cascade ROI head is used instead.\n",
    "\n",
    "Finally, a note about data loading. SageMakerCV uses and optimized TFRecord data format. The COCO dataloader can be found in `amazon-sageamaker-cv/tensorflow/sagemakercv/data/coco/dataloader.py`. It takes a file pattern in the form `data/coco/train2017/train*` which will include all files that start with `train` in the dataset. You can use either a local directory or an S3 location `s3://my-bucket/my-data/coco/train2017/train*`. The dataloader will automatically switch between the two. The S3 functionality is especially useful for distributed training with large datasets, since it means you can train without waiting for your data to download.\n",
    "\n",
    "#### Setting Up Training\n",
    "\n",
    "Let's actually use some of these functions to train a model.\n",
    "\n",
    "Start by importing the default configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We use the [yacs](https://github.com/rbgirshick/yacs) format for configuration files. If you want to see the entire config, run `print(cfg.dump())` but this prints out a lot, and to not overwhelm you with too much information, we'll just focus on the bits we want to change for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "First, let's put in all the file directories for the data and weights we downloaded in the previous section, as well as an output directory for the model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.PATHS.TRAIN_FILE_PATTERN = os.path.join(TF_RECORD_DIR, \"train*\")\n",
    "cfg.PATHS.VAL_FILE_PATTERN = os.path.join(TF_RECORD_DIR, \"val*\")\n",
    "cfg.PATHS.WEIGHTS = os.path.join(WEIGHTS_DIR, \"model.ckpt-112603\")\n",
    "cfg.PATHS.VAL_ANNOTATIONS = os.path.join(COCO_DIR, \"annotations\", \"instances_val2017.json\")\n",
    "cfg.PATHS.OUT_DIR = os.path.join(LOCAL_DATA_DIR, \"output\")\n",
    "\n",
    "# create output dir if it doesn't exist\n",
    "os.makedirs(cfg.PATHS.OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This section specifies model details, including the type of model, and internal hyperparameters. We wont cover the details of all of these, but more information can be found in this blog posts listed above, as well as the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.LOG_INTERVAL = 50 # Number of training steps between logging interval\n",
    "cfg.MODEL.DENSE.PRE_NMS_TOP_N_TRAIN = 2000 # Top regions of interest to select before NMS\n",
    "cfg.MODEL.DENSE.POST_NMS_TOP_N_TRAIN = 1000 # Top regions of interest to select after NMS\n",
    "cfg.MODEL.RCNN.ROI_HEAD = \"StandardRoIHead\" # ROI head with box and mask, if mask is set to true\n",
    "cfg.MODEL.FRCNN.LOSS_TYPE = \"giou\"\n",
    "cfg.MODEL.INCLUDE_MASK = True # include mask. switching this off runs Faster RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next we set up the configuration for training, including the optimizer, hyperparameters, batch size, and training length. Batch size is global, so if you set a batch size of 64 across 8 GPUs, it will be a batch size of 8 per GPU. SageMakerCV currently supports the following optimizers: momentum SGD (stochastic gradient descent) and NovoGrad, and the following learning rate schedulers: stepwise and cosine decay. New, custom optimizers and schedulers can be added by modifying the `sagemakercv/training/builder.py` file.\n",
    "\n",
    "For training on Studio, we'll just run for a thousand steps. We'll be using SageMaker training instances for the full training on multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.INPUT.TRAIN_BATCH_SIZE = 4 # Training batch size\n",
    "cfg.INPUT.EVAL_BATCH_SIZE = 4 # Training batch size\n",
    "cfg.SOLVER.SCHEDULE = \"CosineDecay\" # Learning rate schedule, either CosineDecay or PiecewiseConstantDecay\n",
    "cfg.SOLVER.OPTIMIZER = \"NovoGrad\" # Optimizer type NovoGrad or Momentum\n",
    "cfg.SOLVER.LR = .002 # Base learning rate after warmup\n",
    "cfg.SOLVER.BETA_1 = 0.9 # NovoGrad beta 1 value\n",
    "cfg.SOLVER.BETA_2 = 0.5 # NovoGRad beta 2 value\n",
    "cfg.SOLVER.MAX_ITERS = 1000 # Total training steps\n",
    "cfg.SOLVER.WARMUP_STEPS = 250 # warmup steps\n",
    "cfg.SOLVER.XLA = True # Train with XLA\n",
    "cfg.SOLVER.FP16 = True # Train with mixed precision enables\n",
    "cfg.SOLVER.TF32 = False # Train with TF32 data type enabled, only available on Ampere GPUs and TF 2.4 and up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, SageMakerCV includes a number of training hooks. These work similar to Keras callbacks by adding some functionality to training. We use our own training hooks and runner class which improves performance beyond the standard keras model.fit() training strategy.\n",
    "\n",
    "Here we include three hooks. The `CheckpointHook` loads the backbone weights, and saves a model checkpoint after each epoch. The `IterTimerHook` and `TextLoggerHook` print helpful training progress information out to CloudWatch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.HOOKS=[\"CheckpointHook\",\n",
    "           \"IterTimerHook\",\n",
    "           \"TextLoggerHook\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our new configuration file in case we want to use it in future training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_config_file = f\"configs/local-config-studio.yaml\"\n",
    "with open(local_config_file, 'w') as outfile:\n",
    "    with redirect_stdout(outfile): print(cfg.dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saved model configuration can be loaded by first running `from configs import cfg` and mapping our saved file with `merge_from_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.merge_from_file(local_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can build and train our model. Import build functions so we can build pieces directory with our configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.2 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    }
   ],
   "source": [
    "from sagemakercv.detection import build_detector\n",
    "from sagemakercv.training import build_optimizer, build_scheduler, build_trainer\n",
    "from sagemakercv.data import build_dataset\n",
    "from sagemakercv.utils.dist_utils import get_dist_info, MPI_size, is_sm_dist\n",
    "from sagemakercv.utils.runner import Runner, build_hooks\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And include some standard TensorFlow configuration setup so our model runs in mixed precision with XLA enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank, local_rank, size, local_size = get_dist_info()\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "for device in devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "tf.config.set_visible_devices([devices[local_rank]], 'GPU')\n",
    "logical_devices = tf.config.list_logical_devices('GPU')\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": cfg.SOLVER.FP16})\n",
    "tf.config.optimizer.set_jit(cfg.SOLVER.XLA)\n",
    "if int(tf.__version__.split('.')[1])>=4:\n",
    "    tf.config.experimental.enable_tensor_float_32_execution(cfg.SOLVER.TF32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dataset and create an iterable object from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "dataset = iter(build_dataset(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the detector model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = build_detector(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a single observation through the model so the shapes are set. This is necessary to load the backbone weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-11-11 02:11:33.250 tensorflow-2-3-gpu--ml-g4dn-xlarge-33edf42bcb5531c041d8b56553ba:114 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-11-11 02:11:33.323 tensorflow-2-3-gpu--ml-g4dn-xlarge-33edf42bcb5531c041d8b56553ba:114 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    }
   ],
   "source": [
    "result = detector(features, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model optimizer. This will also build our learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = build_optimizer(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainer contains our training and evaluation step, and sets up our distributed training based on if we're using Horovod or SMDDP (more on this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MaskRCNN] INFO    : Using Horovod For Distributed Training\n"
     ]
    }
   ],
   "source": [
    "trainer = build_trainer(cfg, detector, optimizer, dist='smd' if is_sm_dist() else 'hvd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the runner will manage our training and run our training hooks. This serves a similar role to training with Keras, but provides increased flexibility and training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = Runner(trainer, cfg)\n",
    "hooks = build_hooks(cfg)\n",
    "for hook in hooks:\n",
    "    runner.register_hook(hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training for 1000 steps. This will take about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a partially trained model. Let's go ahead and try visualizing the results. You'll notice it picks up common categories (such as people) better at this point. The images are randomly picked from the training data, so it might take a few tries to get an image where the model picks up objects at this point in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemakercv.utils.visualization import build_image, restore_image\n",
    "from sagemakercv.data.coco.coco_labels import coco_categories\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = detector(features, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = 2 # image number within the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first restore the original image, then extract the boxes and labels from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = restore_image(result['images'][image_num], features['image_info'][image_num]) # converts the image back to its original shape and color\n",
    "boxes = result['detection_boxes'][image_num]\n",
    "classes = result['detection_classes'][image_num]\n",
    "scores = result['detection_scores'][image_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an image with the boxes and labels mapped onto it. The threshold limits the number of boxes to those were the model is at least this confident in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_image = build_image(image, boxes, scores, classes, coco_categories, threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(detection_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So far you've built a partially trained model locally on Studio. For many applications, this might be enough. If all you need is to train a model on a small dataset, you can likely do everything you need with what we've covered so far. \n",
    "\n",
    "On the other hand, if you need to train a model on many GBs or even TBs of data, and don't want to wait weeks for it to finish, you'll need to run a distributed training job across multiple GPUs, or even multiple nodes. With SageMaker training jobs you can train on as many as 512 [A100 GPUs](https://www.nvidia.com/en-us/data-center/a100/). We won't go quite that far, but we'll show you how.\n",
    "\n",
    "The section below is also replicated in the `SageMaker.ipynb` notebook for future training once all the above setup is complete.\n",
    "\n",
    "Before we get started, a few notes about how SageMaker training instances work. SageMaker takes care of a lot of setup for you, but it's important to understand a little of what's happening under the hood so you can customize training to your own needs. \n",
    "\n",
    "First we're going to look at a toy estimator to explain what's happening:\n",
    "\n",
    "```\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TesnorFlow\n",
    "\n",
    "estimator = TesnorFlow(\n",
    "                entry_point='train.py', \n",
    "                source_dir='.', \n",
    "                py_version='py37',\n",
    "                framework_version='2.4.1',\n",
    "                role=get_execution_role(),\n",
    "                instance_count=4,\n",
    "                instance_type='ml.p4d.24xlarge',\n",
    "                distribution=distribution,\n",
    "                output_path='s3://my-bucket/my-output/',\n",
    "                checkpoint_s3_uri='s3://my-bucket/my-checkpoints/',\n",
    "                model_dir='s3://my-bucket/my-model/',\n",
    "                hyperparameters={'config': 'my-config.yaml'},\n",
    "                volume_size=500,\n",
    "                code_location='s3://my-bucket/my-code/',\n",
    ")\n",
    "```\n",
    "\n",
    "The estimator forms the basic configuration of your training job.\n",
    "\n",
    "SageMaker will first launch `instance_count=4` `instance_type=ml.p4d.24xlarge` instances. The `role` is an IAM role that SageMaker will use to launch instances on your behalf. SageMaker includes a `get_execution_role` function which grabs the execution role of your current instance. Each instance will have a `volume_size=500` EBS volume attached for your model and data. On `ml.p4d.24xlarge` and `ml.p3dn.24xlarge` instance types, SageMaker will automatically set up the [Elastic Fabric Adapter](https://aws.amazon.com/hpc/efa/). EFA provides up to 400 GB/s communication between your training nodes, as well as [GPU Direct RDMA](https://aws.amazon.com/about-aws/whats-new/2020/11/efa-supports-nvidia-gpudirect-rdma/) on `ml.p4d.24xlarge`, which allows your GPUs to bypass the host and communicate directly with each other across nodes.\n",
    "\n",
    "Next, SageMaker we copy all the contents of `source_dir='.'` first to the `code_location='s3://my-bucket/my-code/'` S3 location, then to each of your instances. One common mistake is to leave large files or data in this directory or its subdirectories. This will slow down your launch times, or can even cause the launch to hang. Make sure to keep your working data and model artifacts elsewhere on your Studio instance so you don't accidently copy them to your training instance. You should instead use `Channels` to copy data and model artifacts, which we'll cover shortly.\n",
    "\n",
    "SageMaker will then download the training Docker image to all your instances. Which container you download is determined by `py_version='py37'` and `framework_version='2.4.1'`. You can also use your own [custom Docker image](https://aws.amazon.com/blogs/machine-learning/bringing-your-own-custom-container-image-to-amazon-sagemaker-studio-notebooks/) by specifying an ECR address with the `image_uri` option.\n",
    "\n",
    "Before starting training, SageMaker will check your source directory for a `setup.py` file, and install if one is present. Then SageMaker will launch training, via `entry_point='train.py'`. Anything in `hyperparameters={'config': 'my-config.yaml'}` will be passed to the training script as a command line argument (ie `python train.py --config my-config.yaml`). The distribution will determine what form of distributed training to launch. This will be covered in more detail later.\n",
    "\n",
    "During training, anything written to `/opt/ml/checkpoints` on your training instances will be synced to `checkpoint_s3_uri='s3://my-bucket/my-checkpoints/'` at the same time. This can be useful for checkpointing a model you might want to restart later, or for writting Tensorboard logs to monitor your training.\n",
    "\n",
    "When training complets, you can write your model artifacats to `/opt/ml/model` and it will save to `model_dir='s3://my-bucket/my-model/'`. Another option is to also write model artifacts to your checkpoints file.\n",
    "\n",
    "Training logs, and any failure messages will to written to `/opt/ml/output` and saved to `output_path='s3://my-bucket/my-output/'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set some names. You want `AWS_DEFAULT_REGION` to be the same region as the S3 bucket your created earlier, to ensure your training jobs are reading from nearby S3 buckets.\n",
    "\n",
    "Next, set a `user_id`. This is just for naming your training job so it's easier to find later. This can be anything you like. We also get the current date and time to make organizing training jobs a little easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain region. Don't launch a training job in VA with S3  bucket in OR\n",
    "os.environ['AWS_DEFAULT_REGION'] = region # This is the region we set at the beginning, when creating the S3 bucket for our data\n",
    "\n",
    "# this is all for naming\n",
    "user_id=\"jbsnyder-smcv-tutorial\" # This is used for naming your training job, and organizing your results on S3. It can be anything you like.\n",
    "date_str=datetime.now().strftime(\"%d-%m-%Y\") # use the data and time to keep track of training jobs and organize results in S3\n",
    "time_str=datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance type, we'll use an `ml.p4d.24xlarge`. We recommend this instance type for large training. It includes the latest A100 Nvidia GPUs, which can train several times faster than the previous generation. If you would rather train part way on smaller instanes, `ml.p3.2xlarge, ml.p3.8xlarge, ml.p3.16xlarge, ml.p3dn.24xlarge, ml.g4dn.12xlarge` are all good options. In particular, if you're looking for a low cost way to try a short distributed training, but aren't worried about the model fully converging, we recommend the `ml.g4dn.12xlarge` which uses 4 Nvidia T4 GPUs per node.\n",
    "\n",
    "`s3_location` will be the base S3 storage location we used earlier for the COCO data. For `role` we get the execution role from our studio instance. For `source_dir` we use the current directory. Again, make sure you haven't accidently written any large files to this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training type, s3 src and nodes\n",
    "instance_type=\"ml.p4d.24xlarge\" # This can be any of 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p3.16xlarge', 'ml.p3.8xlarge', 'ml.p3.2xlarge', 'ml.g4dn.12xlarge'\n",
    "nodes=2 # number of training nodes\n",
    "s3_location=os.path.join(\"s3://\", S3_BUCKET, S3_DIR)\n",
    "role=get_execution_role() #give Sagemaker permission to launch nodes on our behalf\n",
    "source_dir='.'\n",
    "entry_point='train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Let's modify our previous training configuration for multinode. We don't need to change much. We'll increase the batch size since we have more and large GPUs. For A100 GPUs a batch size of 12 per GPU works well. For V100 and T4 GPUs, a batch size of 6 per GPU is recommended. Make sure to lower the learning rate and increase your number of training steps if you decrease the batch size. For example, if you want to train on 2 `ml.g4dn.12xlarge` instances, you'll have 8 T4 GPUs. A batch size of `cfg.INPUT.TRAIN_BATCH_SIZE = 32`, with inference batch size of `cfg.INPUT.EVAL_BATCH_SIZE = 16`, learning rate of `cfg.SOLVER.LR = .008`, and training steps of `cfg.SOLVER.MAX_ITERS = 25000`` is probably about right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.LOG_INTERVAL = 50 # Number of training steps between logging interval\n",
    "cfg.MODEL.DENSE.PRE_NMS_TOP_N_TRAIN = 2000 # Top regions of interest to select before NMS\n",
    "cfg.MODEL.DENSE.POST_NMS_TOP_N_TRAIN = 1000 # Top regions of interest to select after NMS\n",
    "cfg.MODEL.RCNN.ROI_HEAD = \"StandardRoIHead\"\n",
    "cfg.MODEL.FRCNN.LOSS_TYPE = \"giou\"\n",
    "cfg.MODEL.FRCNN.LABEL_SMOOTHING = 0.1 # label smoothing for box head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.INPUT.TRAIN_BATCH_SIZE = 128 #256 # Training batch size\n",
    "cfg.INPUT.EVAL_BATCH_SIZE = 64 #128 # Training batch size\n",
    "cfg.SOLVER.SCHEDULE = \"CosineDecay\" # Learning rate schedule, either CosineDecay or PiecewiseConstantDecay\n",
    "cfg.SOLVER.OPTIMIZER = \"NovoGrad\" # Optimizer type NovoGrad or Momentum\n",
    "cfg.SOLVER.LR = .028 #.042 # Base learning rate after warmup\n",
    "cfg.SOLVER.BETA_1 = 0.9 # NovoGrad beta 1 value\n",
    "cfg.SOLVER.BETA_2 = 0.3 # NovoGRad beta 2 value\n",
    "cfg.SOLVER.ALPHA = 0.001 # scehduler final alpha\n",
    "cfg.SOLVER.WEIGHT_DECAY = 0.001 # weight decay\n",
    "cfg.SOLVER.MAX_ITERS = 9000 #5500 # Total training steps\n",
    "cfg.SOLVER.WARMUP_STEPS = 500 # warmup steps\n",
    "cfg.SOLVER.XLA = True # Train with XLA\n",
    "cfg.SOLVER.FP16 = True # Train with mixed precision enables\n",
    "cfg.SOLVER.TF32 = True # Train with TF32 data type enabled, only available on Ampere GPUs and TF 2.4 and up\n",
    "cfg.SOLVER.EVAL_EPOCH_EVAL = False # Only run eval at end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.HOOKS=[\"CheckpointHook\",\n",
    "           \"IterTimerHook\",\n",
    "           \"TextLoggerHook\",\n",
    "           \"CocoEvaluator\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Earlier we mentioned the `distrbution` strategy in SageMaker. Distributed training can be either multi GPU single node (ie training on 8 GPU in a single ml.p4d.24xlarge) or mutli GPU multi node (ie training on 32 GPUs across 4 ml.p4d.24xlarges). For TensorFlow SageMakerCV uses either Horovod or [SageMaker Distributed Data Parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html) (SMDDP). For single node multi GPU, or multi node on small instances, we recommend Horovod. For multinode on large instance types, SMDDP is built to fully utilize AWS network topology, and EFA, providing improved scaling efficiency.\n",
    "\n",
    "To enable SMDDP, set `distribution = { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }`. SageMakerCV already has SMDDP integrated. To implement SMDDP for your own models, follow [these instructions](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html). SMDDP will launch training from the first node in your cluster using [MPI](https://www.open-mpi.org/).\n",
    "\n",
    "For Horovod based training, we can call MPI directory by setting `distribution = {\"mpi\": {\"enabled\": True,}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nodes>1 and instance_type in ['ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p3.16xlarge']:\n",
    "    distribution = { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } } \n",
    "else:\n",
    "    distribution = {\"mpi\": {\"enabled\": True,}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We'll set a job name based on the user name and time. We'll then set output directories on S3 using the date and job name.\n",
    "\n",
    "For this training, we'll use the same S3 location for all 3 SageMaker model outputs `/opt/ml/checkpoint`, `/opt/ml/model`, and `/opt/ml/output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{user_id}-{time_str}' # Set the job name to user id and the current time\n",
    "output_path = os.path.join(s3_location, \"sagemaker-output\", date_str, job_name) # Organizes results on S3 by date and job name\n",
    "code_location = os.path.join(s3_location, \"sagemaker-code\", date_str, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next we need to add our data sources to our configuration file, but first let's talk a little more about how SageMaker gets data to your instance.\n",
    "\n",
    "The most straightforward way to get your data is using \"Channels.\" These are S3 locations you specify in a dictionary when you launch a training job. For example, let's say you launch a training job with:\n",
    "\n",
    "```\n",
    "channels = {'train': 's3://my-bucket/data/train/',\n",
    "            'test': 's3://my-bucket/data/test/',\n",
    "            'weights': 's3://my-bucket/data/weights/',\n",
    "            'dave': 's3://my-bucket/data/daves_weird_data/'}\n",
    "\n",
    "pytorch_estimator.fit(channels)\n",
    "```\n",
    "\n",
    "At the start of training, SageMaker will create a set of corresponding directories on each training node:\n",
    "\n",
    "```\n",
    "/opt/ml/input/data/train/\n",
    "/opt/ml/input/data/test/\n",
    "/opt/ml/input/data/weights/\n",
    "/opt/ml/input/data/dave/\n",
    "```\n",
    "\n",
    "SageMaker will then copy all the contents of the corresponding S3 locations to these directories, which you can then access in training.\n",
    "\n",
    "One downside of setting up channels like this is that it requires all the data to be downloaded to your instance at the start of of training, which can delay the training launch if you're dealing with a large dataset.\n",
    "\n",
    "We have two ways to speed up launch. The first is [Fast File Mode](https://aws.amazon.com/about-aws/whats-new/2021/10/amazon-sagemaker-fast-file-mode/) which downloads data from S3 as it's requested by the training model, speeding up your launch time. You can use fast file mode by sepcifying `TrainingInputMode='FastFile'` in your SageMaker estimator configuration. \n",
    "\n",
    "If you're dealing with really large datasets, you might prefer to instead continuously stream data from S3. Luckily, this feature is already supported in TensorFlow and SageMakerCV. If you provide the dataset builder with an S3 file pattern, it will stream TFRecords from S3 instead of reading them locally.\n",
    "\n",
    "In our case, we'll use a mix of channels and streaming from S3. We'll download the smaller pieces at the start of training (the validation data, pretrained weights, and image annotations), and we'll stream our training data directly from S3 during training.\n",
    "\n",
    "First, we setup our training channels. These are the locations where we earlier uploaded our COCO data, annotations, and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = {'val2017': os.path.join(s3_location, 'data', 'coco', 'tfrecord', 'val2017'),\n",
    "            'annotations': os.path.join(s3_location, 'data', 'coco', 'annotations'),\n",
    "            'weights': os.path.join(s3_location, 'data', 'weights', 'resnet')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup the data sources in our configuration. The train file pattern will take and S3 location. The others are all set to the corresponding directory for each channel. We also set the output directory to be the SageMaker checkpoint directory, which will sync to our S3 output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS_DIR='/opt/ml/input/data/' # on node\n",
    "cfg.PATHS.TRAIN_FILE_PATTERN = os.path.join(s3_location, 'data', 'coco', 'tfrecord', 'train2017', 'train*')\n",
    "cfg.PATHS.VAL_FILE_PATTERN = os.path.join(CHANNELS_DIR, \"val2017\", \"val*\")\n",
    "cfg.PATHS.WEIGHTS = os.path.join(CHANNELS_DIR, \"weights\", \"resnet50.ckpt\")\n",
    "cfg.PATHS.VAL_ANNOTATIONS = os.path.join(CHANNELS_DIR, \"annotations\", \"instances_val2017.json\")\n",
    "cfg.PATHS.OUT_DIR = '/opt/ml/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_config_file = f\"configs/dist-training-config.yaml\"\n",
    "with open(dist_config_file, 'w') as outfile:\n",
    "    with redirect_stdout(outfile): print(cfg.dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the config file as a hyperparameter so it will be passed a command line arg when training launches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"config\": dist_config_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can launch training. With 4 P4d instances, this takes about an hour. This section will also print a lot of output logs. By setting `wait=False` you can avoid printing logs in the notebook. This setting will just launch the job then return, and is useful for when you want to launch several jobs at the same time. You can then montior each job from the [SageMaker Training Console](https://us-west-2.console.aws.amazon.com/sagemaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "                entry_point=entry_point, \n",
    "                source_dir=source_dir, \n",
    "                py_version='py37',\n",
    "                framework_version='2.4.1',\n",
    "                role=role,\n",
    "                instance_count=nodes,\n",
    "                instance_type=instance_type,\n",
    "                distribution=distribution,\n",
    "                output_path=output_path,\n",
    "                checkpoint_s3_uri=output_path,\n",
    "                model_dir=output_path,\n",
    "                hyperparameters=hyperparameters,\n",
    "                volume_size=500,\n",
    "                disable_profiler=True,\n",
    "                debugger_hook_config=False,\n",
    "                code_location=code_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jbsnyder-smcv-tutorial-12-11-2021-14-31-25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 14:33:21 Starting - Starting the training job...\n",
      "2021-11-12 14:33:24 Starting - Launching requested ML instances............\n",
      "2021-11-12 14:35:43 Starting - Preparing the instances for training....................................\n",
      "2021-11-12 14:41:42 Downloading - Downloading input data...\n",
      "2021-11-12 14:42:13 Training - Downloading the training image..............\u001b[34m2021-11-12 14:44:35.595971: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:35.598697: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:35.667742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:35.737507: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:38,442 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:40,862 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl\n",
      "  Downloading https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\n",
      "2021-11-12 14:44:37 Training - Training image download completed. Training in progress.\u001b[35m2021-11-12 14:44:37.804436: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:37.806805: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:37.874321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:37.949917: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:40,625 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34mCollecting matplotlib\n",
      "  Downloading matplotlib-3.4.3-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpi4py in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (3.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (4.3.0.36)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/site-packages (from pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl->sagemakercv==0.1) (56.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/site-packages (from pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl->sagemakercv==0.1) (2.6.2)\u001b[0m\n",
      "\u001b[34mCollecting cython>=0.27.3\n",
      "  Downloading Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (8.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (1.19.5)\u001b[0m\n",
      "\u001b[34mCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->sagemakercv==0.1) (1.15.0)\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:43,379 sagemaker-training-toolkit INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/usr/local/bin/python3.7 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.0-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (2.24.0)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (3.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.4.0-py3-none-any.whl (48 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/site-packages (from importlib-resources->tensorflow_datasets->sagemakercv==0.1) (3.4.1)\u001b[0m\n",
      "\u001b[34mCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/site-packages (from yacs->sagemakercv==0.1) (5.4.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemakercv, promise\n",
      "  Building wheel for sagemakercv (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl\n",
      "  Downloading https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35mCollecting tensorflow_addons\n",
      "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[35mCollecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\u001b[0m\n",
      "\u001b[35mCollecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemakercv (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemakercv: filename=sagemakercv-0.1-py3-none-any.whl size=197276 sha256=ee4b3db8b4d24bee2e3a0da7ee376ad20c20f3d7db1fb4f1c4bfb8db9867d0aa\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0cqsb8yq/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=8c532554f13288b97f14b110b032a90724b2bb57c411a87eb0883320a3ffbd65\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemakercv promise\u001b[0m\n",
      "\u001b[35mCollecting matplotlib\n",
      "  Downloading matplotlib-3.4.3-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpi4py in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (3.0.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/site-packages (from sagemakercv==0.1) (4.3.0.36)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/site-packages (from pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl->sagemakercv==0.1) (56.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/site-packages (from pycocotools@ https://sagemakercv.s3.us-west-2.amazonaws.com/cocoapi/pycocotools-2.0%2Bnv0.6.0-cp37-cp37m-linux_x86_64.whl->sagemakercv==0.1) (2.6.2)\u001b[0m\n",
      "\u001b[35mCollecting cython>=0.27.3\n",
      "  Downloading Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (8.2.0)\u001b[0m\n",
      "\u001b[35mCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/site-packages (from matplotlib->sagemakercv==0.1) (1.19.5)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: kiwisolver, googleapis-common-protos, cycler, typeguard, tqdm, tensorflow-metadata, promise, matplotlib, importlib-resources, cython, yacs, tensorflow-datasets, tensorflow-addons, pycocotools, sagemakercv\u001b[0m\n",
      "\u001b[35mCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->sagemakercv==0.1) (1.15.0)\u001b[0m\n",
      "\u001b[35mCollecting typeguard>=2.7\n",
      "  Downloading typeguard-2.13.0-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[35mCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.4.0-py3-none-any.whl (48 kB)\u001b[0m\n",
      "\u001b[35mCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: termcolor in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (2.24.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (3.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (3.7.4.3)\u001b[0m\n",
      "\u001b[35mCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: absl-py in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.10.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (21.2.0)\u001b[0m\n",
      "\u001b[35mCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: future in /usr/local/lib/python3.7/site-packages (from tensorflow_datasets->sagemakercv==0.1) (0.18.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (1.25.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (2020.12.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->sagemakercv==0.1) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/site-packages (from importlib-resources->tensorflow_datasets->sagemakercv==0.1) (3.4.1)\u001b[0m\n",
      "\u001b[35mCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/site-packages (from yacs->sagemakercv==0.1) (5.4.1)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: sagemakercv, promise\n",
      "  Building wheel for sagemakercv (setup.py): started\u001b[0m\n",
      "\u001b[35m  Building wheel for sagemakercv (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemakercv: filename=sagemakercv-0.1-py3-none-any.whl size=197276 sha256=d712fb4cde1832760dfa3d8253297c5b7270da3b0ca1891090cb2691c98f4aa2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bhu7cm2u/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=56ec6333dca900c79b51ba9bec49df861bede8b068b67c49ce7984ae088b49a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\u001b[0m\n",
      "\u001b[35mSuccessfully built sagemakercv promise\u001b[0m\n",
      "\u001b[34mSuccessfully installed cycler-0.11.0 cython-0.29.24 googleapis-common-protos-1.53.0 importlib-resources-5.4.0 kiwisolver-1.3.2 matplotlib-3.4.3 promise-2.3 pycocotools-2.0+nv0.6.0 sagemakercv-0.1 tensorflow-addons-0.15.0 tensorflow-datasets-4.4.0 tensorflow-metadata-1.4.0 tqdm-4.62.3 typeguard-2.13.0 yacs-0.1.8\u001b[0m\n",
      "\u001b[35mInstalling collected packages: kiwisolver, googleapis-common-protos, cycler, typeguard, tqdm, tensorflow-metadata, promise, matplotlib, importlib-resources, cython, yacs, tensorflow-datasets, tensorflow-addons, pycocotools, sagemakercv\u001b[0m\n",
      "\u001b[34m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:48,925 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:48,925 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:48,937 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:48,939 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:48,939 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:49,942 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:49,942 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35mSuccessfully installed cycler-0.11.0 cython-0.29.24 googleapis-common-protos-1.53.0 importlib-resources-5.4.0 kiwisolver-1.3.2 matplotlib-3.4.3 promise-2.3 pycocotools-2.0+nv0.6.0 sagemakercv-0.1 tensorflow-addons-0.15.0 tensorflow-datasets-4.4.0 tensorflow-metadata-1.4.0 tqdm-4.62.3 typeguard-2.13.0 yacs-0.1.8\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:50,944 sagemaker-training-toolkit INFO     Cannot connect to host algo-2 at port 22. Retrying...\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:50,944 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[35m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35mWARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\u001b[0m\n",
      "\u001b[35mYou should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,401 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,402 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,409 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,482 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,482 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,482 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,482 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:51,493 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:51,952 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_7.6p1)\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,020 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,020 sagemaker-training-toolkit INFO     Can connect to host algo-2 at port 22\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,021 sagemaker-training-toolkit INFO     Connection closed\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,021 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,021 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,021 sagemaker-training-toolkit INFO     Host: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,022 sagemaker-training-toolkit INFO     instance type: ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34m2021-11-12 14:44:52,113 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": true,\n",
      "        \"sagemaker_distributed_dataparallel_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\"\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"annotations\": \"/opt/ml/input/data/annotations\",\n",
      "        \"val2017\": \"/opt/ml/input/data/val2017\",\n",
      "        \"weights\": \"/opt/ml/input/data/weights\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-output/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25\",\n",
      "        \"config\": \"configs/dist-training-config.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"annotations\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val2017\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"jbsnyder-smcv-tutorial-12-11-2021-14-31-25\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-code/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25/jbsnyder-smcv-tutorial-12-11-2021-14-31-25/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"configs/dist-training-config.yaml\",\"model_dir\":\"s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-output/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\"}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"annotations\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val2017\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"weights\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"annotations\",\"val2017\",\"weights\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-code/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25/jbsnyder-smcv-tutorial-12-11-2021-14-31-25/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_custom_mpi_options\":\"\",\"sagemaker_distributed_dataparallel_enabled\":true,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\"},\"channel_input_dirs\":{\"annotations\":\"/opt/ml/input/data/annotations\",\"val2017\":\"/opt/ml/input/data/val2017\",\"weights\":\"/opt/ml/input/data/weights\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"config\":\"configs/dist-training-config.yaml\",\"model_dir\":\"s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-output/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"annotations\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val2017\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"weights\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"jbsnyder-smcv-tutorial-12-11-2021-14-31-25\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-code/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25/jbsnyder-smcv-tutorial-12-11-2021-14-31-25/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"configs/dist-training-config.yaml\",\"--model_dir\",\"s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-output/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ANNOTATIONS=/opt/ml/input/data/annotations\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL2017=/opt/ml/input/data/val2017\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_WEIGHTS=/opt/ml/input/data/weights\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-output/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=configs/dist-training-config.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 2 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_HOMOGENEOUS=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/usr/local/lib/python3.7/site-packages/gethostname.cpython-37m-x86_64-linux-gnu.so -x SMDATAPARALLEL_SERVER_ADDR=algo-1 -x SMDATAPARALLEL_SERVER_PORT=7592 -x SAGEMAKER_INSTANCE_TYPE=ml.p4d.24xlarge smddprun /usr/local/bin/python3.7 -m mpi4py -m train --config configs/dist-training-config.yaml --model_dir s3://sagemaker-smcv-tutorial/smcv-tensorflow-tutorial/sagemaker-output/12-11-2021/jbsnyder-smcv-tutorial-12-11-2021-14-31-25\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:53,499 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=199, name='orted', status='sleeping', started='14:44:51')]\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:53,500 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=199, name='orted', status='sleeping', started='14:44:51')]\u001b[0m\n",
      "\u001b[35m2021-11-12 14:44:53,500 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=199, name='orted', status='sleeping', started='14:44:51')]\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:NOTE! Installing ujson may make loading annotations faster.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Using Dataset Sharding\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-11-12 14:45:20.716 algo-2:288 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-11-12 14:45:20.876 algo-2:293 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-11-12 14:45:21.073 algo-2:291 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:[2021-11-12 14:45:21.120 algo-2:291 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-11-12 14:45:21.137 algo-2:290 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-11-12 14:45:21.154 algo-2:315 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:[2021-11-12 14:45:21.215 algo-2:315 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:[2021-11-12 14:45:21.282 algo-2:288 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-11-12 14:45:21.339 algo-2:287 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-12 14:45:21.382 algo-1:280 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-12 14:45:21.383 algo-1:278 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:[2021-11-12 14:45:21.395 algo-2:293 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:[2021-11-12 14:45:21.396 algo-2:287 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:[2021-11-12 14:45:21.603 algo-1:280 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-12 14:45:21.605 algo-1:281 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:[2021-11-12 14:45:21.646 algo-2:290 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:[2021-11-12 14:45:21.834 algo-1:281 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-12 14:45:21.956 algo-1:279 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:[2021-11-12 14:45:21.963 algo-1:278 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-12 14:45:22.048 algo-1:283 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-12 14:45:22.055 algo-1:282 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:[2021-11-12 14:45:22.077 algo-1:283 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:[2021-11-12 14:45:22.135 algo-1:282 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:[2021-11-12 14:45:22.227 algo-1:279 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-12 14:45:22.271 algo-1:284 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-11-12 14:45:22.333 algo-2:289 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-12 14:45:22.346 algo-1:295 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[2021-11-12 14:45:22.374 algo-1:295 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:[2021-11-12 14:45:22.459 algo-2:289 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:[2021-11-12 14:45:22.563 algo-1:284 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-11-12 14:45:22.791 algo-2:292 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:[2021-11-12 14:45:22.911 algo-2:292 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Using Horovod For Distributed Training\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Using Evaluation Dataset Sharding\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Start running, work_dir: /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:max: 10 epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Loading checkpoint from /opt/ml/input/data/weights/resnet50.ckpt...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Start time: 2021-11-12 14:45:34.781551\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 1 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Broadcasting model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:[MaskRCNN] INFO    : Broadcasting model\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:NCCL version 2.7.8+cuda11.0\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Bootstrap : Using [0]eth0:10.0.236.146<0>\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Bootstrap : Using [0]eth0:10.0.210.185<0>\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI Running on P4d platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v3 symbol.\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] graph/search.cc:765 NCCL WARN Could not find a path for pattern 4, falling back to simple order\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Trees [0] 11/-1/-1->10->9|9->10->11/-1/-1 [1] 11/-1/-1->10->9|9->10->11/-1/-1\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Trees [0] 10/-1/-1->9->8|8->9->10/-1/-1 [1] 10/0/-1->9->8|8->9->10/0/-1\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Trees [0] 12/-1/-1->11->10|10->11->12/-1/-1 [1] 12/-1/-1->11->10|10->11->12/-1/-1\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Trees [0] 15/-1/-1->14->13|13->14->15/-1/-1 [1] 15/-1/-1->14->13|13->14->15/-1/-1\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Trees [0] 13/-1/-1->12->11|11->12->13/-1/-1 [1] 13/-1/-1->12->11|11->12->13/-1/-1\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Trees [0] 14/-1/-1->13->12|12->13->14/-1/-1 [1] 14/-1/-1->13->12|12->13->14/-1/-1\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Trees [0] -1/-1/-1->15->14|14->15->-1/-1/-1 [1] -1/-1/-1->15->14|14->15->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Trees [0] 2/8/-1->1->0|0->1->2/8/-1 [1] 2/-1/-1->1->0|0->1->2/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Setting affinity for GPU 1 to 0fc00000,00000fc0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1|-1->0->1/-1/-1 [1] 1/-1/-1->0->9|9->0->1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Trees [0] 3/-1/-1->2->1|1->2->3/-1/-1 [1] 3/-1/-1->2->1|1->2->3/-1/-1\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Setting affinity for GPU 2 to 03,f0000000,0003f000\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Trees [0] 4/-1/-1->3->2|2->3->4/-1/-1 [1] 4/-1/-1->3->2|2->3->4/-1/-1\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Setting affinity for GPU 3 to fc,00000000,00fc0000\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Trees [0] 5/-1/-1->4->3|3->4->5/-1/-1 [1] 5/-1/-1->4->3|3->4->5/-1/-1\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Setting affinity for GPU 4 to 3f00,00000000,3f000000\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Trees [0] 6/-1/-1->5->4|4->5->6/-1/-1 [1] 6/-1/-1->5->4|4->5->6/-1/-1\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Setting affinity for GPU 5 to 0fc000,0000000f,c0000000\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Trees [0] 7/-1/-1->6->5|5->6->7/-1/-1 [1] 7/-1/-1->6->5|5->6->7/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Setting affinity for GPU 6 to 03f00000,000003f0,00000000\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Trees [0] -1/-1/-1->7->6|6->7->-1/-1/-1 [1] -1/-1/-1->7->6|6->7->-1/-1/-1\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Setting affinity for GPU 7 to fc000000,0000fc00,00000000\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/64\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Trees [0] 9/-1/-1->8->1|1->8->9/-1/-1 [1] 9/-1/-1->8->-1|-1->8->9/-1/-1\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Setting affinity for GPU 0 to 3f0000,0000003f\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 00 : 15[a01d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Channel 00 : 7[a01d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Channel 00 : 7[a01d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Channel 00 : 15[a01d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Channel 00 : 8[101c0] -> 1[101d0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,12]<stdout>:algo-2:287:8009 [4] NCCL INFO comm 0x7fbb9c3ac4c0 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,13]<stdout>:algo-2:289:8024 [5] NCCL INFO comm 0x7f12b0398c80 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Channel 01 : 15[a01d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,4]<stdout>:algo-1:284:8034 [4] NCCL INFO comm 0x7f8204389a80 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,5]<stdout>:algo-1:282:8031 [5] NCCL INFO comm 0x7f9050374960 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Channel 01 : 7[a01d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 01 : 15[a01d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Channel 01 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO Channel 01 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,11]<stdout>:algo-2:292:8030 [3] NCCL INFO comm 0x7fe8343aaec0 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,15]<stdout>:algo-2:291:8021 [7] NCCL INFO comm 0x7f33203989a0 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,14]<stdout>:algo-2:290:8011 [6] NCCL INFO comm 0x7f87bc3a5220 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Channel 00 : 8[101c0] -> 1[101d0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Channel 01 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO Channel 01 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,3]<stdout>:algo-1:278:8021 [3] NCCL INFO comm 0x7f57c4385c70 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,2]<stdout>:algo-1:280:8019 [2] NCCL INFO comm 0x7f4548387d00 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Channel 01 : 7[a01d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,10]<stdout>:algo-2:288:8027 [2] NCCL INFO comm 0x7efa243a5c40 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Channel 01 : 0[101c0] -> 9[101d0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,1]<stdout>:algo-1:283:8022 [1] NCCL INFO comm 0x7fdf84387e10 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,7]<stdout>:algo-1:279:8037 [7] NCCL INFO comm 0x7f40ec377be0 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,6]<stdout>:algo-1:281:8020 [6] NCCL INFO comm 0x7fdc28374500 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 01 : 0[101c0] -> 9[101d0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,8]<stdout>:algo-2:315:8010 [0] NCCL INFO comm 0x7fe26439f4e0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Channel 01 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO Channel 01 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO comm 0x7f955046f110 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,9]<stdout>:algo-2:293:8018 [1] NCCL INFO comm 0x7eee543ac220 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:algo-1:295:8040 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][50/924]#011lr: 0.00532, eta: 8:01:38, step time: 3.229, total_loss_bbox: 0.8226, class_loss: 0.4996, box_loss: 0.3230, mask_loss: 0.6935, total_rpn_loss: 0.1925, rpn_score_loss: 0.1351, rpn_box_loss: 0.0575, total_loss: 1.7086\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][100/924]#011lr: 0.00784, eta: 1:00:31, step time: 0.408, total_loss_bbox: 0.8322, class_loss: 0.4783, box_loss: 0.3539, mask_loss: 0.5808, total_rpn_loss: 0.1261, rpn_score_loss: 0.0749, rpn_box_loss: 0.0512, total_loss: 1.5392\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][150/924]#011lr: 0.01036, eta: 1:02:16, step time: 0.422, total_loss_bbox: 0.8577, class_loss: 0.5009, box_loss: 0.3568, mask_loss: 0.5152, total_rpn_loss: 0.1104, rpn_score_loss: 0.0591, rpn_box_loss: 0.0512, total_loss: 1.4832\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][200/924]#011lr: 0.01288, eta: 0:58:17, step time: 0.397, total_loss_bbox: 0.8682, class_loss: 0.5204, box_loss: 0.3479, mask_loss: 0.4678, total_rpn_loss: 0.0978, rpn_score_loss: 0.0512, rpn_box_loss: 0.0467, total_loss: 1.4339\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][250/924]#011lr: 0.01540, eta: 0:58:37, step time: 0.402, total_loss_bbox: 0.8201, class_loss: 0.4815, box_loss: 0.3386, mask_loss: 0.4533, total_rpn_loss: 0.0995, rpn_score_loss: 0.0511, rpn_box_loss: 0.0484, total_loss: 1.3729\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][300/924]#011lr: 0.01792, eta: 0:58:09, step time: 0.401, total_loss_bbox: 0.7808, class_loss: 0.4546, box_loss: 0.3262, mask_loss: 0.4285, total_rpn_loss: 0.0917, rpn_score_loss: 0.0466, rpn_box_loss: 0.0451, total_loss: 1.3010\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][350/924]#011lr: 0.02044, eta: 0:58:26, step time: 0.405, total_loss_bbox: 0.7655, class_loss: 0.4455, box_loss: 0.3200, mask_loss: 0.4163, total_rpn_loss: 0.0966, rpn_score_loss: 0.0503, rpn_box_loss: 0.0464, total_loss: 1.2784\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][400/924]#011lr: 0.02296, eta: 0:57:36, step time: 0.402, total_loss_bbox: 0.7550, class_loss: 0.4413, box_loss: 0.3137, mask_loss: 0.4133, total_rpn_loss: 0.0978, rpn_score_loss: 0.0527, rpn_box_loss: 0.0452, total_loss: 1.2661\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][450/924]#011lr: 0.02548, eta: 0:57:50, step time: 0.406, total_loss_bbox: 0.6999, class_loss: 0.3930, box_loss: 0.3069, mask_loss: 0.3861, total_rpn_loss: 0.0853, rpn_score_loss: 0.0416, rpn_box_loss: 0.0437, total_loss: 1.1714\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][500/924]#011lr: 0.02779, eta: 0:57:03, step time: 0.403, total_loss_bbox: 0.7030, class_loss: 0.4010, box_loss: 0.3020, mask_loss: 0.3893, total_rpn_loss: 0.0976, rpn_score_loss: 0.0505, rpn_box_loss: 0.0470, total_loss: 1.1899\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][550/924]#011lr: 0.02774, eta: 0:58:43, step time: 0.417, total_loss_bbox: 0.6499, class_loss: 0.3548, box_loss: 0.2951, mask_loss: 0.3703, total_rpn_loss: 0.0841, rpn_score_loss: 0.0386, rpn_box_loss: 0.0455, total_loss: 1.1043\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][600/924]#011lr: 0.02769, eta: 0:56:42, step time: 0.405, total_loss_bbox: 0.6634, class_loss: 0.3731, box_loss: 0.2903, mask_loss: 0.3704, total_rpn_loss: 0.0890, rpn_score_loss: 0.0422, rpn_box_loss: 0.0468, total_loss: 1.1228\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][650/924]#011lr: 0.02764, eta: 0:56:03, step time: 0.403, total_loss_bbox: 0.6408, class_loss: 0.3552, box_loss: 0.2856, mask_loss: 0.3669, total_rpn_loss: 0.0877, rpn_score_loss: 0.0436, rpn_box_loss: 0.0441, total_loss: 1.0954\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][700/924]#011lr: 0.02758, eta: 0:56:36, step time: 0.409, total_loss_bbox: 0.6426, class_loss: 0.3628, box_loss: 0.2798, mask_loss: 0.3650, total_rpn_loss: 0.0888, rpn_score_loss: 0.0439, rpn_box_loss: 0.0448, total_loss: 1.0964\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][750/924]#011lr: 0.02752, eta: 0:55:37, step time: 0.405, total_loss_bbox: 0.6234, class_loss: 0.3461, box_loss: 0.2772, mask_loss: 0.3548, total_rpn_loss: 0.0801, rpn_score_loss: 0.0392, rpn_box_loss: 0.0409, total_loss: 1.0582\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][800/924]#011lr: 0.02746, eta: 0:54:35, step time: 0.399, total_loss_bbox: 0.5963, class_loss: 0.3221, box_loss: 0.2742, mask_loss: 0.3439, total_rpn_loss: 0.0801, rpn_score_loss: 0.0383, rpn_box_loss: 0.0418, total_loss: 1.0204\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][850/924]#011lr: 0.02739, eta: 0:53:56, step time: 0.397, total_loss_bbox: 0.6115, class_loss: 0.3409, box_loss: 0.2706, mask_loss: 0.3496, total_rpn_loss: 0.0805, rpn_score_loss: 0.0379, rpn_box_loss: 0.0426, total_loss: 1.0416\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [1][900/924]#011lr: 0.02732, eta: 0:54:56, step time: 0.407, total_loss_bbox: 0.5968, class_loss: 0.3312, box_loss: 0.2656, mask_loss: 0.3421, total_rpn_loss: 0.0785, rpn_score_loss: 0.0372, rpn_box_loss: 0.0413, total_loss: 1.0174\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/001/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 2 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][50/924]#011lr: 0.02720, eta: 1:06:34, step time: 0.498, total_loss_bbox: 0.5862, class_loss: 0.3229, box_loss: 0.2633, mask_loss: 0.3397, total_rpn_loss: 0.0744, rpn_score_loss: 0.0343, rpn_box_loss: 0.0401, total_loss: 1.0003\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][100/924]#011lr: 0.02712, eta: 0:53:25, step time: 0.402, total_loss_bbox: 0.5747, class_loss: 0.3112, box_loss: 0.2635, mask_loss: 0.3354, total_rpn_loss: 0.0786, rpn_score_loss: 0.0375, rpn_box_loss: 0.0411, total_loss: 0.9886\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][150/924]#011lr: 0.02703, eta: 0:53:37, step time: 0.406, total_loss_bbox: 0.5615, class_loss: 0.3017, box_loss: 0.2598, mask_loss: 0.3295, total_rpn_loss: 0.0747, rpn_score_loss: 0.0358, rpn_box_loss: 0.0389, total_loss: 0.9657\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][200/924]#011lr: 0.02694, eta: 0:52:37, step time: 0.401, total_loss_bbox: 0.5923, class_loss: 0.3304, box_loss: 0.2619, mask_loss: 0.3294, total_rpn_loss: 0.0772, rpn_score_loss: 0.0365, rpn_box_loss: 0.0407, total_loss: 0.9990\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][250/924]#011lr: 0.02684, eta: 0:52:51, step time: 0.405, total_loss_bbox: 0.5668, class_loss: 0.3103, box_loss: 0.2565, mask_loss: 0.3243, total_rpn_loss: 0.0706, rpn_score_loss: 0.0313, rpn_box_loss: 0.0393, total_loss: 0.9617\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][300/924]#011lr: 0.02674, eta: 0:52:07, step time: 0.402, total_loss_bbox: 0.5603, class_loss: 0.3081, box_loss: 0.2522, mask_loss: 0.3273, total_rpn_loss: 0.0757, rpn_score_loss: 0.0352, rpn_box_loss: 0.0406, total_loss: 0.9633\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][350/924]#011lr: 0.02664, eta: 0:52:03, step time: 0.404, total_loss_bbox: 0.5672, class_loss: 0.3162, box_loss: 0.2509, mask_loss: 0.3299, total_rpn_loss: 0.0751, rpn_score_loss: 0.0361, rpn_box_loss: 0.0390, total_loss: 0.9721\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][400/924]#011lr: 0.02653, eta: 0:50:46, step time: 0.397, total_loss_bbox: 0.5523, class_loss: 0.3003, box_loss: 0.2520, mask_loss: 0.3268, total_rpn_loss: 0.0711, rpn_score_loss: 0.0320, rpn_box_loss: 0.0390, total_loss: 0.9502\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][450/924]#011lr: 0.02642, eta: 0:51:48, step time: 0.408, total_loss_bbox: 0.5536, class_loss: 0.3045, box_loss: 0.2491, mask_loss: 0.3185, total_rpn_loss: 0.0696, rpn_score_loss: 0.0328, rpn_box_loss: 0.0368, total_loss: 0.9417\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][500/924]#011lr: 0.02631, eta: 0:50:54, step time: 0.403, total_loss_bbox: 0.5376, class_loss: 0.2885, box_loss: 0.2490, mask_loss: 0.3306, total_rpn_loss: 0.0715, rpn_score_loss: 0.0316, rpn_box_loss: 0.0398, total_loss: 0.9396\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][550/924]#011lr: 0.02619, eta: 0:50:23, step time: 0.402, total_loss_bbox: 0.5502, class_loss: 0.3020, box_loss: 0.2481, mask_loss: 0.3193, total_rpn_loss: 0.0735, rpn_score_loss: 0.0348, rpn_box_loss: 0.0387, total_loss: 0.9430\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][600/924]#011lr: 0.02607, eta: 0:49:53, step time: 0.400, total_loss_bbox: 0.5574, class_loss: 0.3060, box_loss: 0.2514, mask_loss: 0.3312, total_rpn_loss: 0.0728, rpn_score_loss: 0.0333, rpn_box_loss: 0.0395, total_loss: 0.9614\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][650/924]#011lr: 0.02594, eta: 0:50:45, step time: 0.410, total_loss_bbox: 0.5588, class_loss: 0.3137, box_loss: 0.2451, mask_loss: 0.3125, total_rpn_loss: 0.0744, rpn_score_loss: 0.0346, rpn_box_loss: 0.0398, total_loss: 0.9458\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][700/924]#011lr: 0.02581, eta: 0:49:55, step time: 0.406, total_loss_bbox: 0.5388, class_loss: 0.2912, box_loss: 0.2475, mask_loss: 0.3228, total_rpn_loss: 0.0761, rpn_score_loss: 0.0361, rpn_box_loss: 0.0400, total_loss: 0.9376\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][750/924]#011lr: 0.02568, eta: 0:49:18, step time: 0.404, total_loss_bbox: 0.5715, class_loss: 0.3270, box_loss: 0.2445, mask_loss: 0.3114, total_rpn_loss: 0.0730, rpn_score_loss: 0.0346, rpn_box_loss: 0.0384, total_loss: 0.9559\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][800/924]#011lr: 0.02554, eta: 0:49:10, step time: 0.405, total_loss_bbox: 0.5436, class_loss: 0.3018, box_loss: 0.2418, mask_loss: 0.3251, total_rpn_loss: 0.0755, rpn_score_loss: 0.0372, rpn_box_loss: 0.0383, total_loss: 0.9442\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][850/924]#011lr: 0.02540, eta: 0:47:23, step time: 0.394, total_loss_bbox: 0.5420, class_loss: 0.3008, box_loss: 0.2412, mask_loss: 0.3149, total_rpn_loss: 0.0657, rpn_score_loss: 0.0279, rpn_box_loss: 0.0378, total_loss: 0.9226\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [2][900/924]#011lr: 0.02526, eta: 0:48:46, step time: 0.408, total_loss_bbox: 0.5490, class_loss: 0.3081, box_loss: 0.2409, mask_loss: 0.3206, total_rpn_loss: 0.0702, rpn_score_loss: 0.0334, rpn_box_loss: 0.0369, total_loss: 0.9399\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/002/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 3 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][50/924]#011lr: 0.02504, eta: 0:50:28, step time: 0.426, total_loss_bbox: 0.5238, class_loss: 0.2860, box_loss: 0.2378, mask_loss: 0.3096, total_rpn_loss: 0.0630, rpn_score_loss: 0.0268, rpn_box_loss: 0.0363, total_loss: 0.8965\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][100/924]#011lr: 0.02489, eta: 0:46:29, step time: 0.396, total_loss_bbox: 0.5061, class_loss: 0.2749, box_loss: 0.2312, mask_loss: 0.3082, total_rpn_loss: 0.0635, rpn_score_loss: 0.0263, rpn_box_loss: 0.0373, total_loss: 0.8779\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][150/924]#011lr: 0.02473, eta: 0:47:45, step time: 0.409, total_loss_bbox: 0.5198, class_loss: 0.2856, box_loss: 0.2342, mask_loss: 0.3044, total_rpn_loss: 0.0663, rpn_score_loss: 0.0289, rpn_box_loss: 0.0374, total_loss: 0.8905\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][200/924]#011lr: 0.02458, eta: 0:47:48, step time: 0.413, total_loss_bbox: 0.5188, class_loss: 0.2842, box_loss: 0.2346, mask_loss: 0.3050, total_rpn_loss: 0.0681, rpn_score_loss: 0.0323, rpn_box_loss: 0.0358, total_loss: 0.8919\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][250/924]#011lr: 0.02441, eta: 0:46:43, step time: 0.406, total_loss_bbox: 0.5427, class_loss: 0.3049, box_loss: 0.2378, mask_loss: 0.3115, total_rpn_loss: 0.0694, rpn_score_loss: 0.0309, rpn_box_loss: 0.0385, total_loss: 0.9236\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][300/924]#011lr: 0.02425, eta: 0:45:44, step time: 0.401, total_loss_bbox: 0.5302, class_loss: 0.2942, box_loss: 0.2360, mask_loss: 0.3106, total_rpn_loss: 0.0657, rpn_score_loss: 0.0297, rpn_box_loss: 0.0360, total_loss: 0.9065\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][350/924]#011lr: 0.02408, eta: 0:46:45, step time: 0.412, total_loss_bbox: 0.5120, class_loss: 0.2811, box_loss: 0.2309, mask_loss: 0.3031, total_rpn_loss: 0.0637, rpn_score_loss: 0.0296, rpn_box_loss: 0.0342, total_loss: 0.8788\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][400/924]#011lr: 0.02391, eta: 0:45:14, step time: 0.402, total_loss_bbox: 0.5401, class_loss: 0.3047, box_loss: 0.2354, mask_loss: 0.3103, total_rpn_loss: 0.0710, rpn_score_loss: 0.0342, rpn_box_loss: 0.0367, total_loss: 0.9213\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][450/924]#011lr: 0.02374, eta: 0:45:26, step time: 0.407, total_loss_bbox: 0.5073, class_loss: 0.2763, box_loss: 0.2310, mask_loss: 0.3001, total_rpn_loss: 0.0640, rpn_score_loss: 0.0286, rpn_box_loss: 0.0354, total_loss: 0.8714\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][500/924]#011lr: 0.02356, eta: 0:44:55, step time: 0.405, total_loss_bbox: 0.5134, class_loss: 0.2791, box_loss: 0.2343, mask_loss: 0.3084, total_rpn_loss: 0.0652, rpn_score_loss: 0.0304, rpn_box_loss: 0.0347, total_loss: 0.8869\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][550/924]#011lr: 0.02338, eta: 0:44:17, step time: 0.402, total_loss_bbox: 0.4942, class_loss: 0.2696, box_loss: 0.2246, mask_loss: 0.2885, total_rpn_loss: 0.0629, rpn_score_loss: 0.0255, rpn_box_loss: 0.0374, total_loss: 0.8457\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][600/924]#011lr: 0.02320, eta: 0:43:57, step time: 0.402, total_loss_bbox: 0.5057, class_loss: 0.2775, box_loss: 0.2282, mask_loss: 0.2986, total_rpn_loss: 0.0673, rpn_score_loss: 0.0312, rpn_box_loss: 0.0361, total_loss: 0.8716\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][650/924]#011lr: 0.02301, eta: 0:44:49, step time: 0.414, total_loss_bbox: 0.5091, class_loss: 0.2820, box_loss: 0.2271, mask_loss: 0.3033, total_rpn_loss: 0.0651, rpn_score_loss: 0.0311, rpn_box_loss: 0.0340, total_loss: 0.8776\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][700/924]#011lr: 0.02282, eta: 0:43:51, step time: 0.408, total_loss_bbox: 0.5072, class_loss: 0.2763, box_loss: 0.2309, mask_loss: 0.3032, total_rpn_loss: 0.0650, rpn_score_loss: 0.0296, rpn_box_loss: 0.0354, total_loss: 0.8755\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][750/924]#011lr: 0.02263, eta: 0:43:26, step time: 0.407, total_loss_bbox: 0.5080, class_loss: 0.2812, box_loss: 0.2268, mask_loss: 0.2967, total_rpn_loss: 0.0621, rpn_score_loss: 0.0295, rpn_box_loss: 0.0326, total_loss: 0.8668\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][800/924]#011lr: 0.02244, eta: 0:42:17, step time: 0.400, total_loss_bbox: 0.5201, class_loss: 0.2926, box_loss: 0.2275, mask_loss: 0.3026, total_rpn_loss: 0.0668, rpn_score_loss: 0.0312, rpn_box_loss: 0.0356, total_loss: 0.8895\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][850/924]#011lr: 0.02224, eta: 0:42:51, step time: 0.408, total_loss_bbox: 0.4977, class_loss: 0.2733, box_loss: 0.2244, mask_loss: 0.2957, total_rpn_loss: 0.0600, rpn_score_loss: 0.0261, rpn_box_loss: 0.0339, total_loss: 0.8534\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [3][900/924]#011lr: 0.02204, eta: 0:43:07, step time: 0.414, total_loss_bbox: 0.5014, class_loss: 0.2726, box_loss: 0.2288, mask_loss: 0.2998, total_rpn_loss: 0.0617, rpn_score_loss: 0.0270, rpn_box_loss: 0.0347, total_loss: 0.8628\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/003/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 4 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][50/924]#011lr: 0.02175, eta: 0:44:09, step time: 0.429, total_loss_bbox: 0.4856, class_loss: 0.2629, box_loss: 0.2227, mask_loss: 0.2938, total_rpn_loss: 0.0593, rpn_score_loss: 0.0265, rpn_box_loss: 0.0328, total_loss: 0.8387\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][100/924]#011lr: 0.02154, eta: 0:41:13, step time: 0.404, total_loss_bbox: 0.4826, class_loss: 0.2568, box_loss: 0.2258, mask_loss: 0.2973, total_rpn_loss: 0.0593, rpn_score_loss: 0.0256, rpn_box_loss: 0.0337, total_loss: 0.8392\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][150/924]#011lr: 0.02133, eta: 0:40:51, step time: 0.403, total_loss_bbox: 0.4998, class_loss: 0.2800, box_loss: 0.2198, mask_loss: 0.2910, total_rpn_loss: 0.0604, rpn_score_loss: 0.0269, rpn_box_loss: 0.0335, total_loss: 0.8512\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][200/924]#011lr: 0.02113, eta: 0:40:54, step time: 0.407, total_loss_bbox: 0.4832, class_loss: 0.2601, box_loss: 0.2230, mask_loss: 0.2911, total_rpn_loss: 0.0615, rpn_score_loss: 0.0263, rpn_box_loss: 0.0352, total_loss: 0.8358\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][250/924]#011lr: 0.02091, eta: 0:40:22, step time: 0.405, total_loss_bbox: 0.4964, class_loss: 0.2690, box_loss: 0.2274, mask_loss: 0.2933, total_rpn_loss: 0.0615, rpn_score_loss: 0.0271, rpn_box_loss: 0.0344, total_loss: 0.8512\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][300/924]#011lr: 0.02070, eta: 0:40:11, step time: 0.407, total_loss_bbox: 0.4884, class_loss: 0.2675, box_loss: 0.2209, mask_loss: 0.2950, total_rpn_loss: 0.0605, rpn_score_loss: 0.0263, rpn_box_loss: 0.0342, total_loss: 0.8439\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][350/924]#011lr: 0.02049, eta: 0:39:43, step time: 0.406, total_loss_bbox: 0.4846, class_loss: 0.2655, box_loss: 0.2191, mask_loss: 0.2939, total_rpn_loss: 0.0609, rpn_score_loss: 0.0273, rpn_box_loss: 0.0336, total_loss: 0.8394\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][400/924]#011lr: 0.02027, eta: 0:39:56, step time: 0.411, total_loss_bbox: 0.4892, class_loss: 0.2671, box_loss: 0.2222, mask_loss: 0.2976, total_rpn_loss: 0.0602, rpn_score_loss: 0.0269, rpn_box_loss: 0.0333, total_loss: 0.8470\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][450/924]#011lr: 0.02005, eta: 0:38:41, step time: 0.402, total_loss_bbox: 0.4767, class_loss: 0.2602, box_loss: 0.2166, mask_loss: 0.2784, total_rpn_loss: 0.0569, rpn_score_loss: 0.0242, rpn_box_loss: 0.0327, total_loss: 0.8120\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][500/924]#011lr: 0.01983, eta: 0:38:47, step time: 0.406, total_loss_bbox: 0.4980, class_loss: 0.2784, box_loss: 0.2196, mask_loss: 0.2987, total_rpn_loss: 0.0609, rpn_score_loss: 0.0276, rpn_box_loss: 0.0332, total_loss: 0.8576\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][550/924]#011lr: 0.01960, eta: 0:38:29, step time: 0.407, total_loss_bbox: 0.4876, class_loss: 0.2685, box_loss: 0.2191, mask_loss: 0.2913, total_rpn_loss: 0.0593, rpn_score_loss: 0.0254, rpn_box_loss: 0.0340, total_loss: 0.8383\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][600/924]#011lr: 0.01938, eta: 0:38:09, step time: 0.407, total_loss_bbox: 0.4674, class_loss: 0.2533, box_loss: 0.2141, mask_loss: 0.2818, total_rpn_loss: 0.0599, rpn_score_loss: 0.0259, rpn_box_loss: 0.0340, total_loss: 0.8091\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][650/924]#011lr: 0.01915, eta: 0:37:59, step time: 0.409, total_loss_bbox: 0.4844, class_loss: 0.2701, box_loss: 0.2143, mask_loss: 0.2861, total_rpn_loss: 0.0581, rpn_score_loss: 0.0260, rpn_box_loss: 0.0321, total_loss: 0.8286\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][700/924]#011lr: 0.01893, eta: 0:37:09, step time: 0.403, total_loss_bbox: 0.4856, class_loss: 0.2685, box_loss: 0.2170, mask_loss: 0.2819, total_rpn_loss: 0.0633, rpn_score_loss: 0.0296, rpn_box_loss: 0.0337, total_loss: 0.8308\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][750/924]#011lr: 0.01870, eta: 0:37:45, step time: 0.414, total_loss_bbox: 0.4897, class_loss: 0.2723, box_loss: 0.2174, mask_loss: 0.2898, total_rpn_loss: 0.0634, rpn_score_loss: 0.0296, rpn_box_loss: 0.0339, total_loss: 0.8430\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][800/924]#011lr: 0.01847, eta: 0:36:26, step time: 0.403, total_loss_bbox: 0.4769, class_loss: 0.2641, box_loss: 0.2129, mask_loss: 0.2808, total_rpn_loss: 0.0533, rpn_score_loss: 0.0236, rpn_box_loss: 0.0296, total_loss: 0.8110\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][850/924]#011lr: 0.01823, eta: 0:35:56, step time: 0.401, total_loss_bbox: 0.4904, class_loss: 0.2718, box_loss: 0.2187, mask_loss: 0.2955, total_rpn_loss: 0.0555, rpn_score_loss: 0.0236, rpn_box_loss: 0.0319, total_loss: 0.8415\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [4][900/924]#011lr: 0.01800, eta: 0:36:10, step time: 0.407, total_loss_bbox: 0.4878, class_loss: 0.2739, box_loss: 0.2139, mask_loss: 0.2912, total_rpn_loss: 0.0592, rpn_score_loss: 0.0266, rpn_box_loss: 0.0326, total_loss: 0.8383\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/004/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 5 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][50/924]#011lr: 0.01765, eta: 0:38:00, step time: 0.434, total_loss_bbox: 0.4480, class_loss: 0.2401, box_loss: 0.2079, mask_loss: 0.2705, total_rpn_loss: 0.0536, rpn_score_loss: 0.0228, rpn_box_loss: 0.0308, total_loss: 0.7721\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][100/924]#011lr: 0.01742, eta: 0:35:11, step time: 0.406, total_loss_bbox: 0.4672, class_loss: 0.2557, box_loss: 0.2115, mask_loss: 0.2790, total_rpn_loss: 0.0558, rpn_score_loss: 0.0244, rpn_box_loss: 0.0314, total_loss: 0.8019\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][150/924]#011lr: 0.01718, eta: 0:34:41, step time: 0.404, total_loss_bbox: 0.4814, class_loss: 0.2683, box_loss: 0.2130, mask_loss: 0.2866, total_rpn_loss: 0.0580, rpn_score_loss: 0.0263, rpn_box_loss: 0.0316, total_loss: 0.8260\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][200/924]#011lr: 0.01694, eta: 0:34:38, step time: 0.407, total_loss_bbox: 0.4702, class_loss: 0.2590, box_loss: 0.2112, mask_loss: 0.2839, total_rpn_loss: 0.0578, rpn_score_loss: 0.0239, rpn_box_loss: 0.0339, total_loss: 0.8120\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][250/924]#011lr: 0.01670, eta: 0:34:03, step time: 0.404, total_loss_bbox: 0.4699, class_loss: 0.2568, box_loss: 0.2132, mask_loss: 0.2875, total_rpn_loss: 0.0570, rpn_score_loss: 0.0249, rpn_box_loss: 0.0321, total_loss: 0.8144\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][300/924]#011lr: 0.01646, eta: 0:33:33, step time: 0.402, total_loss_bbox: 0.4730, class_loss: 0.2611, box_loss: 0.2119, mask_loss: 0.2811, total_rpn_loss: 0.0568, rpn_score_loss: 0.0257, rpn_box_loss: 0.0312, total_loss: 0.8110\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][350/924]#011lr: 0.01622, eta: 0:33:13, step time: 0.402, total_loss_bbox: 0.4544, class_loss: 0.2443, box_loss: 0.2100, mask_loss: 0.2832, total_rpn_loss: 0.0545, rpn_score_loss: 0.0229, rpn_box_loss: 0.0315, total_loss: 0.7921\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][400/924]#011lr: 0.01598, eta: 0:32:49, step time: 0.402, total_loss_bbox: 0.4517, class_loss: 0.2428, box_loss: 0.2090, mask_loss: 0.2789, total_rpn_loss: 0.0559, rpn_score_loss: 0.0235, rpn_box_loss: 0.0324, total_loss: 0.7865\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][450/924]#011lr: 0.01574, eta: 0:32:50, step time: 0.406, total_loss_bbox: 0.4841, class_loss: 0.2710, box_loss: 0.2131, mask_loss: 0.2823, total_rpn_loss: 0.0588, rpn_score_loss: 0.0267, rpn_box_loss: 0.0321, total_loss: 0.8251\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][500/924]#011lr: 0.01550, eta: 0:32:08, step time: 0.402, total_loss_bbox: 0.4767, class_loss: 0.2624, box_loss: 0.2142, mask_loss: 0.2857, total_rpn_loss: 0.0570, rpn_score_loss: 0.0258, rpn_box_loss: 0.0312, total_loss: 0.8195\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][550/924]#011lr: 0.01525, eta: 0:32:02, step time: 0.404, total_loss_bbox: 0.4542, class_loss: 0.2456, box_loss: 0.2086, mask_loss: 0.2796, total_rpn_loss: 0.0530, rpn_score_loss: 0.0229, rpn_box_loss: 0.0302, total_loss: 0.7868\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][600/924]#011lr: 0.01501, eta: 0:31:20, step time: 0.400, total_loss_bbox: 0.4531, class_loss: 0.2454, box_loss: 0.2077, mask_loss: 0.2798, total_rpn_loss: 0.0516, rpn_score_loss: 0.0214, rpn_box_loss: 0.0301, total_loss: 0.7845\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][650/924]#011lr: 0.01477, eta: 0:31:20, step time: 0.404, total_loss_bbox: 0.4550, class_loss: 0.2498, box_loss: 0.2053, mask_loss: 0.2700, total_rpn_loss: 0.0531, rpn_score_loss: 0.0239, rpn_box_loss: 0.0292, total_loss: 0.7781\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][700/924]#011lr: 0.01452, eta: 0:30:55, step time: 0.403, total_loss_bbox: 0.4500, class_loss: 0.2408, box_loss: 0.2092, mask_loss: 0.2833, total_rpn_loss: 0.0536, rpn_score_loss: 0.0221, rpn_box_loss: 0.0315, total_loss: 0.7869\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][750/924]#011lr: 0.01428, eta: 0:30:51, step time: 0.406, total_loss_bbox: 0.4514, class_loss: 0.2442, box_loss: 0.2072, mask_loss: 0.2742, total_rpn_loss: 0.0544, rpn_score_loss: 0.0222, rpn_box_loss: 0.0322, total_loss: 0.7800\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][800/924]#011lr: 0.01403, eta: 0:30:42, step time: 0.409, total_loss_bbox: 0.4620, class_loss: 0.2554, box_loss: 0.2066, mask_loss: 0.2869, total_rpn_loss: 0.0564, rpn_score_loss: 0.0254, rpn_box_loss: 0.0309, total_loss: 0.8052\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][850/924]#011lr: 0.01379, eta: 0:30:12, step time: 0.407, total_loss_bbox: 0.4703, class_loss: 0.2589, box_loss: 0.2113, mask_loss: 0.2883, total_rpn_loss: 0.0556, rpn_score_loss: 0.0246, rpn_box_loss: 0.0311, total_loss: 0.8142\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [5][900/924]#011lr: 0.01355, eta: 0:29:32, step time: 0.403, total_loss_bbox: 0.4528, class_loss: 0.2433, box_loss: 0.2094, mask_loss: 0.2792, total_rpn_loss: 0.0511, rpn_score_loss: 0.0225, rpn_box_loss: 0.0286, total_loss: 0.7830\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/005/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 6 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][50/924]#011lr: 0.01318, eta: 0:30:47, step time: 0.427, total_loss_bbox: 0.4381, class_loss: 0.2373, box_loss: 0.2008, mask_loss: 0.2667, total_rpn_loss: 0.0494, rpn_score_loss: 0.0215, rpn_box_loss: 0.0279, total_loss: 0.7542\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][100/924]#011lr: 0.01294, eta: 0:29:01, step time: 0.407, total_loss_bbox: 0.4416, class_loss: 0.2364, box_loss: 0.2052, mask_loss: 0.2740, total_rpn_loss: 0.0530, rpn_score_loss: 0.0217, rpn_box_loss: 0.0313, total_loss: 0.7686\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][150/924]#011lr: 0.01270, eta: 0:28:50, step time: 0.409, total_loss_bbox: 0.4502, class_loss: 0.2433, box_loss: 0.2069, mask_loss: 0.2750, total_rpn_loss: 0.0516, rpn_score_loss: 0.0210, rpn_box_loss: 0.0305, total_loss: 0.7768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][200/924]#011lr: 0.01245, eta: 0:28:11, step time: 0.405, total_loss_bbox: 0.4644, class_loss: 0.2558, box_loss: 0.2086, mask_loss: 0.2800, total_rpn_loss: 0.0536, rpn_score_loss: 0.0238, rpn_box_loss: 0.0298, total_loss: 0.7981\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][250/924]#011lr: 0.01221, eta: 0:27:44, step time: 0.403, total_loss_bbox: 0.4442, class_loss: 0.2404, box_loss: 0.2038, mask_loss: 0.2746, total_rpn_loss: 0.0509, rpn_score_loss: 0.0228, rpn_box_loss: 0.0281, total_loss: 0.7697\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][300/924]#011lr: 0.01197, eta: 0:27:53, step time: 0.410, total_loss_bbox: 0.4334, class_loss: 0.2322, box_loss: 0.2012, mask_loss: 0.2740, total_rpn_loss: 0.0494, rpn_score_loss: 0.0211, rpn_box_loss: 0.0282, total_loss: 0.7568\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][350/924]#011lr: 0.01173, eta: 0:27:09, step time: 0.404, total_loss_bbox: 0.4413, class_loss: 0.2356, box_loss: 0.2057, mask_loss: 0.2779, total_rpn_loss: 0.0508, rpn_score_loss: 0.0216, rpn_box_loss: 0.0292, total_loss: 0.7700\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][400/924]#011lr: 0.01149, eta: 0:26:50, step time: 0.405, total_loss_bbox: 0.4519, class_loss: 0.2478, box_loss: 0.2041, mask_loss: 0.2756, total_rpn_loss: 0.0507, rpn_score_loss: 0.0217, rpn_box_loss: 0.0291, total_loss: 0.7782\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][450/924]#011lr: 0.01125, eta: 0:27:13, step time: 0.416, total_loss_bbox: 0.4412, class_loss: 0.2402, box_loss: 0.2010, mask_loss: 0.2677, total_rpn_loss: 0.0496, rpn_score_loss: 0.0210, rpn_box_loss: 0.0287, total_loss: 0.7586\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][500/924]#011lr: 0.01101, eta: 0:26:12, step time: 0.405, total_loss_bbox: 0.4291, class_loss: 0.2293, box_loss: 0.1999, mask_loss: 0.2672, total_rpn_loss: 0.0501, rpn_score_loss: 0.0220, rpn_box_loss: 0.0281, total_loss: 0.7464\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][550/924]#011lr: 0.01077, eta: 0:25:34, step time: 0.401, total_loss_bbox: 0.4227, class_loss: 0.2272, box_loss: 0.1955, mask_loss: 0.2710, total_rpn_loss: 0.0464, rpn_score_loss: 0.0188, rpn_box_loss: 0.0276, total_loss: 0.7400\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][600/924]#011lr: 0.01054, eta: 0:25:27, step time: 0.404, total_loss_bbox: 0.4252, class_loss: 0.2247, box_loss: 0.2005, mask_loss: 0.2672, total_rpn_loss: 0.0480, rpn_score_loss: 0.0201, rpn_box_loss: 0.0279, total_loss: 0.7404\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][650/924]#011lr: 0.01030, eta: 0:24:58, step time: 0.402, total_loss_bbox: 0.4424, class_loss: 0.2399, box_loss: 0.2025, mask_loss: 0.2776, total_rpn_loss: 0.0510, rpn_score_loss: 0.0223, rpn_box_loss: 0.0287, total_loss: 0.7710\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][700/924]#011lr: 0.01007, eta: 0:25:18, step time: 0.413, total_loss_bbox: 0.4439, class_loss: 0.2444, box_loss: 0.1995, mask_loss: 0.2689, total_rpn_loss: 0.0514, rpn_score_loss: 0.0227, rpn_box_loss: 0.0287, total_loss: 0.7642\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][750/924]#011lr: 0.00983, eta: 0:24:54, step time: 0.412, total_loss_bbox: 0.4353, class_loss: 0.2325, box_loss: 0.2028, mask_loss: 0.2686, total_rpn_loss: 0.0477, rpn_score_loss: 0.0202, rpn_box_loss: 0.0275, total_loss: 0.7516\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][800/924]#011lr: 0.00960, eta: 0:23:51, step time: 0.400, total_loss_bbox: 0.4319, class_loss: 0.2339, box_loss: 0.1979, mask_loss: 0.2691, total_rpn_loss: 0.0481, rpn_score_loss: 0.0198, rpn_box_loss: 0.0282, total_loss: 0.7490\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][850/924]#011lr: 0.00937, eta: 0:23:54, step time: 0.406, total_loss_bbox: 0.4328, class_loss: 0.2355, box_loss: 0.1973, mask_loss: 0.2621, total_rpn_loss: 0.0494, rpn_score_loss: 0.0218, rpn_box_loss: 0.0275, total_loss: 0.7443\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [6][900/924]#011lr: 0.00914, eta: 0:23:12, step time: 0.400, total_loss_bbox: 0.4306, class_loss: 0.2324, box_loss: 0.1982, mask_loss: 0.2742, total_rpn_loss: 0.0486, rpn_score_loss: 0.0202, rpn_box_loss: 0.0285, total_loss: 0.7535\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/006/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 7 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][50/924]#011lr: 0.00880, eta: 0:24:14, step time: 0.427, total_loss_bbox: 0.4212, class_loss: 0.2282, box_loss: 0.1930, mask_loss: 0.2626, total_rpn_loss: 0.0475, rpn_score_loss: 0.0192, rpn_box_loss: 0.0283, total_loss: 0.7313\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][100/924]#011lr: 0.00858, eta: 0:22:23, step time: 0.400, total_loss_bbox: 0.4400, class_loss: 0.2433, box_loss: 0.1967, mask_loss: 0.2705, total_rpn_loss: 0.0488, rpn_score_loss: 0.0210, rpn_box_loss: 0.0278, total_loss: 0.7593\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][150/924]#011lr: 0.00835, eta: 0:22:06, step time: 0.401, total_loss_bbox: 0.4414, class_loss: 0.2418, box_loss: 0.1996, mask_loss: 0.2661, total_rpn_loss: 0.0474, rpn_score_loss: 0.0201, rpn_box_loss: 0.0273, total_loss: 0.7550\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][200/924]#011lr: 0.00813, eta: 0:22:07, step time: 0.408, total_loss_bbox: 0.4253, class_loss: 0.2274, box_loss: 0.1978, mask_loss: 0.2660, total_rpn_loss: 0.0481, rpn_score_loss: 0.0204, rpn_box_loss: 0.0277, total_loss: 0.7394\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][250/924]#011lr: 0.00791, eta: 0:21:46, step time: 0.407, total_loss_bbox: 0.4106, class_loss: 0.2178, box_loss: 0.1928, mask_loss: 0.2637, total_rpn_loss: 0.0479, rpn_score_loss: 0.0208, rpn_box_loss: 0.0271, total_loss: 0.7223\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][300/924]#011lr: 0.00769, eta: 0:21:15, step time: 0.404, total_loss_bbox: 0.4072, class_loss: 0.2147, box_loss: 0.1925, mask_loss: 0.2591, total_rpn_loss: 0.0457, rpn_score_loss: 0.0193, rpn_box_loss: 0.0264, total_loss: 0.7120\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][350/924]#011lr: 0.00747, eta: 0:21:05, step time: 0.407, total_loss_bbox: 0.4191, class_loss: 0.2230, box_loss: 0.1961, mask_loss: 0.2668, total_rpn_loss: 0.0443, rpn_score_loss: 0.0184, rpn_box_loss: 0.0259, total_loss: 0.7302\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][400/924]#011lr: 0.00726, eta: 0:20:46, step time: 0.408, total_loss_bbox: 0.4173, class_loss: 0.2208, box_loss: 0.1965, mask_loss: 0.2645, total_rpn_loss: 0.0441, rpn_score_loss: 0.0167, rpn_box_loss: 0.0275, total_loss: 0.7260\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][450/924]#011lr: 0.00705, eta: 0:20:18, step time: 0.405, total_loss_bbox: 0.4270, class_loss: 0.2289, box_loss: 0.1981, mask_loss: 0.2740, total_rpn_loss: 0.0462, rpn_score_loss: 0.0194, rpn_box_loss: 0.0268, total_loss: 0.7472\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][500/924]#011lr: 0.00684, eta: 0:19:47, step time: 0.402, total_loss_bbox: 0.3971, class_loss: 0.2081, box_loss: 0.1890, mask_loss: 0.2521, total_rpn_loss: 0.0461, rpn_score_loss: 0.0184, rpn_box_loss: 0.0276, total_loss: 0.6952\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][550/924]#011lr: 0.00663, eta: 0:19:26, step time: 0.402, total_loss_bbox: 0.4282, class_loss: 0.2289, box_loss: 0.1993, mask_loss: 0.2684, total_rpn_loss: 0.0459, rpn_score_loss: 0.0184, rpn_box_loss: 0.0275, total_loss: 0.7425\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][600/924]#011lr: 0.00642, eta: 0:19:00, step time: 0.399, total_loss_bbox: 0.4044, class_loss: 0.2113, box_loss: 0.1931, mask_loss: 0.2590, total_rpn_loss: 0.0437, rpn_score_loss: 0.0174, rpn_box_loss: 0.0263, total_loss: 0.7070\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][650/924]#011lr: 0.00622, eta: 0:19:07, step time: 0.409, total_loss_bbox: 0.3994, class_loss: 0.2099, box_loss: 0.1895, mask_loss: 0.2599, total_rpn_loss: 0.0436, rpn_score_loss: 0.0176, rpn_box_loss: 0.0260, total_loss: 0.7029\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][700/924]#011lr: 0.00602, eta: 0:18:39, step time: 0.406, total_loss_bbox: 0.4201, class_loss: 0.2213, box_loss: 0.1988, mask_loss: 0.2714, total_rpn_loss: 0.0468, rpn_score_loss: 0.0204, rpn_box_loss: 0.0264, total_loss: 0.7383\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][750/924]#011lr: 0.00582, eta: 0:18:14, step time: 0.404, total_loss_bbox: 0.4237, class_loss: 0.2315, box_loss: 0.1922, mask_loss: 0.2664, total_rpn_loss: 0.0483, rpn_score_loss: 0.0229, rpn_box_loss: 0.0254, total_loss: 0.7385\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][800/924]#011lr: 0.00562, eta: 0:17:48, step time: 0.402, total_loss_bbox: 0.4099, class_loss: 0.2133, box_loss: 0.1967, mask_loss: 0.2679, total_rpn_loss: 0.0433, rpn_score_loss: 0.0176, rpn_box_loss: 0.0258, total_loss: 0.7212\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][850/924]#011lr: 0.00543, eta: 0:17:29, step time: 0.403, total_loss_bbox: 0.4178, class_loss: 0.2227, box_loss: 0.1951, mask_loss: 0.2633, total_rpn_loss: 0.0439, rpn_score_loss: 0.0189, rpn_box_loss: 0.0249, total_loss: 0.7249\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [7][900/924]#011lr: 0.00524, eta: 0:17:25, step time: 0.409, total_loss_bbox: 0.4052, class_loss: 0.2146, box_loss: 0.1905, mask_loss: 0.2579, total_rpn_loss: 0.0435, rpn_score_loss: 0.0178, rpn_box_loss: 0.0257, total_loss: 0.7065\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/007/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 8 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][50/924]#011lr: 0.00496, eta: 0:17:31, step time: 0.424, total_loss_bbox: 0.3943, class_loss: 0.2070, box_loss: 0.1873, mask_loss: 0.2520, total_rpn_loss: 0.0437, rpn_score_loss: 0.0174, rpn_box_loss: 0.0262, total_loss: 0.6900\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][100/924]#011lr: 0.00477, eta: 0:16:19, step time: 0.403, total_loss_bbox: 0.4022, class_loss: 0.2129, box_loss: 0.1893, mask_loss: 0.2588, total_rpn_loss: 0.0410, rpn_score_loss: 0.0167, rpn_box_loss: 0.0242, total_loss: 0.7019\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][150/924]#011lr: 0.00459, eta: 0:16:05, step time: 0.405, total_loss_bbox: 0.3856, class_loss: 0.1982, box_loss: 0.1874, mask_loss: 0.2470, total_rpn_loss: 0.0414, rpn_score_loss: 0.0166, rpn_box_loss: 0.0248, total_loss: 0.6740\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][200/924]#011lr: 0.00441, eta: 0:15:52, step time: 0.408, total_loss_bbox: 0.3970, class_loss: 0.2085, box_loss: 0.1886, mask_loss: 0.2581, total_rpn_loss: 0.0424, rpn_score_loss: 0.0170, rpn_box_loss: 0.0254, total_loss: 0.6975\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][250/924]#011lr: 0.00424, eta: 0:15:33, step time: 0.409, total_loss_bbox: 0.4018, class_loss: 0.2121, box_loss: 0.1898, mask_loss: 0.2601, total_rpn_loss: 0.0441, rpn_score_loss: 0.0186, rpn_box_loss: 0.0255, total_loss: 0.7061\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][300/924]#011lr: 0.00406, eta: 0:14:55, step time: 0.401, total_loss_bbox: 0.3934, class_loss: 0.2076, box_loss: 0.1858, mask_loss: 0.2505, total_rpn_loss: 0.0422, rpn_score_loss: 0.0183, rpn_box_loss: 0.0239, total_loss: 0.6861\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][350/924]#011lr: 0.00389, eta: 0:14:39, step time: 0.403, total_loss_bbox: 0.3866, class_loss: 0.2008, box_loss: 0.1858, mask_loss: 0.2513, total_rpn_loss: 0.0411, rpn_score_loss: 0.0169, rpn_box_loss: 0.0242, total_loss: 0.6791\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][400/924]#011lr: 0.00373, eta: 0:14:26, step time: 0.407, total_loss_bbox: 0.4196, class_loss: 0.2268, box_loss: 0.1927, mask_loss: 0.2650, total_rpn_loss: 0.0420, rpn_score_loss: 0.0172, rpn_box_loss: 0.0248, total_loss: 0.7266\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][450/924]#011lr: 0.00356, eta: 0:14:01, step time: 0.404, total_loss_bbox: 0.4026, class_loss: 0.2134, box_loss: 0.1891, mask_loss: 0.2587, total_rpn_loss: 0.0410, rpn_score_loss: 0.0164, rpn_box_loss: 0.0246, total_loss: 0.7023\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][500/924]#011lr: 0.00340, eta: 0:13:57, step time: 0.412, total_loss_bbox: 0.4053, class_loss: 0.2175, box_loss: 0.1879, mask_loss: 0.2500, total_rpn_loss: 0.0399, rpn_score_loss: 0.0160, rpn_box_loss: 0.0239, total_loss: 0.6952\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][550/924]#011lr: 0.00324, eta: 0:13:27, step time: 0.408, total_loss_bbox: 0.4024, class_loss: 0.2109, box_loss: 0.1914, mask_loss: 0.2645, total_rpn_loss: 0.0397, rpn_score_loss: 0.0158, rpn_box_loss: 0.0239, total_loss: 0.7065\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][600/924]#011lr: 0.00309, eta: 0:13:07, step time: 0.408, total_loss_bbox: 0.4022, class_loss: 0.2147, box_loss: 0.1875, mask_loss: 0.2564, total_rpn_loss: 0.0432, rpn_score_loss: 0.0188, rpn_box_loss: 0.0243, total_loss: 0.7017\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][650/924]#011lr: 0.00294, eta: 0:12:35, step time: 0.401, total_loss_bbox: 0.3952, class_loss: 0.2092, box_loss: 0.1860, mask_loss: 0.2598, total_rpn_loss: 0.0412, rpn_score_loss: 0.0169, rpn_box_loss: 0.0243, total_loss: 0.6962\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][700/924]#011lr: 0.00279, eta: 0:12:25, step time: 0.407, total_loss_bbox: 0.4087, class_loss: 0.2151, box_loss: 0.1935, mask_loss: 0.2609, total_rpn_loss: 0.0433, rpn_score_loss: 0.0184, rpn_box_loss: 0.0249, total_loss: 0.7128\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][750/924]#011lr: 0.00265, eta: 0:12:08, step time: 0.409, total_loss_bbox: 0.3735, class_loss: 0.1911, box_loss: 0.1825, mask_loss: 0.2483, total_rpn_loss: 0.0378, rpn_score_loss: 0.0148, rpn_box_loss: 0.0231, total_loss: 0.6596\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][800/924]#011lr: 0.00251, eta: 0:11:43, step time: 0.406, total_loss_bbox: 0.3955, class_loss: 0.2082, box_loss: 0.1874, mask_loss: 0.2542, total_rpn_loss: 0.0382, rpn_score_loss: 0.0147, rpn_box_loss: 0.0234, total_loss: 0.6879\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][850/924]#011lr: 0.00237, eta: 0:11:22, step time: 0.406, total_loss_bbox: 0.3835, class_loss: 0.1978, box_loss: 0.1857, mask_loss: 0.2557, total_rpn_loss: 0.0398, rpn_score_loss: 0.0161, rpn_box_loss: 0.0237, total_loss: 0.6789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [8][900/924]#011lr: 0.00224, eta: 0:11:04, step time: 0.407, total_loss_bbox: 0.4118, class_loss: 0.2240, box_loss: 0.1878, mask_loss: 0.2568, total_rpn_loss: 0.0421, rpn_score_loss: 0.0180, rpn_box_loss: 0.0241, total_loss: 0.7107\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/008/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 9 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][50/924]#011lr: 0.00205, eta: 0:11:02, step time: 0.425, total_loss_bbox: 0.3799, class_loss: 0.1978, box_loss: 0.1821, mask_loss: 0.2506, total_rpn_loss: 0.0385, rpn_score_loss: 0.0147, rpn_box_loss: 0.0238, total_loss: 0.6690\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][100/924]#011lr: 0.00192, eta: 0:10:11, step time: 0.406, total_loss_bbox: 0.3889, class_loss: 0.2016, box_loss: 0.1872, mask_loss: 0.2542, total_rpn_loss: 0.0406, rpn_score_loss: 0.0170, rpn_box_loss: 0.0236, total_loss: 0.6837\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][150/924]#011lr: 0.00180, eta: 0:09:50, step time: 0.405, total_loss_bbox: 0.3787, class_loss: 0.1956, box_loss: 0.1831, mask_loss: 0.2522, total_rpn_loss: 0.0383, rpn_score_loss: 0.0157, rpn_box_loss: 0.0227, total_loss: 0.6692\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][200/924]#011lr: 0.00168, eta: 0:09:35, step time: 0.409, total_loss_bbox: 0.3939, class_loss: 0.2047, box_loss: 0.1891, mask_loss: 0.2561, total_rpn_loss: 0.0401, rpn_score_loss: 0.0165, rpn_box_loss: 0.0236, total_loss: 0.6901\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][250/924]#011lr: 0.00157, eta: 0:09:11, step time: 0.406, total_loss_bbox: 0.3871, class_loss: 0.2022, box_loss: 0.1849, mask_loss: 0.2521, total_rpn_loss: 0.0398, rpn_score_loss: 0.0166, rpn_box_loss: 0.0232, total_loss: 0.6789\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][300/924]#011lr: 0.00146, eta: 0:08:46, step time: 0.402, total_loss_bbox: 0.3811, class_loss: 0.1962, box_loss: 0.1849, mask_loss: 0.2516, total_rpn_loss: 0.0402, rpn_score_loss: 0.0155, rpn_box_loss: 0.0246, total_loss: 0.6730\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][350/924]#011lr: 0.00135, eta: 0:08:36, step time: 0.411, total_loss_bbox: 0.3895, class_loss: 0.2051, box_loss: 0.1844, mask_loss: 0.2517, total_rpn_loss: 0.0391, rpn_score_loss: 0.0163, rpn_box_loss: 0.0228, total_loss: 0.6803\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][400/924]#011lr: 0.00125, eta: 0:08:09, step time: 0.405, total_loss_bbox: 0.3911, class_loss: 0.2070, box_loss: 0.1841, mask_loss: 0.2517, total_rpn_loss: 0.0418, rpn_score_loss: 0.0174, rpn_box_loss: 0.0244, total_loss: 0.6847\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][450/924]#011lr: 0.00116, eta: 0:07:48, step time: 0.404, total_loss_bbox: 0.3873, class_loss: 0.2057, box_loss: 0.1816, mask_loss: 0.2509, total_rpn_loss: 0.0380, rpn_score_loss: 0.0147, rpn_box_loss: 0.0234, total_loss: 0.6762\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][500/924]#011lr: 0.00106, eta: 0:07:23, step time: 0.400, total_loss_bbox: 0.3843, class_loss: 0.1989, box_loss: 0.1854, mask_loss: 0.2549, total_rpn_loss: 0.0376, rpn_score_loss: 0.0152, rpn_box_loss: 0.0224, total_loss: 0.6768\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][550/924]#011lr: 0.00097, eta: 0:07:04, step time: 0.401, total_loss_bbox: 0.3938, class_loss: 0.2084, box_loss: 0.1854, mask_loss: 0.2604, total_rpn_loss: 0.0387, rpn_score_loss: 0.0149, rpn_box_loss: 0.0238, total_loss: 0.6929\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][600/924]#011lr: 0.00088, eta: 0:06:44, step time: 0.401, total_loss_bbox: 0.3689, class_loss: 0.1879, box_loss: 0.1810, mask_loss: 0.2467, total_rpn_loss: 0.0373, rpn_score_loss: 0.0140, rpn_box_loss: 0.0233, total_loss: 0.6529\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][650/924]#011lr: 0.00080, eta: 0:06:30, step time: 0.408, total_loss_bbox: 0.3666, class_loss: 0.1857, box_loss: 0.1809, mask_loss: 0.2411, total_rpn_loss: 0.0357, rpn_score_loss: 0.0144, rpn_box_loss: 0.0214, total_loss: 0.6434\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][700/924]#011lr: 0.00072, eta: 0:06:07, step time: 0.405, total_loss_bbox: 0.3952, class_loss: 0.2105, box_loss: 0.1848, mask_loss: 0.2584, total_rpn_loss: 0.0408, rpn_score_loss: 0.0167, rpn_box_loss: 0.0241, total_loss: 0.6944\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][750/924]#011lr: 0.00065, eta: 0:05:50, step time: 0.408, total_loss_bbox: 0.3874, class_loss: 0.2021, box_loss: 0.1854, mask_loss: 0.2525, total_rpn_loss: 0.0384, rpn_score_loss: 0.0164, rpn_box_loss: 0.0220, total_loss: 0.6783\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][800/924]#011lr: 0.00058, eta: 0:05:30, step time: 0.408, total_loss_bbox: 0.3748, class_loss: 0.1956, box_loss: 0.1792, mask_loss: 0.2442, total_rpn_loss: 0.0357, rpn_score_loss: 0.0135, rpn_box_loss: 0.0222, total_loss: 0.6548\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][850/924]#011lr: 0.00051, eta: 0:05:13, step time: 0.414, total_loss_bbox: 0.3630, class_loss: 0.1834, box_loss: 0.1796, mask_loss: 0.2438, total_rpn_loss: 0.0396, rpn_score_loss: 0.0160, rpn_box_loss: 0.0236, total_loss: 0.6464\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [9][900/924]#011lr: 0.00045, eta: 0:04:50, step time: 0.410, total_loss_bbox: 0.3752, class_loss: 0.1924, box_loss: 0.1828, mask_loss: 0.2493, total_rpn_loss: 0.0385, rpn_score_loss: 0.0157, rpn_box_loss: 0.0228, total_loss: 0.6630\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/009/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Starting epoch: 10 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][50/924]#011lr: 0.00037, eta: 0:04:30, step time: 0.427, total_loss_bbox: 0.3879, class_loss: 0.2062, box_loss: 0.1817, mask_loss: 0.2499, total_rpn_loss: 0.0373, rpn_score_loss: 0.0148, rpn_box_loss: 0.0225, total_loss: 0.6751\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][100/924]#011lr: 0.00032, eta: 0:04:02, step time: 0.415, total_loss_bbox: 0.3870, class_loss: 0.2003, box_loss: 0.1867, mask_loss: 0.2593, total_rpn_loss: 0.0391, rpn_score_loss: 0.0160, rpn_box_loss: 0.0231, total_loss: 0.6853\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][150/924]#011lr: 0.00027, eta: 0:03:35, step time: 0.404, total_loss_bbox: 0.3778, class_loss: 0.1954, box_loss: 0.1823, mask_loss: 0.2490, total_rpn_loss: 0.0362, rpn_score_loss: 0.0146, rpn_box_loss: 0.0216, total_loss: 0.6630\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][200/924]#011lr: 0.00023, eta: 0:03:17, step time: 0.409, total_loss_bbox: 0.3758, class_loss: 0.1912, box_loss: 0.1846, mask_loss: 0.2486, total_rpn_loss: 0.0374, rpn_score_loss: 0.0153, rpn_box_loss: 0.0221, total_loss: 0.6618\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][250/924]#011lr: 0.00019, eta: 0:02:57, step time: 0.409, total_loss_bbox: 0.3592, class_loss: 0.1825, box_loss: 0.1767, mask_loss: 0.2423, total_rpn_loss: 0.0350, rpn_score_loss: 0.0130, rpn_box_loss: 0.0221, total_loss: 0.6365\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][300/924]#011lr: 0.00015, eta: 0:02:35, step time: 0.406, total_loss_bbox: 0.3885, class_loss: 0.2063, box_loss: 0.1822, mask_loss: 0.2502, total_rpn_loss: 0.0385, rpn_score_loss: 0.0158, rpn_box_loss: 0.0227, total_loss: 0.6772\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][350/924]#011lr: 0.00012, eta: 0:02:17, step time: 0.412, total_loss_bbox: 0.3813, class_loss: 0.1968, box_loss: 0.1845, mask_loss: 0.2552, total_rpn_loss: 0.0400, rpn_score_loss: 0.0171, rpn_box_loss: 0.0229, total_loss: 0.6766\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][400/924]#011lr: 0.00010, eta: 0:01:56, step time: 0.409, total_loss_bbox: 0.3735, class_loss: 0.1958, box_loss: 0.1777, mask_loss: 0.2464, total_rpn_loss: 0.0357, rpn_score_loss: 0.0140, rpn_box_loss: 0.0217, total_loss: 0.6556\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][450/924]#011lr: 0.00007, eta: 0:01:34, step time: 0.403, total_loss_bbox: 0.3711, class_loss: 0.1887, box_loss: 0.1824, mask_loss: 0.2501, total_rpn_loss: 0.0373, rpn_score_loss: 0.0149, rpn_box_loss: 0.0224, total_loss: 0.6585\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][500/924]#011lr: 0.00006, eta: 0:01:15, step time: 0.408, total_loss_bbox: 0.3563, class_loss: 0.1800, box_loss: 0.1763, mask_loss: 0.2482, total_rpn_loss: 0.0353, rpn_score_loss: 0.0129, rpn_box_loss: 0.0224, total_loss: 0.6398\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][550/924]#011lr: 0.00004, eta: 0:00:54, step time: 0.405, total_loss_bbox: 0.3560, class_loss: 0.1786, box_loss: 0.1774, mask_loss: 0.2409, total_rpn_loss: 0.0348, rpn_score_loss: 0.0134, rpn_box_loss: 0.0215, total_loss: 0.6317\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][600/924]#011lr: 0.00003, eta: 0:00:34, step time: 0.407, total_loss_bbox: 0.3669, class_loss: 0.1921, box_loss: 0.1748, mask_loss: 0.2403, total_rpn_loss: 0.0369, rpn_score_loss: 0.0143, rpn_box_loss: 0.0226, total_loss: 0.6442\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Epoch [10][650/924]#011lr: 0.00003, eta: 0:00:14, step time: 0.413, total_loss_bbox: 0.3888, class_loss: 0.2037, box_loss: 0.1851, mask_loss: 0.2517, total_rpn_loss: 0.0397, rpn_score_loss: 0.0168, rpn_box_loss: 0.0229, total_loss: 0.6803\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/010/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Saved checkpoint at: /opt/ml/checkpoints/trained_model/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:End time: 2021-11-12 14:45:34.781551\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Elapsed time: 1:03:29.227159\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running eval for epoch 10\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Processing final eval\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running Evaluation for 5000 images\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:loading annotations into memory...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Done (t=0.43s)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:creating index...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Loading and preparing results...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:DONE (t=0.42s)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running per image evaluation...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Evaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:DONE (t=2.29s).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Accumulating evaluation results...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:DONE (t=0.00s).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.37777\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.58585\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.41036\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.22862\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.41212\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.48417\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.31836\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.51595\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.54745\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.37008\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.58764\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.68750\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Loading and preparing results...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:DONE (t=3.08s)\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Running per image evaluation...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Evaluate annotation type *segm*\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:DONE (t=3.98s).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:Accumulating evaluation results...\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:DONE (t=0.00s).\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.34499\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.55589\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.36847\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.16578\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.36862\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.49939\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.29938\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.47453\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.50094\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.31511\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.53965\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>: Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.66185\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:{'bbox': {'bbox AP 0.50:0.95 all': 0.37777172464257047}, 'segm': {'segm AP 0.50:0.95 all': 0.3449911449009801}}\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:bbox bbox AP 0.50:0.95 all: 0.37777172464257047\u001b[0m\n",
      "\u001b[34m[1,0]<stdout>:segm segm AP 0.50:0.95 all: 0.3449911449009801\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.236.146' (ECDSA) to the list of known hosts.#015\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:44:53.667932: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:44:53.668043: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:44:53.700513: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:44:53.746989: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:44:53.747104: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:44:53.763437: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:44:53.763555: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:44:53.767949: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:44:53.768068: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:44:53.779304: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:44:53.783081: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:44:53.783080: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:44:53.783202: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:44:53.783202: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:44:53.794240: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:44:53.794367: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:44:53.794916: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:44:53.795393: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:44:53.795027: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:44:53.795509: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:44:53.795365: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:44:53.797539: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:44:53.797645: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:44:53.800431: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:44:53.800470: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:44:53.800563: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:44:53.814808: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:44:53.815971: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:44:53.818917: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:44:53.819030: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:44:53.826445: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:44:53.826944: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:44:53.826779: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:44:53.829054: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:44:53.830181: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:44:53.830286: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:44:53.831558: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:44:53.838778: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:44:53.838889: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:44:53.851160: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:44:53.861508: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:44:53.870925: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:44:53.979308: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:44:53.979423: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:44:53.982093: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:44:53.982219: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:44:54.012094: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:44:54.014357: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,12]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,10]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m2021-11-12 15:50:10,588 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path /opt/ml/model. Your training job will not save any model files to S3.\u001b[0m\n",
      "\u001b[34mFor details of how to construct your training script see:\u001b[0m\n",
      "\u001b[34mhttps://sagemaker.readthedocs.io/en/stable/using_tf.html#adapting-your-local-tensorflow-script\u001b[0m\n",
      "\u001b[34m2021-11-12 15:50:10,588 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,1]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,4]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,14]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,11]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,15]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,9]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:/usr/local/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.5.0 and strictly below 2.8.0 (nightly versions are not supported). \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>: The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Some things might work, some things might not.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:If you were to encounter a bug, do not file an issue.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:You can find the compatibility matrix in TensorFlow Addon's readme:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:https://github.com/tensorflow/addons\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:  UserWarning,\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:Instructions for updating:\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:Use fn_output_signature instead\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:15.044080: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:15.044132: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:15.044145: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:45:15.044361: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:45:15.044401: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:45:15.044413: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:45:15.078361: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:45:15.078411: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 14:45:15.078423: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:15.078815: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:15.078852: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:15.078863: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:15.299420: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:15.299465: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:15.299477: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:15.330051: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:15.330097: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:15.330109: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:45:15.344544: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:45:15.344600: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:45:15.344614: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:15.345266: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:15.345311: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:15.345324: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:15.345346: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:15.345391: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:15.345403: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:45:15.361752: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:45:15.361808: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 14:45:15.361823: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:15.441062: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:15.441106: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:15.441117: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:15.456473: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:15.456520: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:15.456531: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:45:15.656919: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:45:15.656971: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:45:15.656982: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:15.661826: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:15.661872: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:15.661885: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:45:15.673064: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:45:15.673116: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 14:45:15.673127: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:15.673221: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:15.673262: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:15.673273: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:15.677331: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:15.677381: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:15.677392: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:15.688776: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:15.688831: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:15.688842: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:15.903551: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:15.903600: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:15.903611: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:15.919285: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:15.919332: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:15.919343: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:45:15.965367: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:45:15.965407: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:45:15.965418: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:45:15.980132: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:45:15.980171: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 14:45:15.980182: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:16.048215: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:16.048264: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:16.048275: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:16.063208: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:16.063255: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:16.063266: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:16.094059: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:16.094104: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:16.094115: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:16.110328: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:16.110376: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:16.110387: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:45:16.153378: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:45:16.153420: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:45:16.153431: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:45:16.168385: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:45:16.168433: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 14:45:16.168444: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:16.384756: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:16.384808: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:16.384823: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:16.401540: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:16.401593: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:16.401608: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:45:16.434323: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:45:16.434371: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:45:16.434382: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:45:16.450857: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_8\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:45:16.450902: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_11\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 14:45:16.450913: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_3\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:20.443706: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:20.443795: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:20.481774: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 14:45:20.481973: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:20.697025: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 14:45:20.697116: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:20.790975: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 14:45:20.791239: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:20.924020: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 14:45:20.924106: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:21.009462: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 14:45:21.009554: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:21.023482: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:21.023580: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:21.100263: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 14:45:21.100512: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:21.212540: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:21.212635: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:21.246102: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 14:45:21.246180: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:21.439325: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 14:45:21.439447: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:21.548515: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 14:45:21.548810: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:21.706209: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxIntersectionOverUnion\" device_type: \"GPU\"') for unknown op: BoxIntersectionOverUnion\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 14:45:21.706299: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: \"BoxEncode\" device_type: \"GPU\"') for unknown op: BoxEncode\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Start running, work_dir: /opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:max: 10 epochs\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Loading checkpoint from /opt/ml/input/data/weights/resnet50.ckpt...\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Start time: 2021-11-12 14:45:34.781551\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 1 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][50/924]#011lr: 0.00532, eta: 8:01:38, step time: 3.229, total_loss_bbox: 0.8226, class_loss: 0.4996, box_loss: 0.3230, mask_loss: 0.6935, total_rpn_loss: 0.1925, rpn_score_loss: 0.1351, rpn_box_loss: 0.0575, total_loss: 1.7086\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][100/924]#011lr: 0.00784, eta: 1:00:31, step time: 0.408, total_loss_bbox: 0.8322, class_loss: 0.4783, box_loss: 0.3539, mask_loss: 0.5808, total_rpn_loss: 0.1261, rpn_score_loss: 0.0749, rpn_box_loss: 0.0512, total_loss: 1.5392\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][150/924]#011lr: 0.01036, eta: 1:02:16, step time: 0.422, total_loss_bbox: 0.8577, class_loss: 0.5009, box_loss: 0.3568, mask_loss: 0.5152, total_rpn_loss: 0.1104, rpn_score_loss: 0.0591, rpn_box_loss: 0.0512, total_loss: 1.4832\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][200/924]#011lr: 0.01288, eta: 0:58:17, step time: 0.397, total_loss_bbox: 0.8682, class_loss: 0.5204, box_loss: 0.3479, mask_loss: 0.4678, total_rpn_loss: 0.0978, rpn_score_loss: 0.0512, rpn_box_loss: 0.0467, total_loss: 1.4339\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][250/924]#011lr: 0.01540, eta: 0:58:37, step time: 0.402, total_loss_bbox: 0.8201, class_loss: 0.4815, box_loss: 0.3386, mask_loss: 0.4533, total_rpn_loss: 0.0995, rpn_score_loss: 0.0511, rpn_box_loss: 0.0484, total_loss: 1.3729\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][300/924]#011lr: 0.01792, eta: 0:58:09, step time: 0.401, total_loss_bbox: 0.7808, class_loss: 0.4546, box_loss: 0.3262, mask_loss: 0.4285, total_rpn_loss: 0.0917, rpn_score_loss: 0.0466, rpn_box_loss: 0.0451, total_loss: 1.3010\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][350/924]#011lr: 0.02044, eta: 0:58:26, step time: 0.405, total_loss_bbox: 0.7655, class_loss: 0.4455, box_loss: 0.3200, mask_loss: 0.4163, total_rpn_loss: 0.0966, rpn_score_loss: 0.0503, rpn_box_loss: 0.0464, total_loss: 1.2784\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][400/924]#011lr: 0.02296, eta: 0:57:36, step time: 0.402, total_loss_bbox: 0.7550, class_loss: 0.4413, box_loss: 0.3137, mask_loss: 0.4133, total_rpn_loss: 0.0978, rpn_score_loss: 0.0527, rpn_box_loss: 0.0452, total_loss: 1.2661\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][450/924]#011lr: 0.02548, eta: 0:57:50, step time: 0.406, total_loss_bbox: 0.6999, class_loss: 0.3930, box_loss: 0.3069, mask_loss: 0.3861, total_rpn_loss: 0.0853, rpn_score_loss: 0.0416, rpn_box_loss: 0.0437, total_loss: 1.1714\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][500/924]#011lr: 0.02779, eta: 0:57:03, step time: 0.403, total_loss_bbox: 0.7030, class_loss: 0.4010, box_loss: 0.3020, mask_loss: 0.3893, total_rpn_loss: 0.0976, rpn_score_loss: 0.0505, rpn_box_loss: 0.0470, total_loss: 1.1899\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][550/924]#011lr: 0.02774, eta: 0:58:43, step time: 0.417, total_loss_bbox: 0.6499, class_loss: 0.3548, box_loss: 0.2951, mask_loss: 0.3703, total_rpn_loss: 0.0841, rpn_score_loss: 0.0386, rpn_box_loss: 0.0455, total_loss: 1.1043\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][600/924]#011lr: 0.02769, eta: 0:56:42, step time: 0.405, total_loss_bbox: 0.6634, class_loss: 0.3731, box_loss: 0.2903, mask_loss: 0.3704, total_rpn_loss: 0.0890, rpn_score_loss: 0.0422, rpn_box_loss: 0.0468, total_loss: 1.1228\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][650/924]#011lr: 0.02764, eta: 0:56:03, step time: 0.403, total_loss_bbox: 0.6408, class_loss: 0.3552, box_loss: 0.2856, mask_loss: 0.3669, total_rpn_loss: 0.0877, rpn_score_loss: 0.0436, rpn_box_loss: 0.0441, total_loss: 1.0954\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][700/924]#011lr: 0.02758, eta: 0:56:36, step time: 0.409, total_loss_bbox: 0.6426, class_loss: 0.3628, box_loss: 0.2798, mask_loss: 0.3650, total_rpn_loss: 0.0888, rpn_score_loss: 0.0439, rpn_box_loss: 0.0448, total_loss: 1.0964\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][750/924]#011lr: 0.02752, eta: 0:55:37, step time: 0.405, total_loss_bbox: 0.6234, class_loss: 0.3461, box_loss: 0.2772, mask_loss: 0.3548, total_rpn_loss: 0.0801, rpn_score_loss: 0.0392, rpn_box_loss: 0.0409, total_loss: 1.0582\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][800/924]#011lr: 0.02746, eta: 0:54:35, step time: 0.399, total_loss_bbox: 0.5963, class_loss: 0.3221, box_loss: 0.2742, mask_loss: 0.3439, total_rpn_loss: 0.0801, rpn_score_loss: 0.0383, rpn_box_loss: 0.0418, total_loss: 1.0204\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][850/924]#011lr: 0.02739, eta: 0:53:56, step time: 0.397, total_loss_bbox: 0.6115, class_loss: 0.3409, box_loss: 0.2706, mask_loss: 0.3496, total_rpn_loss: 0.0805, rpn_score_loss: 0.0379, rpn_box_loss: 0.0426, total_loss: 1.0416\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [1][900/924]#011lr: 0.02732, eta: 0:54:56, step time: 0.407, total_loss_bbox: 0.5968, class_loss: 0.3312, box_loss: 0.2656, mask_loss: 0.3421, total_rpn_loss: 0.0785, rpn_score_loss: 0.0372, rpn_box_loss: 0.0413, total_loss: 1.0174\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/001/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 2 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][50/924]#011lr: 0.02720, eta: 1:06:34, step time: 0.498, total_loss_bbox: 0.5862, class_loss: 0.3229, box_loss: 0.2633, mask_loss: 0.3397, total_rpn_loss: 0.0744, rpn_score_loss: 0.0343, rpn_box_loss: 0.0401, total_loss: 1.0003\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][100/924]#011lr: 0.02712, eta: 0:53:25, step time: 0.402, total_loss_bbox: 0.5747, class_loss: 0.3112, box_loss: 0.2635, mask_loss: 0.3354, total_rpn_loss: 0.0786, rpn_score_loss: 0.0375, rpn_box_loss: 0.0411, total_loss: 0.9886\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][150/924]#011lr: 0.02703, eta: 0:53:37, step time: 0.406, total_loss_bbox: 0.5615, class_loss: 0.3017, box_loss: 0.2598, mask_loss: 0.3295, total_rpn_loss: 0.0747, rpn_score_loss: 0.0358, rpn_box_loss: 0.0389, total_loss: 0.9657\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][200/924]#011lr: 0.02694, eta: 0:52:37, step time: 0.401, total_loss_bbox: 0.5923, class_loss: 0.3304, box_loss: 0.2619, mask_loss: 0.3294, total_rpn_loss: 0.0772, rpn_score_loss: 0.0365, rpn_box_loss: 0.0407, total_loss: 0.9990\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][250/924]#011lr: 0.02684, eta: 0:52:51, step time: 0.405, total_loss_bbox: 0.5668, class_loss: 0.3103, box_loss: 0.2565, mask_loss: 0.3243, total_rpn_loss: 0.0706, rpn_score_loss: 0.0313, rpn_box_loss: 0.0393, total_loss: 0.9617\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][300/924]#011lr: 0.02674, eta: 0:52:07, step time: 0.402, total_loss_bbox: 0.5603, class_loss: 0.3081, box_loss: 0.2522, mask_loss: 0.3273, total_rpn_loss: 0.0757, rpn_score_loss: 0.0352, rpn_box_loss: 0.0406, total_loss: 0.9633\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][350/924]#011lr: 0.02664, eta: 0:52:03, step time: 0.404, total_loss_bbox: 0.5672, class_loss: 0.3162, box_loss: 0.2509, mask_loss: 0.3299, total_rpn_loss: 0.0751, rpn_score_loss: 0.0361, rpn_box_loss: 0.0390, total_loss: 0.9721\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][400/924]#011lr: 0.02653, eta: 0:50:46, step time: 0.397, total_loss_bbox: 0.5523, class_loss: 0.3003, box_loss: 0.2520, mask_loss: 0.3268, total_rpn_loss: 0.0711, rpn_score_loss: 0.0320, rpn_box_loss: 0.0390, total_loss: 0.9502\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][450/924]#011lr: 0.02642, eta: 0:51:48, step time: 0.408, total_loss_bbox: 0.5536, class_loss: 0.3045, box_loss: 0.2491, mask_loss: 0.3185, total_rpn_loss: 0.0696, rpn_score_loss: 0.0328, rpn_box_loss: 0.0368, total_loss: 0.9417\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][500/924]#011lr: 0.02631, eta: 0:50:54, step time: 0.403, total_loss_bbox: 0.5376, class_loss: 0.2885, box_loss: 0.2490, mask_loss: 0.3306, total_rpn_loss: 0.0715, rpn_score_loss: 0.0316, rpn_box_loss: 0.0398, total_loss: 0.9396\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][550/924]#011lr: 0.02619, eta: 0:50:23, step time: 0.402, total_loss_bbox: 0.5502, class_loss: 0.3020, box_loss: 0.2481, mask_loss: 0.3193, total_rpn_loss: 0.0735, rpn_score_loss: 0.0348, rpn_box_loss: 0.0387, total_loss: 0.9430\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][600/924]#011lr: 0.02607, eta: 0:49:53, step time: 0.400, total_loss_bbox: 0.5574, class_loss: 0.3060, box_loss: 0.2514, mask_loss: 0.3312, total_rpn_loss: 0.0728, rpn_score_loss: 0.0333, rpn_box_loss: 0.0395, total_loss: 0.9614\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][650/924]#011lr: 0.02594, eta: 0:50:45, step time: 0.410, total_loss_bbox: 0.5588, class_loss: 0.3137, box_loss: 0.2451, mask_loss: 0.3125, total_rpn_loss: 0.0744, rpn_score_loss: 0.0346, rpn_box_loss: 0.0398, total_loss: 0.9458\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][700/924]#011lr: 0.02581, eta: 0:49:55, step time: 0.406, total_loss_bbox: 0.5388, class_loss: 0.2912, box_loss: 0.2475, mask_loss: 0.3228, total_rpn_loss: 0.0761, rpn_score_loss: 0.0361, rpn_box_loss: 0.0400, total_loss: 0.9376\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][750/924]#011lr: 0.02568, eta: 0:49:18, step time: 0.404, total_loss_bbox: 0.5715, class_loss: 0.3270, box_loss: 0.2445, mask_loss: 0.3114, total_rpn_loss: 0.0730, rpn_score_loss: 0.0346, rpn_box_loss: 0.0384, total_loss: 0.9559\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][800/924]#011lr: 0.02554, eta: 0:49:10, step time: 0.405, total_loss_bbox: 0.5436, class_loss: 0.3018, box_loss: 0.2418, mask_loss: 0.3251, total_rpn_loss: 0.0755, rpn_score_loss: 0.0372, rpn_box_loss: 0.0383, total_loss: 0.9442\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][850/924]#011lr: 0.02540, eta: 0:47:23, step time: 0.394, total_loss_bbox: 0.5420, class_loss: 0.3008, box_loss: 0.2412, mask_loss: 0.3149, total_rpn_loss: 0.0657, rpn_score_loss: 0.0279, rpn_box_loss: 0.0378, total_loss: 0.9226\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [2][900/924]#011lr: 0.02526, eta: 0:48:46, step time: 0.408, total_loss_bbox: 0.5490, class_loss: 0.3081, box_loss: 0.2409, mask_loss: 0.3206, total_rpn_loss: 0.0702, rpn_score_loss: 0.0334, rpn_box_loss: 0.0369, total_loss: 0.9399\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/002/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 3 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][50/924]#011lr: 0.02504, eta: 0:50:28, step time: 0.426, total_loss_bbox: 0.5238, class_loss: 0.2860, box_loss: 0.2378, mask_loss: 0.3096, total_rpn_loss: 0.0630, rpn_score_loss: 0.0268, rpn_box_loss: 0.0363, total_loss: 0.8965\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][100/924]#011lr: 0.02489, eta: 0:46:29, step time: 0.396, total_loss_bbox: 0.5061, class_loss: 0.2749, box_loss: 0.2312, mask_loss: 0.3082, total_rpn_loss: 0.0635, rpn_score_loss: 0.0263, rpn_box_loss: 0.0373, total_loss: 0.8779\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][150/924]#011lr: 0.02473, eta: 0:47:45, step time: 0.409, total_loss_bbox: 0.5198, class_loss: 0.2856, box_loss: 0.2342, mask_loss: 0.3044, total_rpn_loss: 0.0663, rpn_score_loss: 0.0289, rpn_box_loss: 0.0374, total_loss: 0.8905\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][200/924]#011lr: 0.02458, eta: 0:47:48, step time: 0.413, total_loss_bbox: 0.5188, class_loss: 0.2842, box_loss: 0.2346, mask_loss: 0.3050, total_rpn_loss: 0.0681, rpn_score_loss: 0.0323, rpn_box_loss: 0.0358, total_loss: 0.8919\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][250/924]#011lr: 0.02441, eta: 0:46:43, step time: 0.406, total_loss_bbox: 0.5427, class_loss: 0.3049, box_loss: 0.2378, mask_loss: 0.3115, total_rpn_loss: 0.0694, rpn_score_loss: 0.0309, rpn_box_loss: 0.0385, total_loss: 0.9236\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][300/924]#011lr: 0.02425, eta: 0:45:44, step time: 0.401, total_loss_bbox: 0.5302, class_loss: 0.2942, box_loss: 0.2360, mask_loss: 0.3106, total_rpn_loss: 0.0657, rpn_score_loss: 0.0297, rpn_box_loss: 0.0360, total_loss: 0.9065\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][350/924]#011lr: 0.02408, eta: 0:46:45, step time: 0.412, total_loss_bbox: 0.5120, class_loss: 0.2811, box_loss: 0.2309, mask_loss: 0.3031, total_rpn_loss: 0.0637, rpn_score_loss: 0.0296, rpn_box_loss: 0.0342, total_loss: 0.8788\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][400/924]#011lr: 0.02391, eta: 0:45:14, step time: 0.402, total_loss_bbox: 0.5401, class_loss: 0.3047, box_loss: 0.2354, mask_loss: 0.3103, total_rpn_loss: 0.0710, rpn_score_loss: 0.0342, rpn_box_loss: 0.0367, total_loss: 0.9213\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][450/924]#011lr: 0.02374, eta: 0:45:26, step time: 0.407, total_loss_bbox: 0.5073, class_loss: 0.2763, box_loss: 0.2310, mask_loss: 0.3001, total_rpn_loss: 0.0640, rpn_score_loss: 0.0286, rpn_box_loss: 0.0354, total_loss: 0.8714\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][500/924]#011lr: 0.02356, eta: 0:44:55, step time: 0.405, total_loss_bbox: 0.5134, class_loss: 0.2791, box_loss: 0.2343, mask_loss: 0.3084, total_rpn_loss: 0.0652, rpn_score_loss: 0.0304, rpn_box_loss: 0.0347, total_loss: 0.8869\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][550/924]#011lr: 0.02338, eta: 0:44:17, step time: 0.402, total_loss_bbox: 0.4942, class_loss: 0.2696, box_loss: 0.2246, mask_loss: 0.2885, total_rpn_loss: 0.0629, rpn_score_loss: 0.0255, rpn_box_loss: 0.0374, total_loss: 0.8457\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][600/924]#011lr: 0.02320, eta: 0:43:57, step time: 0.402, total_loss_bbox: 0.5057, class_loss: 0.2775, box_loss: 0.2282, mask_loss: 0.2986, total_rpn_loss: 0.0673, rpn_score_loss: 0.0312, rpn_box_loss: 0.0361, total_loss: 0.8716\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][650/924]#011lr: 0.02301, eta: 0:44:49, step time: 0.414, total_loss_bbox: 0.5091, class_loss: 0.2820, box_loss: 0.2271, mask_loss: 0.3033, total_rpn_loss: 0.0651, rpn_score_loss: 0.0311, rpn_box_loss: 0.0340, total_loss: 0.8776\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][700/924]#011lr: 0.02282, eta: 0:43:51, step time: 0.408, total_loss_bbox: 0.5072, class_loss: 0.2763, box_loss: 0.2309, mask_loss: 0.3032, total_rpn_loss: 0.0650, rpn_score_loss: 0.0296, rpn_box_loss: 0.0354, total_loss: 0.8755\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][750/924]#011lr: 0.02263, eta: 0:43:26, step time: 0.407, total_loss_bbox: 0.5080, class_loss: 0.2812, box_loss: 0.2268, mask_loss: 0.2967, total_rpn_loss: 0.0621, rpn_score_loss: 0.0295, rpn_box_loss: 0.0326, total_loss: 0.8668\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][800/924]#011lr: 0.02244, eta: 0:42:17, step time: 0.400, total_loss_bbox: 0.5201, class_loss: 0.2926, box_loss: 0.2275, mask_loss: 0.3026, total_rpn_loss: 0.0668, rpn_score_loss: 0.0312, rpn_box_loss: 0.0356, total_loss: 0.8895\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][850/924]#011lr: 0.02224, eta: 0:42:51, step time: 0.408, total_loss_bbox: 0.4977, class_loss: 0.2733, box_loss: 0.2244, mask_loss: 0.2957, total_rpn_loss: 0.0600, rpn_score_loss: 0.0261, rpn_box_loss: 0.0339, total_loss: 0.8534\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [3][900/924]#011lr: 0.02204, eta: 0:43:07, step time: 0.414, total_loss_bbox: 0.5014, class_loss: 0.2726, box_loss: 0.2288, mask_loss: 0.2998, total_rpn_loss: 0.0617, rpn_score_loss: 0.0270, rpn_box_loss: 0.0347, total_loss: 0.8628\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/003/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 4 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][50/924]#011lr: 0.02175, eta: 0:44:09, step time: 0.429, total_loss_bbox: 0.4856, class_loss: 0.2629, box_loss: 0.2227, mask_loss: 0.2938, total_rpn_loss: 0.0593, rpn_score_loss: 0.0265, rpn_box_loss: 0.0328, total_loss: 0.8387\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][100/924]#011lr: 0.02154, eta: 0:41:13, step time: 0.404, total_loss_bbox: 0.4826, class_loss: 0.2568, box_loss: 0.2258, mask_loss: 0.2973, total_rpn_loss: 0.0593, rpn_score_loss: 0.0256, rpn_box_loss: 0.0337, total_loss: 0.8392\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][150/924]#011lr: 0.02133, eta: 0:40:51, step time: 0.403, total_loss_bbox: 0.4998, class_loss: 0.2800, box_loss: 0.2198, mask_loss: 0.2910, total_rpn_loss: 0.0604, rpn_score_loss: 0.0269, rpn_box_loss: 0.0335, total_loss: 0.8512\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][200/924]#011lr: 0.02113, eta: 0:40:54, step time: 0.407, total_loss_bbox: 0.4832, class_loss: 0.2601, box_loss: 0.2230, mask_loss: 0.2911, total_rpn_loss: 0.0615, rpn_score_loss: 0.0263, rpn_box_loss: 0.0352, total_loss: 0.8358\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][250/924]#011lr: 0.02091, eta: 0:40:22, step time: 0.405, total_loss_bbox: 0.4964, class_loss: 0.2690, box_loss: 0.2274, mask_loss: 0.2933, total_rpn_loss: 0.0615, rpn_score_loss: 0.0271, rpn_box_loss: 0.0344, total_loss: 0.8512\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][300/924]#011lr: 0.02070, eta: 0:40:11, step time: 0.407, total_loss_bbox: 0.4884, class_loss: 0.2675, box_loss: 0.2209, mask_loss: 0.2950, total_rpn_loss: 0.0605, rpn_score_loss: 0.0263, rpn_box_loss: 0.0342, total_loss: 0.8439\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][350/924]#011lr: 0.02049, eta: 0:39:43, step time: 0.406, total_loss_bbox: 0.4846, class_loss: 0.2655, box_loss: 0.2191, mask_loss: 0.2939, total_rpn_loss: 0.0609, rpn_score_loss: 0.0273, rpn_box_loss: 0.0336, total_loss: 0.8394\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][400/924]#011lr: 0.02027, eta: 0:39:56, step time: 0.411, total_loss_bbox: 0.4892, class_loss: 0.2671, box_loss: 0.2222, mask_loss: 0.2976, total_rpn_loss: 0.0602, rpn_score_loss: 0.0269, rpn_box_loss: 0.0333, total_loss: 0.8470\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][450/924]#011lr: 0.02005, eta: 0:38:41, step time: 0.402, total_loss_bbox: 0.4767, class_loss: 0.2602, box_loss: 0.2166, mask_loss: 0.2784, total_rpn_loss: 0.0569, rpn_score_loss: 0.0242, rpn_box_loss: 0.0327, total_loss: 0.8120\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][500/924]#011lr: 0.01983, eta: 0:38:47, step time: 0.406, total_loss_bbox: 0.4980, class_loss: 0.2784, box_loss: 0.2196, mask_loss: 0.2987, total_rpn_loss: 0.0609, rpn_score_loss: 0.0276, rpn_box_loss: 0.0332, total_loss: 0.8576\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][550/924]#011lr: 0.01960, eta: 0:38:29, step time: 0.407, total_loss_bbox: 0.4876, class_loss: 0.2685, box_loss: 0.2191, mask_loss: 0.2913, total_rpn_loss: 0.0593, rpn_score_loss: 0.0254, rpn_box_loss: 0.0340, total_loss: 0.8383\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][600/924]#011lr: 0.01938, eta: 0:38:09, step time: 0.407, total_loss_bbox: 0.4674, class_loss: 0.2533, box_loss: 0.2141, mask_loss: 0.2818, total_rpn_loss: 0.0599, rpn_score_loss: 0.0259, rpn_box_loss: 0.0340, total_loss: 0.8091\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][650/924]#011lr: 0.01915, eta: 0:37:59, step time: 0.409, total_loss_bbox: 0.4844, class_loss: 0.2701, box_loss: 0.2143, mask_loss: 0.2861, total_rpn_loss: 0.0581, rpn_score_loss: 0.0260, rpn_box_loss: 0.0321, total_loss: 0.8286\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][700/924]#011lr: 0.01893, eta: 0:37:09, step time: 0.403, total_loss_bbox: 0.4856, class_loss: 0.2685, box_loss: 0.2170, mask_loss: 0.2819, total_rpn_loss: 0.0633, rpn_score_loss: 0.0296, rpn_box_loss: 0.0337, total_loss: 0.8308\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][750/924]#011lr: 0.01870, eta: 0:37:45, step time: 0.414, total_loss_bbox: 0.4897, class_loss: 0.2723, box_loss: 0.2174, mask_loss: 0.2898, total_rpn_loss: 0.0634, rpn_score_loss: 0.0296, rpn_box_loss: 0.0339, total_loss: 0.8430\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][800/924]#011lr: 0.01847, eta: 0:36:26, step time: 0.403, total_loss_bbox: 0.4769, class_loss: 0.2641, box_loss: 0.2129, mask_loss: 0.2808, total_rpn_loss: 0.0533, rpn_score_loss: 0.0236, rpn_box_loss: 0.0296, total_loss: 0.8110\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][850/924]#011lr: 0.01823, eta: 0:35:56, step time: 0.401, total_loss_bbox: 0.4904, class_loss: 0.2718, box_loss: 0.2187, mask_loss: 0.2955, total_rpn_loss: 0.0555, rpn_score_loss: 0.0236, rpn_box_loss: 0.0319, total_loss: 0.8415\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [4][900/924]#011lr: 0.01800, eta: 0:36:10, step time: 0.407, total_loss_bbox: 0.4878, class_loss: 0.2739, box_loss: 0.2139, mask_loss: 0.2912, total_rpn_loss: 0.0592, rpn_score_loss: 0.0266, rpn_box_loss: 0.0326, total_loss: 0.8383\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/004/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 5 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][50/924]#011lr: 0.01765, eta: 0:38:00, step time: 0.434, total_loss_bbox: 0.4480, class_loss: 0.2401, box_loss: 0.2079, mask_loss: 0.2705, total_rpn_loss: 0.0536, rpn_score_loss: 0.0228, rpn_box_loss: 0.0308, total_loss: 0.7721\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][100/924]#011lr: 0.01742, eta: 0:35:11, step time: 0.406, total_loss_bbox: 0.4672, class_loss: 0.2557, box_loss: 0.2115, mask_loss: 0.2790, total_rpn_loss: 0.0558, rpn_score_loss: 0.0244, rpn_box_loss: 0.0314, total_loss: 0.8019\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][150/924]#011lr: 0.01718, eta: 0:34:41, step time: 0.404, total_loss_bbox: 0.4814, class_loss: 0.2683, box_loss: 0.2130, mask_loss: 0.2866, total_rpn_loss: 0.0580, rpn_score_loss: 0.0263, rpn_box_loss: 0.0316, total_loss: 0.8260\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][200/924]#011lr: 0.01694, eta: 0:34:38, step time: 0.407, total_loss_bbox: 0.4702, class_loss: 0.2590, box_loss: 0.2112, mask_loss: 0.2839, total_rpn_loss: 0.0578, rpn_score_loss: 0.0239, rpn_box_loss: 0.0339, total_loss: 0.8120\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][250/924]#011lr: 0.01670, eta: 0:34:03, step time: 0.404, total_loss_bbox: 0.4699, class_loss: 0.2568, box_loss: 0.2132, mask_loss: 0.2875, total_rpn_loss: 0.0570, rpn_score_loss: 0.0249, rpn_box_loss: 0.0321, total_loss: 0.8144\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][300/924]#011lr: 0.01646, eta: 0:33:33, step time: 0.402, total_loss_bbox: 0.4730, class_loss: 0.2611, box_loss: 0.2119, mask_loss: 0.2811, total_rpn_loss: 0.0568, rpn_score_loss: 0.0257, rpn_box_loss: 0.0312, total_loss: 0.8110\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][350/924]#011lr: 0.01622, eta: 0:33:13, step time: 0.402, total_loss_bbox: 0.4544, class_loss: 0.2443, box_loss: 0.2100, mask_loss: 0.2832, total_rpn_loss: 0.0545, rpn_score_loss: 0.0229, rpn_box_loss: 0.0315, total_loss: 0.7921\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][400/924]#011lr: 0.01598, eta: 0:32:49, step time: 0.402, total_loss_bbox: 0.4517, class_loss: 0.2428, box_loss: 0.2090, mask_loss: 0.2789, total_rpn_loss: 0.0559, rpn_score_loss: 0.0235, rpn_box_loss: 0.0324, total_loss: 0.7865\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][450/924]#011lr: 0.01574, eta: 0:32:50, step time: 0.406, total_loss_bbox: 0.4841, class_loss: 0.2710, box_loss: 0.2131, mask_loss: 0.2823, total_rpn_loss: 0.0588, rpn_score_loss: 0.0267, rpn_box_loss: 0.0321, total_loss: 0.8251\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][500/924]#011lr: 0.01550, eta: 0:32:08, step time: 0.402, total_loss_bbox: 0.4767, class_loss: 0.2624, box_loss: 0.2142, mask_loss: 0.2857, total_rpn_loss: 0.0570, rpn_score_loss: 0.0258, rpn_box_loss: 0.0312, total_loss: 0.8195\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][550/924]#011lr: 0.01525, eta: 0:32:02, step time: 0.404, total_loss_bbox: 0.4542, class_loss: 0.2456, box_loss: 0.2086, mask_loss: 0.2796, total_rpn_loss: 0.0530, rpn_score_loss: 0.0229, rpn_box_loss: 0.0302, total_loss: 0.7868\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][600/924]#011lr: 0.01501, eta: 0:31:20, step time: 0.400, total_loss_bbox: 0.4531, class_loss: 0.2454, box_loss: 0.2077, mask_loss: 0.2798, total_rpn_loss: 0.0516, rpn_score_loss: 0.0214, rpn_box_loss: 0.0301, total_loss: 0.7845\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][650/924]#011lr: 0.01477, eta: 0:31:20, step time: 0.404, total_loss_bbox: 0.4550, class_loss: 0.2498, box_loss: 0.2053, mask_loss: 0.2700, total_rpn_loss: 0.0531, rpn_score_loss: 0.0239, rpn_box_loss: 0.0292, total_loss: 0.7781\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][700/924]#011lr: 0.01452, eta: 0:30:55, step time: 0.403, total_loss_bbox: 0.4500, class_loss: 0.2408, box_loss: 0.2092, mask_loss: 0.2833, total_rpn_loss: 0.0536, rpn_score_loss: 0.0221, rpn_box_loss: 0.0315, total_loss: 0.7869\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][750/924]#011lr: 0.01428, eta: 0:30:51, step time: 0.406, total_loss_bbox: 0.4514, class_loss: 0.2442, box_loss: 0.2072, mask_loss: 0.2742, total_rpn_loss: 0.0544, rpn_score_loss: 0.0222, rpn_box_loss: 0.0322, total_loss: 0.7800\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][800/924]#011lr: 0.01403, eta: 0:30:42, step time: 0.409, total_loss_bbox: 0.4620, class_loss: 0.2554, box_loss: 0.2066, mask_loss: 0.2869, total_rpn_loss: 0.0564, rpn_score_loss: 0.0254, rpn_box_loss: 0.0309, total_loss: 0.8052\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][850/924]#011lr: 0.01379, eta: 0:30:12, step time: 0.407, total_loss_bbox: 0.4703, class_loss: 0.2589, box_loss: 0.2113, mask_loss: 0.2883, total_rpn_loss: 0.0556, rpn_score_loss: 0.0246, rpn_box_loss: 0.0311, total_loss: 0.8142\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [5][900/924]#011lr: 0.01355, eta: 0:29:32, step time: 0.403, total_loss_bbox: 0.4528, class_loss: 0.2433, box_loss: 0.2094, mask_loss: 0.2792, total_rpn_loss: 0.0511, rpn_score_loss: 0.0225, rpn_box_loss: 0.0286, total_loss: 0.7830\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/005/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 6 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][50/924]#011lr: 0.01318, eta: 0:30:47, step time: 0.427, total_loss_bbox: 0.4381, class_loss: 0.2373, box_loss: 0.2008, mask_loss: 0.2667, total_rpn_loss: 0.0494, rpn_score_loss: 0.0215, rpn_box_loss: 0.0279, total_loss: 0.7542\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][100/924]#011lr: 0.01294, eta: 0:29:01, step time: 0.407, total_loss_bbox: 0.4416, class_loss: 0.2364, box_loss: 0.2052, mask_loss: 0.2740, total_rpn_loss: 0.0530, rpn_score_loss: 0.0217, rpn_box_loss: 0.0313, total_loss: 0.7686\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][150/924]#011lr: 0.01270, eta: 0:28:50, step time: 0.409, total_loss_bbox: 0.4502, class_loss: 0.2433, box_loss: 0.2069, mask_loss: 0.2750, total_rpn_loss: 0.0516, rpn_score_loss: 0.0210, rpn_box_loss: 0.0305, total_loss: 0.7768\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][200/924]#011lr: 0.01245, eta: 0:28:11, step time: 0.405, total_loss_bbox: 0.4644, class_loss: 0.2558, box_loss: 0.2086, mask_loss: 0.2800, total_rpn_loss: 0.0536, rpn_score_loss: 0.0238, rpn_box_loss: 0.0298, total_loss: 0.7981\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][250/924]#011lr: 0.01221, eta: 0:27:44, step time: 0.403, total_loss_bbox: 0.4442, class_loss: 0.2404, box_loss: 0.2038, mask_loss: 0.2746, total_rpn_loss: 0.0509, rpn_score_loss: 0.0228, rpn_box_loss: 0.0281, total_loss: 0.7697\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][300/924]#011lr: 0.01197, eta: 0:27:53, step time: 0.410, total_loss_bbox: 0.4334, class_loss: 0.2322, box_loss: 0.2012, mask_loss: 0.2740, total_rpn_loss: 0.0494, rpn_score_loss: 0.0211, rpn_box_loss: 0.0282, total_loss: 0.7568\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][350/924]#011lr: 0.01173, eta: 0:27:09, step time: 0.404, total_loss_bbox: 0.4413, class_loss: 0.2356, box_loss: 0.2057, mask_loss: 0.2779, total_rpn_loss: 0.0508, rpn_score_loss: 0.0216, rpn_box_loss: 0.0292, total_loss: 0.7700\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][400/924]#011lr: 0.01149, eta: 0:26:50, step time: 0.405, total_loss_bbox: 0.4519, class_loss: 0.2478, box_loss: 0.2041, mask_loss: 0.2756, total_rpn_loss: 0.0507, rpn_score_loss: 0.0217, rpn_box_loss: 0.0291, total_loss: 0.7782\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][450/924]#011lr: 0.01125, eta: 0:27:13, step time: 0.416, total_loss_bbox: 0.4412, class_loss: 0.2402, box_loss: 0.2010, mask_loss: 0.2677, total_rpn_loss: 0.0496, rpn_score_loss: 0.0210, rpn_box_loss: 0.0287, total_loss: 0.7586\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][500/924]#011lr: 0.01101, eta: 0:26:12, step time: 0.405, total_loss_bbox: 0.4291, class_loss: 0.2293, box_loss: 0.1999, mask_loss: 0.2672, total_rpn_loss: 0.0501, rpn_score_loss: 0.0220, rpn_box_loss: 0.0281, total_loss: 0.7464\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][550/924]#011lr: 0.01077, eta: 0:25:34, step time: 0.401, total_loss_bbox: 0.4227, class_loss: 0.2272, box_loss: 0.1955, mask_loss: 0.2710, total_rpn_loss: 0.0464, rpn_score_loss: 0.0188, rpn_box_loss: 0.0276, total_loss: 0.7400\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][600/924]#011lr: 0.01054, eta: 0:25:27, step time: 0.404, total_loss_bbox: 0.4252, class_loss: 0.2247, box_loss: 0.2005, mask_loss: 0.2672, total_rpn_loss: 0.0480, rpn_score_loss: 0.0201, rpn_box_loss: 0.0279, total_loss: 0.7404\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][650/924]#011lr: 0.01030, eta: 0:24:58, step time: 0.402, total_loss_bbox: 0.4424, class_loss: 0.2399, box_loss: 0.2025, mask_loss: 0.2776, total_rpn_loss: 0.0510, rpn_score_loss: 0.0223, rpn_box_loss: 0.0287, total_loss: 0.7710\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][700/924]#011lr: 0.01007, eta: 0:25:18, step time: 0.413, total_loss_bbox: 0.4439, class_loss: 0.2444, box_loss: 0.1995, mask_loss: 0.2689, total_rpn_loss: 0.0514, rpn_score_loss: 0.0227, rpn_box_loss: 0.0287, total_loss: 0.7642\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][750/924]#011lr: 0.00983, eta: 0:24:54, step time: 0.412, total_loss_bbox: 0.4353, class_loss: 0.2325, box_loss: 0.2028, mask_loss: 0.2686, total_rpn_loss: 0.0477, rpn_score_loss: 0.0202, rpn_box_loss: 0.0275, total_loss: 0.7516\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][800/924]#011lr: 0.00960, eta: 0:23:51, step time: 0.400, total_loss_bbox: 0.4319, class_loss: 0.2339, box_loss: 0.1979, mask_loss: 0.2691, total_rpn_loss: 0.0481, rpn_score_loss: 0.0198, rpn_box_loss: 0.0282, total_loss: 0.7490\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][850/924]#011lr: 0.00937, eta: 0:23:54, step time: 0.406, total_loss_bbox: 0.4328, class_loss: 0.2355, box_loss: 0.1973, mask_loss: 0.2621, total_rpn_loss: 0.0494, rpn_score_loss: 0.0218, rpn_box_loss: 0.0275, total_loss: 0.7443\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [6][900/924]#011lr: 0.00914, eta: 0:23:12, step time: 0.400, total_loss_bbox: 0.4306, class_loss: 0.2324, box_loss: 0.1982, mask_loss: 0.2742, total_rpn_loss: 0.0486, rpn_score_loss: 0.0202, rpn_box_loss: 0.0285, total_loss: 0.7535\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/006/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 7 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][50/924]#011lr: 0.00880, eta: 0:24:14, step time: 0.427, total_loss_bbox: 0.4212, class_loss: 0.2282, box_loss: 0.1930, mask_loss: 0.2626, total_rpn_loss: 0.0475, rpn_score_loss: 0.0192, rpn_box_loss: 0.0283, total_loss: 0.7313\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][100/924]#011lr: 0.00858, eta: 0:22:23, step time: 0.400, total_loss_bbox: 0.4400, class_loss: 0.2433, box_loss: 0.1967, mask_loss: 0.2705, total_rpn_loss: 0.0488, rpn_score_loss: 0.0210, rpn_box_loss: 0.0278, total_loss: 0.7593\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][150/924]#011lr: 0.00835, eta: 0:22:06, step time: 0.401, total_loss_bbox: 0.4414, class_loss: 0.2418, box_loss: 0.1996, mask_loss: 0.2661, total_rpn_loss: 0.0474, rpn_score_loss: 0.0201, rpn_box_loss: 0.0273, total_loss: 0.7550\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][200/924]#011lr: 0.00813, eta: 0:22:07, step time: 0.408, total_loss_bbox: 0.4253, class_loss: 0.2274, box_loss: 0.1978, mask_loss: 0.2660, total_rpn_loss: 0.0481, rpn_score_loss: 0.0204, rpn_box_loss: 0.0277, total_loss: 0.7394\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][250/924]#011lr: 0.00791, eta: 0:21:46, step time: 0.407, total_loss_bbox: 0.4106, class_loss: 0.2178, box_loss: 0.1928, mask_loss: 0.2637, total_rpn_loss: 0.0479, rpn_score_loss: 0.0208, rpn_box_loss: 0.0271, total_loss: 0.7223\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][300/924]#011lr: 0.00769, eta: 0:21:15, step time: 0.404, total_loss_bbox: 0.4072, class_loss: 0.2147, box_loss: 0.1925, mask_loss: 0.2591, total_rpn_loss: 0.0457, rpn_score_loss: 0.0193, rpn_box_loss: 0.0264, total_loss: 0.7120\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][350/924]#011lr: 0.00747, eta: 0:21:05, step time: 0.407, total_loss_bbox: 0.4191, class_loss: 0.2230, box_loss: 0.1961, mask_loss: 0.2668, total_rpn_loss: 0.0443, rpn_score_loss: 0.0184, rpn_box_loss: 0.0259, total_loss: 0.7302\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][400/924]#011lr: 0.00726, eta: 0:20:46, step time: 0.408, total_loss_bbox: 0.4173, class_loss: 0.2208, box_loss: 0.1965, mask_loss: 0.2645, total_rpn_loss: 0.0441, rpn_score_loss: 0.0167, rpn_box_loss: 0.0275, total_loss: 0.7260\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][450/924]#011lr: 0.00705, eta: 0:20:18, step time: 0.405, total_loss_bbox: 0.4270, class_loss: 0.2289, box_loss: 0.1981, mask_loss: 0.2740, total_rpn_loss: 0.0462, rpn_score_loss: 0.0194, rpn_box_loss: 0.0268, total_loss: 0.7472\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][500/924]#011lr: 0.00684, eta: 0:19:47, step time: 0.402, total_loss_bbox: 0.3971, class_loss: 0.2081, box_loss: 0.1890, mask_loss: 0.2521, total_rpn_loss: 0.0461, rpn_score_loss: 0.0184, rpn_box_loss: 0.0276, total_loss: 0.6952\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][550/924]#011lr: 0.00663, eta: 0:19:26, step time: 0.402, total_loss_bbox: 0.4282, class_loss: 0.2289, box_loss: 0.1993, mask_loss: 0.2684, total_rpn_loss: 0.0459, rpn_score_loss: 0.0184, rpn_box_loss: 0.0275, total_loss: 0.7425\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][600/924]#011lr: 0.00642, eta: 0:19:00, step time: 0.399, total_loss_bbox: 0.4044, class_loss: 0.2113, box_loss: 0.1931, mask_loss: 0.2590, total_rpn_loss: 0.0437, rpn_score_loss: 0.0174, rpn_box_loss: 0.0263, total_loss: 0.7070\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][650/924]#011lr: 0.00622, eta: 0:19:07, step time: 0.409, total_loss_bbox: 0.3994, class_loss: 0.2099, box_loss: 0.1895, mask_loss: 0.2599, total_rpn_loss: 0.0436, rpn_score_loss: 0.0176, rpn_box_loss: 0.0260, total_loss: 0.7029\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][700/924]#011lr: 0.00602, eta: 0:18:39, step time: 0.406, total_loss_bbox: 0.4201, class_loss: 0.2213, box_loss: 0.1988, mask_loss: 0.2714, total_rpn_loss: 0.0468, rpn_score_loss: 0.0204, rpn_box_loss: 0.0264, total_loss: 0.7383\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][750/924]#011lr: 0.00582, eta: 0:18:14, step time: 0.404, total_loss_bbox: 0.4237, class_loss: 0.2315, box_loss: 0.1922, mask_loss: 0.2664, total_rpn_loss: 0.0483, rpn_score_loss: 0.0229, rpn_box_loss: 0.0254, total_loss: 0.7385\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][800/924]#011lr: 0.00562, eta: 0:17:48, step time: 0.402, total_loss_bbox: 0.4099, class_loss: 0.2133, box_loss: 0.1967, mask_loss: 0.2679, total_rpn_loss: 0.0433, rpn_score_loss: 0.0176, rpn_box_loss: 0.0258, total_loss: 0.7212\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][850/924]#011lr: 0.00543, eta: 0:17:29, step time: 0.403, total_loss_bbox: 0.4178, class_loss: 0.2227, box_loss: 0.1951, mask_loss: 0.2633, total_rpn_loss: 0.0439, rpn_score_loss: 0.0189, rpn_box_loss: 0.0249, total_loss: 0.7249\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [7][900/924]#011lr: 0.00524, eta: 0:17:25, step time: 0.409, total_loss_bbox: 0.4052, class_loss: 0.2146, box_loss: 0.1905, mask_loss: 0.2579, total_rpn_loss: 0.0435, rpn_score_loss: 0.0178, rpn_box_loss: 0.0257, total_loss: 0.7065\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/007/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 8 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][50/924]#011lr: 0.00496, eta: 0:17:31, step time: 0.424, total_loss_bbox: 0.3943, class_loss: 0.2070, box_loss: 0.1873, mask_loss: 0.2520, total_rpn_loss: 0.0437, rpn_score_loss: 0.0174, rpn_box_loss: 0.0262, total_loss: 0.6900\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][100/924]#011lr: 0.00477, eta: 0:16:19, step time: 0.403, total_loss_bbox: 0.4022, class_loss: 0.2129, box_loss: 0.1893, mask_loss: 0.2588, total_rpn_loss: 0.0410, rpn_score_loss: 0.0167, rpn_box_loss: 0.0242, total_loss: 0.7019\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][150/924]#011lr: 0.00459, eta: 0:16:05, step time: 0.405, total_loss_bbox: 0.3856, class_loss: 0.1982, box_loss: 0.1874, mask_loss: 0.2470, total_rpn_loss: 0.0414, rpn_score_loss: 0.0166, rpn_box_loss: 0.0248, total_loss: 0.6740\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][200/924]#011lr: 0.00441, eta: 0:15:52, step time: 0.408, total_loss_bbox: 0.3970, class_loss: 0.2085, box_loss: 0.1886, mask_loss: 0.2581, total_rpn_loss: 0.0424, rpn_score_loss: 0.0170, rpn_box_loss: 0.0254, total_loss: 0.6975\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][250/924]#011lr: 0.00424, eta: 0:15:33, step time: 0.409, total_loss_bbox: 0.4018, class_loss: 0.2121, box_loss: 0.1898, mask_loss: 0.2601, total_rpn_loss: 0.0441, rpn_score_loss: 0.0186, rpn_box_loss: 0.0255, total_loss: 0.7061\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][300/924]#011lr: 0.00406, eta: 0:14:55, step time: 0.401, total_loss_bbox: 0.3934, class_loss: 0.2076, box_loss: 0.1858, mask_loss: 0.2505, total_rpn_loss: 0.0422, rpn_score_loss: 0.0183, rpn_box_loss: 0.0239, total_loss: 0.6861\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][350/924]#011lr: 0.00389, eta: 0:14:39, step time: 0.403, total_loss_bbox: 0.3866, class_loss: 0.2008, box_loss: 0.1858, mask_loss: 0.2513, total_rpn_loss: 0.0411, rpn_score_loss: 0.0169, rpn_box_loss: 0.0242, total_loss: 0.6791\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][400/924]#011lr: 0.00373, eta: 0:14:26, step time: 0.407, total_loss_bbox: 0.4196, class_loss: 0.2268, box_loss: 0.1927, mask_loss: 0.2650, total_rpn_loss: 0.0420, rpn_score_loss: 0.0172, rpn_box_loss: 0.0248, total_loss: 0.7266\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][450/924]#011lr: 0.00356, eta: 0:14:01, step time: 0.404, total_loss_bbox: 0.4026, class_loss: 0.2134, box_loss: 0.1891, mask_loss: 0.2587, total_rpn_loss: 0.0410, rpn_score_loss: 0.0164, rpn_box_loss: 0.0246, total_loss: 0.7023\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][500/924]#011lr: 0.00340, eta: 0:13:57, step time: 0.412, total_loss_bbox: 0.4053, class_loss: 0.2175, box_loss: 0.1879, mask_loss: 0.2500, total_rpn_loss: 0.0399, rpn_score_loss: 0.0160, rpn_box_loss: 0.0239, total_loss: 0.6952\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][550/924]#011lr: 0.00324, eta: 0:13:27, step time: 0.408, total_loss_bbox: 0.4024, class_loss: 0.2109, box_loss: 0.1914, mask_loss: 0.2645, total_rpn_loss: 0.0397, rpn_score_loss: 0.0158, rpn_box_loss: 0.0239, total_loss: 0.7065\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][600/924]#011lr: 0.00309, eta: 0:13:07, step time: 0.408, total_loss_bbox: 0.4022, class_loss: 0.2147, box_loss: 0.1875, mask_loss: 0.2564, total_rpn_loss: 0.0432, rpn_score_loss: 0.0188, rpn_box_loss: 0.0243, total_loss: 0.7017\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][650/924]#011lr: 0.00294, eta: 0:12:35, step time: 0.401, total_loss_bbox: 0.3952, class_loss: 0.2092, box_loss: 0.1860, mask_loss: 0.2598, total_rpn_loss: 0.0412, rpn_score_loss: 0.0169, rpn_box_loss: 0.0243, total_loss: 0.6962\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][700/924]#011lr: 0.00279, eta: 0:12:25, step time: 0.407, total_loss_bbox: 0.4087, class_loss: 0.2151, box_loss: 0.1935, mask_loss: 0.2609, total_rpn_loss: 0.0433, rpn_score_loss: 0.0184, rpn_box_loss: 0.0249, total_loss: 0.7128\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][750/924]#011lr: 0.00265, eta: 0:12:08, step time: 0.409, total_loss_bbox: 0.3735, class_loss: 0.1911, box_loss: 0.1825, mask_loss: 0.2483, total_rpn_loss: 0.0378, rpn_score_loss: 0.0148, rpn_box_loss: 0.0231, total_loss: 0.6596\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][800/924]#011lr: 0.00251, eta: 0:11:43, step time: 0.406, total_loss_bbox: 0.3955, class_loss: 0.2082, box_loss: 0.1874, mask_loss: 0.2542, total_rpn_loss: 0.0382, rpn_score_loss: 0.0147, rpn_box_loss: 0.0234, total_loss: 0.6879\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][850/924]#011lr: 0.00237, eta: 0:11:22, step time: 0.406, total_loss_bbox: 0.3835, class_loss: 0.1978, box_loss: 0.1857, mask_loss: 0.2557, total_rpn_loss: 0.0398, rpn_score_loss: 0.0161, rpn_box_loss: 0.0237, total_loss: 0.6789\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [8][900/924]#011lr: 0.00224, eta: 0:11:04, step time: 0.407, total_loss_bbox: 0.4118, class_loss: 0.2240, box_loss: 0.1878, mask_loss: 0.2568, total_rpn_loss: 0.0421, rpn_score_loss: 0.0180, rpn_box_loss: 0.0241, total_loss: 0.7107\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/008/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 9 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][50/924]#011lr: 0.00205, eta: 0:11:02, step time: 0.425, total_loss_bbox: 0.3799, class_loss: 0.1978, box_loss: 0.1821, mask_loss: 0.2506, total_rpn_loss: 0.0385, rpn_score_loss: 0.0147, rpn_box_loss: 0.0238, total_loss: 0.6690\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][100/924]#011lr: 0.00192, eta: 0:10:11, step time: 0.406, total_loss_bbox: 0.3889, class_loss: 0.2016, box_loss: 0.1872, mask_loss: 0.2542, total_rpn_loss: 0.0406, rpn_score_loss: 0.0170, rpn_box_loss: 0.0236, total_loss: 0.6837\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][150/924]#011lr: 0.00180, eta: 0:09:50, step time: 0.405, total_loss_bbox: 0.3787, class_loss: 0.1956, box_loss: 0.1831, mask_loss: 0.2522, total_rpn_loss: 0.0383, rpn_score_loss: 0.0157, rpn_box_loss: 0.0227, total_loss: 0.6692\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][200/924]#011lr: 0.00168, eta: 0:09:35, step time: 0.409, total_loss_bbox: 0.3939, class_loss: 0.2047, box_loss: 0.1891, mask_loss: 0.2561, total_rpn_loss: 0.0401, rpn_score_loss: 0.0165, rpn_box_loss: 0.0236, total_loss: 0.6901\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][250/924]#011lr: 0.00157, eta: 0:09:11, step time: 0.406, total_loss_bbox: 0.3871, class_loss: 0.2022, box_loss: 0.1849, mask_loss: 0.2521, total_rpn_loss: 0.0398, rpn_score_loss: 0.0166, rpn_box_loss: 0.0232, total_loss: 0.6789\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][300/924]#011lr: 0.00146, eta: 0:08:46, step time: 0.402, total_loss_bbox: 0.3811, class_loss: 0.1962, box_loss: 0.1849, mask_loss: 0.2516, total_rpn_loss: 0.0402, rpn_score_loss: 0.0155, rpn_box_loss: 0.0246, total_loss: 0.6730\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][350/924]#011lr: 0.00135, eta: 0:08:36, step time: 0.411, total_loss_bbox: 0.3895, class_loss: 0.2051, box_loss: 0.1844, mask_loss: 0.2517, total_rpn_loss: 0.0391, rpn_score_loss: 0.0163, rpn_box_loss: 0.0228, total_loss: 0.6803\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][400/924]#011lr: 0.00125, eta: 0:08:09, step time: 0.405, total_loss_bbox: 0.3911, class_loss: 0.2070, box_loss: 0.1841, mask_loss: 0.2517, total_rpn_loss: 0.0418, rpn_score_loss: 0.0174, rpn_box_loss: 0.0244, total_loss: 0.6847\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][450/924]#011lr: 0.00116, eta: 0:07:48, step time: 0.404, total_loss_bbox: 0.3873, class_loss: 0.2057, box_loss: 0.1816, mask_loss: 0.2509, total_rpn_loss: 0.0380, rpn_score_loss: 0.0147, rpn_box_loss: 0.0234, total_loss: 0.6762\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][500/924]#011lr: 0.00106, eta: 0:07:23, step time: 0.400, total_loss_bbox: 0.3843, class_loss: 0.1989, box_loss: 0.1854, mask_loss: 0.2549, total_rpn_loss: 0.0376, rpn_score_loss: 0.0152, rpn_box_loss: 0.0224, total_loss: 0.6768\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][550/924]#011lr: 0.00097, eta: 0:07:04, step time: 0.401, total_loss_bbox: 0.3938, class_loss: 0.2084, box_loss: 0.1854, mask_loss: 0.2604, total_rpn_loss: 0.0387, rpn_score_loss: 0.0149, rpn_box_loss: 0.0238, total_loss: 0.6929\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][600/924]#011lr: 0.00088, eta: 0:06:44, step time: 0.401, total_loss_bbox: 0.3689, class_loss: 0.1879, box_loss: 0.1810, mask_loss: 0.2467, total_rpn_loss: 0.0373, rpn_score_loss: 0.0140, rpn_box_loss: 0.0233, total_loss: 0.6529\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][650/924]#011lr: 0.00080, eta: 0:06:30, step time: 0.408, total_loss_bbox: 0.3666, class_loss: 0.1857, box_loss: 0.1809, mask_loss: 0.2411, total_rpn_loss: 0.0357, rpn_score_loss: 0.0144, rpn_box_loss: 0.0214, total_loss: 0.6434\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][700/924]#011lr: 0.00072, eta: 0:06:07, step time: 0.405, total_loss_bbox: 0.3952, class_loss: 0.2105, box_loss: 0.1848, mask_loss: 0.2584, total_rpn_loss: 0.0408, rpn_score_loss: 0.0167, rpn_box_loss: 0.0241, total_loss: 0.6944\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][750/924]#011lr: 0.00065, eta: 0:05:50, step time: 0.408, total_loss_bbox: 0.3874, class_loss: 0.2021, box_loss: 0.1854, mask_loss: 0.2525, total_rpn_loss: 0.0384, rpn_score_loss: 0.0164, rpn_box_loss: 0.0220, total_loss: 0.6783\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][800/924]#011lr: 0.00058, eta: 0:05:30, step time: 0.408, total_loss_bbox: 0.3748, class_loss: 0.1956, box_loss: 0.1792, mask_loss: 0.2442, total_rpn_loss: 0.0357, rpn_score_loss: 0.0135, rpn_box_loss: 0.0222, total_loss: 0.6548\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][850/924]#011lr: 0.00051, eta: 0:05:13, step time: 0.414, total_loss_bbox: 0.3630, class_loss: 0.1834, box_loss: 0.1796, mask_loss: 0.2438, total_rpn_loss: 0.0396, rpn_score_loss: 0.0160, rpn_box_loss: 0.0236, total_loss: 0.6464\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [9][900/924]#011lr: 0.00045, eta: 0:04:50, step time: 0.410, total_loss_bbox: 0.3752, class_loss: 0.1924, box_loss: 0.1828, mask_loss: 0.2493, total_rpn_loss: 0.0385, rpn_score_loss: 0.0157, rpn_box_loss: 0.0228, total_loss: 0.6630\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/009/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Starting epoch: 10 of 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][50/924]#011lr: 0.00037, eta: 0:04:30, step time: 0.427, total_loss_bbox: 0.3879, class_loss: 0.2062, box_loss: 0.1817, mask_loss: 0.2499, total_rpn_loss: 0.0373, rpn_score_loss: 0.0148, rpn_box_loss: 0.0225, total_loss: 0.6751\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][100/924]#011lr: 0.00032, eta: 0:04:02, step time: 0.415, total_loss_bbox: 0.3870, class_loss: 0.2003, box_loss: 0.1867, mask_loss: 0.2593, total_rpn_loss: 0.0391, rpn_score_loss: 0.0160, rpn_box_loss: 0.0231, total_loss: 0.6853\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][150/924]#011lr: 0.00027, eta: 0:03:35, step time: 0.404, total_loss_bbox: 0.3778, class_loss: 0.1954, box_loss: 0.1823, mask_loss: 0.2490, total_rpn_loss: 0.0362, rpn_score_loss: 0.0146, rpn_box_loss: 0.0216, total_loss: 0.6630\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][200/924]#011lr: 0.00023, eta: 0:03:17, step time: 0.409, total_loss_bbox: 0.3758, class_loss: 0.1912, box_loss: 0.1846, mask_loss: 0.2486, total_rpn_loss: 0.0374, rpn_score_loss: 0.0153, rpn_box_loss: 0.0221, total_loss: 0.6618\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][250/924]#011lr: 0.00019, eta: 0:02:57, step time: 0.409, total_loss_bbox: 0.3592, class_loss: 0.1825, box_loss: 0.1767, mask_loss: 0.2423, total_rpn_loss: 0.0350, rpn_score_loss: 0.0130, rpn_box_loss: 0.0221, total_loss: 0.6365\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][300/924]#011lr: 0.00015, eta: 0:02:35, step time: 0.406, total_loss_bbox: 0.3885, class_loss: 0.2063, box_loss: 0.1822, mask_loss: 0.2502, total_rpn_loss: 0.0385, rpn_score_loss: 0.0158, rpn_box_loss: 0.0227, total_loss: 0.6772\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][350/924]#011lr: 0.00012, eta: 0:02:17, step time: 0.412, total_loss_bbox: 0.3813, class_loss: 0.1968, box_loss: 0.1845, mask_loss: 0.2552, total_rpn_loss: 0.0400, rpn_score_loss: 0.0171, rpn_box_loss: 0.0229, total_loss: 0.6766\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][400/924]#011lr: 0.00010, eta: 0:01:56, step time: 0.409, total_loss_bbox: 0.3735, class_loss: 0.1958, box_loss: 0.1777, mask_loss: 0.2464, total_rpn_loss: 0.0357, rpn_score_loss: 0.0140, rpn_box_loss: 0.0217, total_loss: 0.6556\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][450/924]#011lr: 0.00007, eta: 0:01:34, step time: 0.403, total_loss_bbox: 0.3711, class_loss: 0.1887, box_loss: 0.1824, mask_loss: 0.2501, total_rpn_loss: 0.0373, rpn_score_loss: 0.0149, rpn_box_loss: 0.0224, total_loss: 0.6585\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][500/924]#011lr: 0.00006, eta: 0:01:15, step time: 0.408, total_loss_bbox: 0.3563, class_loss: 0.1800, box_loss: 0.1763, mask_loss: 0.2482, total_rpn_loss: 0.0353, rpn_score_loss: 0.0129, rpn_box_loss: 0.0224, total_loss: 0.6398\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][550/924]#011lr: 0.00004, eta: 0:00:54, step time: 0.405, total_loss_bbox: 0.3560, class_loss: 0.1786, box_loss: 0.1774, mask_loss: 0.2409, total_rpn_loss: 0.0348, rpn_score_loss: 0.0134, rpn_box_loss: 0.0215, total_loss: 0.6317\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][600/924]#011lr: 0.00003, eta: 0:00:34, step time: 0.407, total_loss_bbox: 0.3669, class_loss: 0.1921, box_loss: 0.1748, mask_loss: 0.2403, total_rpn_loss: 0.0369, rpn_score_loss: 0.0143, rpn_box_loss: 0.0226, total_loss: 0.6442\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Epoch [10][650/924]#011lr: 0.00003, eta: 0:00:14, step time: 0.413, total_loss_bbox: 0.3888, class_loss: 0.2037, box_loss: 0.1851, mask_loss: 0.2517, total_rpn_loss: 0.0397, rpn_score_loss: 0.0168, rpn_box_loss: 0.0229, total_loss: 0.6803\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 15:49:03.425390: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 15:49:03.427605: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 15:49:03.429166: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 15:49:03.429397: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 15:49:03.429793: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 15:49:03.430163: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 15:49:03.430337: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 15:49:03.431057: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 15:49:03.438162: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 15:49:03.439270: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 15:49:03.446540: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 15:49:03.446975: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 15:49:03.460942: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 15:49:03.488725: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 15:49:03.523346: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/010/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Saved checkpoint at: /opt/ml/checkpoints/trained_model/model.h5\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:End time: 2021-11-12 14:45:34.781551\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Elapsed time: 1:03:29.227159\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Running eval for epoch 10\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 15:49:04.127142: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at partitioned_function_ops.cc:114 : Not found: Failed to find definition for function \"cluster_210\"\u001b[0m\n",
      "\u001b[34m[1,10]<stderr>:2021-11-12 15:49:06.026878: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,5]<stderr>:2021-11-12 15:49:06.037898: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,13]<stderr>:2021-11-12 15:49:06.050127: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,15]<stderr>:2021-11-12 15:49:06.050387: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,7]<stderr>:2021-11-12 15:49:06.057200: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,12]<stderr>:2021-11-12 15:49:06.064188: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,4]<stderr>:2021-11-12 15:49:06.064134: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,14]<stderr>:2021-11-12 15:49:06.065758: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,6]<stderr>:2021-11-12 15:49:06.072786: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,11]<stderr>:2021-11-12 15:49:06.092472: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,1]<stderr>:2021-11-12 15:49:06.127004: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,2]<stderr>:2021-11-12 15:49:06.181737: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,3]<stderr>:2021-11-12 15:49:06.197351: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,9]<stderr>:2021-11-12 15:49:06.460112: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,8]<stderr>:2021-11-12 15:49:06.555033: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:2021-11-12 15:49:07.344316: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Processing final eval\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:Running Evaluation for 5000 images\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:{'bbox': {'bbox AP 0.50:0.95 all': 0.37777172464257047}, 'segm': {'segm AP 0.50:0.95 all': 0.3449911449009801}}\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:bbox bbox AP 0.50:0.95 all: 0.37777172464257047\u001b[0m\n",
      "\u001b[34m[1,0]<stderr>:INFO:sagemakercv.utils.runner.runner:segm segm AP 0.50:0.95 all: 0.3449911449009801\n",
      "\u001b[0m\n",
      "\n",
      "2021-11-12 15:50:44 Uploading - Uploading generated training model\n",
      "2021-11-12 15:50:44 Completed - Training job completed\n",
      "Training seconds: 8284\n",
      "Billable seconds: 8284\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(channels, wait=True, job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Visualizing Results\n",
    "\n",
    "And there you have it, a fully trained Mask RCNN model in about an hour. Now let's see how our model does on prediction by actually visualizing the output.\n",
    "\n",
    "Our model is stored at the S3 location we gave to the training job in `output_path`. The checkpointer hook creates a `trained_model` directory and stores the final checkpoint there. We'll need to grab the results and store them on our studio instance so we can check performance, and visualize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = os.path.join(estimator.output_path, 'trained_model', 'model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the model from S3 to our Studio instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3fs.get(model_loc, model_loc.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the trained model weights into the detector model we created earlier for the local training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.load_weights(model_loc.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did for the local model, let's grab a random image from the dataset and visualize the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(dataset)\n",
    "result = detector(features, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_num = 2 # image number within the batch\n",
    "image = restore_image(result['images'][image_num], features['image_info'][image_num]) # converts the image back to its original shape and color\n",
    "boxes = result['detection_boxes'][image_num]\n",
    "classes = result['detection_classes'][image_num]\n",
    "scores = result['detection_scores'][image_num]\n",
    "detection_image = build_image(image, boxes, scores, classes, coco_categories, threshold=0.8)\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(detection_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "In this notebook, we've walked through the entire process of training Mask RCNN on SageMaker. We've implemented several of SageMaker's more advanced features, such as distributed training, EFA, and streaming data directly from S3. From here you can use the provided template datasets to train on your own data, or modify the framework with your own object detection model.\n",
    "\n",
    "When you're done, make sure to check that all of your SageMaker training jobs have stopped by checking the [SageMaker Training Console](https://us-west-2.console.aws.amazon.com/sagemaker). Also check that you've stopped any Studio instance you have running by selecting the session monitor on the left (the circle with a square in it), and clicking the power button next to any running instances. Your files will still be saved on the Studio EBS volume.\n",
    "\n",
    "<img src=\"../assets/running_instances.png\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
