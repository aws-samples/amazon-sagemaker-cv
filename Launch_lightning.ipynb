{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe7e64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803b1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_REGION'] = 'us-east-1'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4a96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "region = os.environ['AWS_REGION'] #boto3.session.Session().region_name\n",
    "boto_sess = boto3.Session()\n",
    "sm = boto_sess.client('sagemaker')\n",
    "\n",
    "s3_bucket = \"s3://jbsnyder-sagemaker-us-east/\"\n",
    "s3_output_bucket = \"s3://lilianwa-sagemaker-useast1\"\n",
    "\n",
    "base_job_name = \"lilianwa-mlperf-mrcnn-lightning\"\n",
    "date_str = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "time_str = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "job_name = f\"{base_job_name}-{time_str}\"\n",
    "\n",
    "output_path = os.path.join(s3_output_bucket, \"sagemaker-output\", date_str, job_name)\n",
    "code_location = os.path.join(s3_output_bucket, \"sagemaker-code\", date_str, job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a3ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = 'ml.p4d.24xlarge'\n",
    "instance_count = 1\n",
    "repo = \"jbsnyder\"\n",
    "tag = \"pytorch-mrcnn\"\n",
    "#account = os.popen(f\"aws sts get-caller-identity --region {region} --endpoint-url https://sts.{region}.amazonaws.com --query Account --output text\").read().strip()\n",
    "#image_uri = f\"{account}.dkr.ecr.{region}.amazonaws.com/{repo}:{tag}\"\n",
    "image_uri = \"920076894685.dkr.ecr.us-east-1.amazonaws.com/lilianwa:mlperf2lightning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e6cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_strategy = \"nccl\"\n",
    "# assert dist_strategy in [\"smddp\", \"nccl\"]\n",
    "# For now, just using NCCL not SMDDP\n",
    "assert dist_strategy in [\"nccl\"]\n",
    "hyperparameters = {\"config-file\": f'configs/e2e_mask_rcnn_R_50_FPN_1x_{instance_count}_node_test.yaml'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b45fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dist_strategy==\"nccl\":\n",
    "    distribution=None\n",
    "    entry_point=\"launch_ddp_lightning.py\"\n",
    "    # hyperparameters['training_script']=\"aws_train_mlperf.py\"\n",
    "else:\n",
    "    distribution={ \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }\n",
    "    entry_point = \"aws_train_mlperf.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ee38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = {\"all_data\": os.path.join(s3_bucket, \"data\", \"yolo/\"),\n",
    "            \"annotations\": os.path.join(s3_bucket, \"data\", \"coco\", \"annotations/\"),\n",
    "            \"weights\": os.path.join(s3_bucket, \"data\", \"weights\", \"pt-resnet/\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f8c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    source_dir=\"./src\",\n",
    "    entry_point=entry_point,\n",
    "    base_job_name=job_name,\n",
    "    role=get_execution_role(),\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    distribution=distribution,\n",
    "    # volume_size=400, # Not necessary for P4d\n",
    "    max_run=7200,\n",
    "    hyperparameters=hyperparameters,\n",
    "    image_uri=image_uri,\n",
    "    output_path=os.path.join(output_path, 'training-output'),\n",
    "    checkpoint_s3_uri=os.path.join(output_path, 'training-checkpoints'),\n",
    "    model_dir=os.path.join(output_path, 'training-model'),\n",
    "    code_location=code_location,\n",
    "    input_mode='File',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4451278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs=channels,\n",
    "    wait=False,\n",
    "    job_name=job_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a8add0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-28 21:04:16 Starting - Starting the training job...ProfilerReport-1659042256: InProgress\n",
      "...\n",
      "2022-07-28 21:04:59 Starting - Preparing the instances for training.............................................\n",
      "2022-07-28 21:12:41 Downloading - Downloading input data...................................................\n",
      "2022-07-28 21:21:24 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34msed: can't read changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: fatal error: no input files\u001b[0m\n",
      "\u001b[34mcompilation terminated.\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.o: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-07-28 21:21:14,949 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"all_data\": \"/opt/ml/input/data/all_data\",\n",
      "        \"annotations\": \"/opt/ml/input/data/annotations\",\n",
      "        \"weights\": \"/opt/ml/input/data/weights\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config-file\": \"configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"all_data\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"annotations\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"weights\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://lilianwa-sagemaker-useast1/sagemaker-code/28-07-2022/lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14/lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"launch_ddp_lightning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"launch_ddp_lightning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config-file\":\"configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=launch_ddp_lightning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"all_data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"annotations\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"weights\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"all_data\",\"annotations\",\"weights\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=launch_ddp_lightning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://lilianwa-sagemaker-useast1/sagemaker-code/28-07-2022/lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14/lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"all_data\":\"/opt/ml/input/data/all_data\",\"annotations\":\"/opt/ml/input/data/annotations\",\"weights\":\"/opt/ml/input/data/weights\"},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config-file\":\"configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"all_data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"annotations\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"weights\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://lilianwa-sagemaker-useast1/sagemaker-code/28-07-2022/lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14/lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14/source/sourcedir.tar.gz\",\"module_name\":\"launch_ddp_lightning\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"launch_ddp_lightning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config-file\",\"configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ALL_DATA=/opt/ml/input/data/all_data\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_ANNOTATIONS=/opt/ml/input/data/annotations\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_WEIGHTS=/opt/ml/input/data/weights\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG-FILE=configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/torchtext-0.13.0a0-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.18b20220706-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/workspace/maskrcnn:/workspace/cocoapi/PythonAPI\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python launch_ddp_lightning.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mUnarchiving COCO data\u001b[0m\n",
      "\u001b[34m['coco', 'coco.tar']\u001b[0m\n",
      "\u001b[34mFollowing command will be executed: \n",
      " export FI_PROVIDER=efa && export FI_EFA_TX_MIN_CREDITS=64 && export NCCL_DEBUG=INFO &&export NCCL_TREE_THRESHOLD=0 &&export NCCL_SOCKET_IFNAME=eth0 &&export FI_EFA_USE_DEVICE_RDMA=1 &&torchrun --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=algo-1 --master_port=55555 /opt/ml/code/train_lightning.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mNCCL version 2.12.10+cuda11.6\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:110 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:105 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:107 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO Bootstrap : Using eth0:10.0.238.231<0>\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v5 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:106 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:109 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:111 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:108 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 00 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 01 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 02 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 02 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 03 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 04 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 04 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 04 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 04 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 04 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 04 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 04 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 04 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 05 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 05 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 05 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 05 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 05 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 05 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 05 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 05 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 06 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 06 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 06 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 06 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 06 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 06 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 06 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 06 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 07 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 07 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 07 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 07 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 07 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 07 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 07 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 07 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 08 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 08 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 08 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 08 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 08 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 08 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 08 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 08 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 09 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 09 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 09 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 09 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 09 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 09 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 09 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 09 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 10 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 10 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 10 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 10 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 10 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 10 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 10 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 10 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 11 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 11 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 11 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 11 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 11 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 11 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 11 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 11 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 12 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 12 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 12 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 12 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 12 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 12 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 12 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 12 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 13 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 13 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 13 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 13 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 13 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 13 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 13 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 13 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 14 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 14 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 14 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 14 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 14 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 14 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 14 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 14 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 15 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 15 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 15 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 15 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 15 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 15 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 15 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 15 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 16 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 16 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 16 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 16 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 16 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 16 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 16 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 16 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 17 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 17 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 17 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 17 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 17 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 17 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 17 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 17 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 18 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 18 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 18 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 18 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 18 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 18 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 18 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 18 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 19 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 19 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 19 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 19 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 19 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 19 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 19 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 19 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 20 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 20 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 20 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 20 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 20 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 20 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 20 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 20 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 21 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 21 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 21 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 21 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 21 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 21 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 21 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 21 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 22 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 22 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 22 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 22 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 22 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 22 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 22 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 22 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 23 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 23 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 23 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 23 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 23 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 23 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 23 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Channel 23 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 04 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 05 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 06 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 07 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 08 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 09 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 10 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 11 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 12 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 13 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 14 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 15 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 16 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 17 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 18 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 19 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 20 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 21 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 22 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Channel 23 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 01 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 03 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 04 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 04 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 04 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 04 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 04 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 04 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 05 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 05 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 05 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 05 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 05 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 05 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 06 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 06 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 06 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 06 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 06 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 06 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 07 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 07 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 07 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 07 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 07 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 07 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 08 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 08 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 08 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 08 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 08 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 08 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 09 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 09 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 09 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 09 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 09 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 09 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 10 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 10 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 10 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 10 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 10 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 10 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 11 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 11 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 11 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 11 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 11 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 11 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 12 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 12 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 12 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 12 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 12 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 12 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 13 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 13 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 13 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 13 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 13 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 13 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 14 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 14 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 14 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 14 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 14 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 14 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 15 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 15 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 15 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 15 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 15 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 15 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 16 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 16 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 16 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 16 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 16 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 16 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 17 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 17 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 17 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 17 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 17 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 17 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 18 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 18 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 18 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 18 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 18 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 18 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 19 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 19 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 19 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 19 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 19 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 19 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 20 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 20 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 20 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 20 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 20 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 20 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 21 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 21 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 21 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 21 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 21 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 21 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 22 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 22 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 22 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 22 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 22 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 22 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Channel 23 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Channel 23 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Channel 23 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Channel 23 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Channel 23 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Channel 23 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:107:164 [3] NCCL INFO comm 0x7f0880008fb0 rank 3 nranks 8 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:111:167 [7] NCCL INFO comm 0x7f3658008fb0 rank 7 nranks 8 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:161 [0] NCCL INFO comm 0x7fbaf0008fb0 rank 0 nranks 8 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:109:166 [5] NCCL INFO comm 0x7fb060008fb0 rank 5 nranks 8 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:110:162 [6] NCCL INFO comm 0x7f3028008fb0 rank 6 nranks 8 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:108:168 [4] NCCL INFO comm 0x7f9950008fb0 rank 4 nranks 8 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:106:165 [2] NCCL INFO comm 0x7f4f14008fb0 rank 2 nranks 8 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:104:104 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34mip-10-0-238-231:105:163 [1] NCCL INFO comm 0x7f63b4008fb0 rank 1 nranks 8 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311052, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 56, \"tensor\": \"FPN_inner_block1\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311138, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 59, \"tensor\": \"FPN_layer_block1\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311140, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 56, \"tensor\": \"FPN_inner_block2\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311150, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 59, \"tensor\": \"FPN_layer_block2\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311153, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 56, \"tensor\": \"FPN_inner_block3\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311163, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 59, \"tensor\": \"FPN_layer_block3\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311169, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 56, \"tensor\": \"FPN_inner_block4\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311179, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/backbone/fpn.py\", \"lineno\": 59, \"tensor\": \"FPN_layer_block4\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311188, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", \"lineno\": 105, \"tensor\": \"RPNHead_conv\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311188, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", \"lineno\": 106, \"tensor\": \"RPNHead_cls\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311188, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", \"lineno\": 107, \"tensor\": \"RPNHead_bbox\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311346, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/box_head/roi_box_feature_extractors.py\", \"lineno\": 95, \"tensor\": \"ROI_BOX_FEATURE_EXTRACTOR_fc6\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311360, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/box_head/roi_box_feature_extractors.py\", \"lineno\": 98, \"tensor\": \"ROI_BOX_FEATURE_EXTRACTOR_fc7\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311363, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/box_head/roi_box_predictors.py\", \"lineno\": 50, \"tensor\": \"ROI_BOX_PREDICTOR_cls\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311365, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/box_head/roi_box_predictors.py\", \"lineno\": 52, \"tensor\": \"ROI_BOX_PREDICTOR_bbox\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311380, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_feature_extractors.py\", \"lineno\": 64, \"tensor\": \"ROI_MASK_FEATURE_EXTRACTOR_fcn1\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311392, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_feature_extractors.py\", \"lineno\": 64, \"tensor\": \"ROI_MASK_FEATURE_EXTRACTOR_fcn2\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311405, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_feature_extractors.py\", \"lineno\": 64, \"tensor\": \"ROI_MASK_FEATURE_EXTRACTOR_fcn3\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311417, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_feature_extractors.py\", \"lineno\": 64, \"tensor\": \"ROI_MASK_FEATURE_EXTRACTOR_fcn4\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311422, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_predictors.py\", \"lineno\": 54, \"tensor\": \"ROI_MASK_PREDICTOR_fcn5\"}}\u001b[0m\n",
      "\u001b[34m:::MLLOG {\"namespace\": \"\", \"time_ms\": 1659043311423, \"event_type\": \"POINT_IN_TIME\", \"key\": \"weights_initialization\", \"value\": null, \"metadata\": {\"file\": \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/roi_heads/mask_head/roi_mask_predictors.py\", \"lineno\": 55, \"tensor\": \"ROI_MASK_PREDICTOR_fcn_logits\"}}\u001b[0m\n",
      "\u001b[34mGPU available: True, used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34m| Name  | Type            | Params\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m0 | model | GeneralizedRCNN | 44.3 M\u001b[0m\n",
      "\u001b[34m------------------------------------------\u001b[0m\n",
      "\u001b[34m44.1 M    Trainable params\u001b[0m\n",
      "\u001b[34m222 K     Non-trainable params\u001b[0m\n",
      "\u001b[34m44.3 M    Total params\u001b[0m\n",
      "\u001b[34m177.390   Total estimated model params size (MB)\u001b[0m\n",
      "\u001b[34m[(800, 1344), (1344, 800)]\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mloading annotations into memory...\u001b[0m\n",
      "\u001b[34mDone (t=13.54s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mDone (t=13.46s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mDone (t=13.49s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mDone (t=14.05s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mDone (t=13.92s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mDone (t=14.05s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mDone (t=13.99s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mDone (t=14.00s)\u001b[0m\n",
      "\u001b[34mcreating index...\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mindex created!\u001b[0m\n",
      "\u001b[34mTraining: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]#015Training: 0it [00:00, ?it/s]#015Epoch 0: : 0it [00:00, ?it/s]Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 180, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 175, in main\u001b[0m\n",
      "\u001b[34mtrainer.fit(model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\u001b[0m\n",
      "\u001b[34mself._call_and_handle_interrupt(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\u001b[0m\n",
      "\u001b[34mreturn trainer_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\u001b[0m\n",
      "\u001b[34mresults = self._run(model, ckpt_path=self.ckpt_path)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\u001b[0m\n",
      "\u001b[34mresults = self._run_stage()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\u001b[0m\n",
      "\u001b[34mreturn self._run_train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\u001b[0m\n",
      "\u001b[34mself.fit_loop.run()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\u001b[0m\n",
      "\u001b[34mself._outputs = self.epoch_loop.run(self._data_fetcher)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\u001b[0m\n",
      "\u001b[34mbatch_output = self.batch_loop.run(batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 90, in advance\u001b[0m\n",
      "\u001b[34moutputs = self.manual_loop.run(split_batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 115, in advance\u001b[0m\n",
      "\u001b[34mtraining_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\u001b[0m\n",
      "\u001b[34moutput = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\u001b[0m\n",
      "\u001b[34mreturn self.model.training_step(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 191, in training_step\u001b[0m\n",
      "\u001b[34mloss_dict = self(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 49, in forward\u001b[0m\n",
      "\u001b[34mreturn self.model(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 199, in forward\u001b[0m\n",
      "\u001b[34mflat_res = self.graphable(images.tensors, images.image_sizes_tensor)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 33, in forward\u001b[0m\n",
      "\u001b[34mobjectness, rpn_box_regression = self.head(features)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", line 138, in forward\u001b[0m\n",
      "\u001b[34mlogit = self.cls_logits(t)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/misc.py\", line 42, in forward\u001b[0m\n",
      "\u001b[34mreturn conv.Conv2d_NHWC.forward(self, x)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 223, in forward\u001b[0m\n",
      "\u001b[34mresult = conv2d_NHWC_impl.apply(x, self.weight, None, self.stride,\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 67, in forward\u001b[0m\n",
      "\u001b[34moutput =  NHWC.cudnn_convolution_nhwc(x, padded_w,\u001b[0m\n",
      "\u001b[34mRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 5 does not equal 0 (while checking arguments for cudnn_convolution_nhwc)\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 180, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 175, in main\u001b[0m\n",
      "\u001b[34mtrainer.fit(model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\u001b[0m\n",
      "\u001b[34mself._call_and_handle_interrupt(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\u001b[0m\n",
      "\u001b[34mreturn trainer_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\u001b[0m\n",
      "\u001b[34mresults = self._run(model, ckpt_path=self.ckpt_path)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\u001b[0m\n",
      "\u001b[34mresults = self._run_stage()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\u001b[0m\n",
      "\u001b[34mreturn self._run_train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\u001b[0m\n",
      "\u001b[34mself.fit_loop.run()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\u001b[0m\n",
      "\u001b[34mself._outputs = self.epoch_loop.run(self._data_fetcher)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\u001b[0m\n",
      "\u001b[34mbatch_output = self.batch_loop.run(batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 90, in advance\u001b[0m\n",
      "\u001b[34moutputs = self.manual_loop.run(split_batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 115, in advance\u001b[0m\n",
      "\u001b[34mtraining_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\u001b[0m\n",
      "\u001b[34moutput = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\u001b[0m\n",
      "\u001b[34mreturn self.model.training_step(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 191, in training_step\u001b[0m\n",
      "\u001b[34mloss_dict = self(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 49, in forward\u001b[0m\n",
      "\u001b[34mreturn self.model(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 199, in forward\u001b[0m\n",
      "\u001b[34mflat_res = self.graphable(images.tensors, images.image_sizes_tensor)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 33, in forward\u001b[0m\n",
      "\u001b[34mobjectness, rpn_box_regression = self.head(features)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", line 138, in forward\u001b[0m\n",
      "\u001b[34mlogit = self.cls_logits(t)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/misc.py\", line 42, in forward\u001b[0m\n",
      "\u001b[34mreturn conv.Conv2d_NHWC.forward(self, x)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 223, in forward\u001b[0m\n",
      "\u001b[34mresult = conv2d_NHWC_impl.apply(x, self.weight, None, self.stride,\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 67, in forward\u001b[0m\n",
      "\u001b[34moutput =  NHWC.cudnn_convolution_nhwc(x, padded_w,\u001b[0m\n",
      "\u001b[34mRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 4 does not equal 0 (while checking arguments for cudnn_convolution_nhwc)\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 180, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 175, in main\u001b[0m\n",
      "\u001b[34mtrainer.fit(model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\u001b[0m\n",
      "\u001b[34mself._call_and_handle_interrupt(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\u001b[0m\n",
      "\u001b[34mreturn trainer_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\u001b[0m\n",
      "\u001b[34mresults = self._run(model, ckpt_path=self.ckpt_path)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\u001b[0m\n",
      "\u001b[34mresults = self._run_stage()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\u001b[0m\n",
      "\u001b[34mreturn self._run_train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\u001b[0m\n",
      "\u001b[34mself.fit_loop.run()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\u001b[0m\n",
      "\u001b[34mself._outputs = self.epoch_loop.run(self._data_fetcher)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\u001b[0m\n",
      "\u001b[34mbatch_output = self.batch_loop.run(batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 90, in advance\u001b[0m\n",
      "\u001b[34moutputs = self.manual_loop.run(split_batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 115, in advance\u001b[0m\n",
      "\u001b[34mtraining_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\u001b[0m\n",
      "\u001b[34moutput = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\u001b[0m\n",
      "\u001b[34mreturn self.model.training_step(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 191, in training_step\u001b[0m\n",
      "\u001b[34mloss_dict = self(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 49, in forward\u001b[0m\n",
      "\u001b[34mreturn self.model(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 199, in forward\u001b[0m\n",
      "\u001b[34mflat_res = self.graphable(images.tensors, images.image_sizes_tensor)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 33, in forward\u001b[0m\n",
      "\u001b[34mobjectness, rpn_box_regression = self.head(features)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", line 138, in forward\u001b[0m\n",
      "\u001b[34mlogit = self.cls_logits(t)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/misc.py\", line 42, in forward\u001b[0m\n",
      "\u001b[34mreturn conv.Conv2d_NHWC.forward(self, x)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 223, in forward\u001b[0m\n",
      "\u001b[34mresult = conv2d_NHWC_impl.apply(x, self.weight, None, self.stride,\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 67, in forward\u001b[0m\n",
      "\u001b[34moutput =  NHWC.cudnn_convolution_nhwc(x, padded_w,\u001b[0m\n",
      "\u001b[34mRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 7 does not equal 0 (while checking arguments for cudnn_convolution_nhwc)\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 180, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 175, in main\u001b[0m\n",
      "\u001b[34mtrainer.fit(model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\u001b[0m\n",
      "\u001b[34mself._call_and_handle_interrupt(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\u001b[0m\n",
      "\u001b[34mreturn trainer_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\u001b[0m\n",
      "\u001b[34mresults = self._run(model, ckpt_path=self.ckpt_path)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\u001b[0m\n",
      "\u001b[34mresults = self._run_stage()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\u001b[0m\n",
      "\u001b[34mreturn self._run_train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\u001b[0m\n",
      "\u001b[34mself.fit_loop.run()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\u001b[0m\n",
      "\u001b[34mself._outputs = self.epoch_loop.run(self._data_fetcher)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\u001b[0m\n",
      "\u001b[34mbatch_output = self.batch_loop.run(batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 90, in advance\u001b[0m\n",
      "\u001b[34moutputs = self.manual_loop.run(split_batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 115, in advance\u001b[0m\n",
      "\u001b[34mtraining_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\u001b[0m\n",
      "\u001b[34moutput = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\u001b[0m\n",
      "\u001b[34mreturn self.model.training_step(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 191, in training_step\u001b[0m\n",
      "\u001b[34mloss_dict = self(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 49, in forward\u001b[0m\n",
      "\u001b[34mreturn self.model(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 199, in forward\u001b[0m\n",
      "\u001b[34mflat_res = self.graphable(images.tensors, images.image_sizes_tensor)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 33, in forward\u001b[0m\n",
      "\u001b[34mobjectness, rpn_box_regression = self.head(features)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", line 138, in forward\u001b[0m\n",
      "\u001b[34mlogit = self.cls_logits(t)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/misc.py\", line 42, in forward\u001b[0m\n",
      "\u001b[34mreturn conv.Conv2d_NHWC.forward(self, x)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 223, in forward\u001b[0m\n",
      "\u001b[34mresult = conv2d_NHWC_impl.apply(x, self.weight, None, self.stride,\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 67, in forward\u001b[0m\n",
      "\u001b[34moutput =  NHWC.cudnn_convolution_nhwc(x, padded_w,\u001b[0m\n",
      "\u001b[34mRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 6 does not equal 0 (while checking arguments for cudnn_convolution_nhwc)\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 180, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 175, in main\u001b[0m\n",
      "\u001b[34mtrainer.fit(model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\u001b[0m\n",
      "\u001b[34mself._call_and_handle_interrupt(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\u001b[0m\n",
      "\u001b[34mreturn trainer_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\u001b[0m\n",
      "\u001b[34mresults = self._run(model, ckpt_path=self.ckpt_path)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\u001b[0m\n",
      "\u001b[34mresults = self._run_stage()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\u001b[0m\n",
      "\u001b[34mreturn self._run_train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\u001b[0m\n",
      "\u001b[34mself.fit_loop.run()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\u001b[0m\n",
      "\u001b[34mself._outputs = self.epoch_loop.run(self._data_fetcher)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\u001b[0m\n",
      "\u001b[34mbatch_output = self.batch_loop.run(batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 90, in advance\u001b[0m\n",
      "\u001b[34moutputs = self.manual_loop.run(split_batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 115, in advance\u001b[0m\n",
      "\u001b[34mtraining_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\u001b[0m\n",
      "\u001b[34moutput = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\u001b[0m\n",
      "\u001b[34mreturn self.model.training_step(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 191, in training_step\u001b[0m\n",
      "\u001b[34mloss_dict = self(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 49, in forward\u001b[0m\n",
      "\u001b[34mreturn self.model(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 199, in forward\u001b[0m\n",
      "\u001b[34mflat_res = self.graphable(images.tensors, images.image_sizes_tensor)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 33, in forward\u001b[0m\n",
      "\u001b[34mobjectness, rpn_box_regression = self.head(features)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", line 138, in forward\u001b[0m\n",
      "\u001b[34mlogit = self.cls_logits(t)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/misc.py\", line 42, in forward\u001b[0m\n",
      "\u001b[34mreturn conv.Conv2d_NHWC.forward(self, x)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 223, in forward\u001b[0m\n",
      "\u001b[34mresult = conv2d_NHWC_impl.apply(x, self.weight, None, self.stride,\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 67, in forward\u001b[0m\n",
      "\u001b[34moutput =  NHWC.cudnn_convolution_nhwc(x, padded_w,\u001b[0m\n",
      "\u001b[34mRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 2 does not equal 0 (while checking arguments for cudnn_convolution_nhwc)\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 180, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 175, in main\u001b[0m\n",
      "\u001b[34mtrainer.fit(model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\u001b[0m\n",
      "\u001b[34mself._call_and_handle_interrupt(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\u001b[0m\n",
      "\u001b[34mreturn trainer_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\u001b[0m\n",
      "\u001b[34mresults = self._run(model, ckpt_path=self.ckpt_path)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\u001b[0m\n",
      "\u001b[34mresults = self._run_stage()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\u001b[0m\n",
      "\u001b[34mreturn self._run_train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\u001b[0m\n",
      "\u001b[34mself.fit_loop.run()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\u001b[0m\n",
      "\u001b[34mself._outputs = self.epoch_loop.run(self._data_fetcher)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\u001b[0m\n",
      "\u001b[34mbatch_output = self.batch_loop.run(batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 90, in advance\u001b[0m\n",
      "\u001b[34moutputs = self.manual_loop.run(split_batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 115, in advance\u001b[0m\n",
      "\u001b[34mtraining_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\u001b[0m\n",
      "\u001b[34moutput = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\u001b[0m\n",
      "\u001b[34mreturn self.model.training_step(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 191, in training_step\u001b[0m\n",
      "\u001b[34mloss_dict = self(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 49, in forward\u001b[0m\n",
      "\u001b[34mreturn self.model(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 199, in forward\u001b[0m\n",
      "\u001b[34mflat_res = self.graphable(images.tensors, images.image_sizes_tensor)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 33, in forward\u001b[0m\n",
      "\u001b[34mobjectness, rpn_box_regression = self.head(features)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", line 138, in forward\u001b[0m\n",
      "\u001b[34mlogit = self.cls_logits(t)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/misc.py\", line 42, in forward\u001b[0m\n",
      "\u001b[34mreturn conv.Conv2d_NHWC.forward(self, x)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 223, in forward\u001b[0m\n",
      "\u001b[34mresult = conv2d_NHWC_impl.apply(x, self.weight, None, self.stride,\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 67, in forward\u001b[0m\n",
      "\u001b[34moutput =  NHWC.cudnn_convolution_nhwc(x, padded_w,\u001b[0m\n",
      "\u001b[34mRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution_nhwc)\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 180, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/train_lightning.py\", line 175, in main\u001b[0m\n",
      "\u001b[34mtrainer.fit(model)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\u001b[0m\n",
      "\u001b[34mself._call_and_handle_interrupt(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\u001b[0m\n",
      "\u001b[34mreturn trainer_fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\u001b[0m\n",
      "\u001b[34mresults = self._run(model, ckpt_path=self.ckpt_path)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\u001b[0m\n",
      "\u001b[34mresults = self._run_stage()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\u001b[0m\n",
      "\u001b[34mreturn self._run_train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1353, in _run_train\u001b[0m\n",
      "\u001b[34mself.fit_loop.run()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 266, in advance\u001b[0m\n",
      "\u001b[34mself._outputs = self.epoch_loop.run(self._data_fetcher)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 208, in advance\u001b[0m\n",
      "\u001b[34mbatch_output = self.batch_loop.run(batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 90, in advance\u001b[0m\n",
      "\u001b[34moutputs = self.manual_loop.run(split_batch, batch_idx)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\u001b[0m\n",
      "\u001b[34mself.advance(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/manual_loop.py\", line 115, in advance\u001b[0m\n",
      "\u001b[34mtraining_step_output = self.trainer._call_strategy_hook(\"training_step\", *step_kwargs.values())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\u001b[0m\n",
      "\u001b[34moutput = fn(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py\", line 333, in training_step\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"launch_ddp_lightning.py\", line 78, in <module>\n",
      "    main()\u001b[0m\n",
      "\u001b[34mreturn self.model.training_step(*args, **kwargs)\n",
      "  File \"launch_ddp_lightning.py\", line 73, in main\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=joint_cmd)\u001b[0m\n",
      "\u001b[34msubprocess.CalledProcessError: Command 'export FI_PROVIDER=efa && export FI_EFA_TX_MIN_CREDITS=64 && export NCCL_DEBUG=INFO &&export NCCL_TREE_THRESHOLD=0 &&export NCCL_SOCKET_IFNAME=eth0 &&export FI_EFA_USE_DEVICE_RDMA=1 &&torchrun --nnodes=1 --node_rank=0 --nproc_per_node=8 --master_addr=algo-1 --master_port=55555 /opt/ml/code/train_lightning.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml' returned non-zero exit status 1.\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 191, in training_step\u001b[0m\n",
      "\u001b[34mloss_dict = self(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/ml/code/rcnn_lightning.py\", line 49, in forward\u001b[0m\n",
      "\u001b[34mreturn self.model(images, targets)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 199, in forward\u001b[0m\n",
      "\u001b[34mflat_res = self.graphable(images.tensors, images.image_sizes_tensor)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py\", line 33, in forward\u001b[0m\n",
      "\u001b[34mobjectness, rpn_box_regression = self.head(features)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/modeling/rpn/rpn.py\", line 138, in forward\u001b[0m\n",
      "\u001b[34mlogit = self.cls_logits(t)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1111, in _call_impl\u001b[0m\n",
      "\u001b[34mreturn forward_call(*input, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/misc.py\", line 42, in forward\u001b[0m\n",
      "\u001b[34mreturn conv.Conv2d_NHWC.forward(self, x)\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 223, in forward\u001b[0m\n",
      "\u001b[34mresult = conv2d_NHWC_impl.apply(x, self.weight, None, self.stride,\u001b[0m\n",
      "\u001b[34mFile \"/workspace/maskrcnn/maskrcnn_benchmark/layers/nhwc/conv.py\", line 67, in forward\u001b[0m\n",
      "\u001b[34moutput =  NHWC.cudnn_convolution_nhwc(x, padded_w,\u001b[0m\n",
      "\u001b[34mRuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 3 does not equal 0 (while checking arguments for cudnn_convolution_nhwc)\u001b[0m\n",
      "\u001b[34mEpoch 0: : 0it [00:46, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0: : 0it [00:46, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0: : 0it [00:46, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0: : 0it [00:46, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0: : 0it [00:48, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0: : 0it [00:48, ?it/s]\u001b[0m\n",
      "\u001b[34mEpoch 0: : 0it [00:48, ?it/s]\u001b[0m\n",
      "\u001b[34mtensor(62.0902, device='cuda:0', grad_fn=<AddBackward0>)\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 104 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 105 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 110 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 111 closing signal SIGTERM\u001b[0m\n",
      "\u001b[34mERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 106) of binary: /opt/conda/bin/python\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/bin/torchrun\", line 33, in <module>\u001b[0m\n",
      "\u001b[34msys.exit(load_entry_point('torch==1.12.0a0+bd13bc6', 'console_scripts', 'torchrun')())\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\u001b[0m\n",
      "\u001b[34mreturn f(*args, **kwargs)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 761, in main\u001b[0m\n",
      "\u001b[34mrun(args)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py\", line 752, in run\u001b[0m\n",
      "\u001b[34melastic_launch(\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\u001b[0m\n",
      "\u001b[34mreturn launch_agent(self._config, self._entrypoint, list(args))\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py\", line 245, in launch_agent\u001b[0m\n",
      "\u001b[34mraise ChildFailedError(\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError:\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m/opt/ml/code/train_lightning.py FAILED\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mFailures:\u001b[0m\n",
      "\u001b[34m[1]:\u001b[0m\n",
      "\u001b[34mtime      : 2022-07-28_21:23:04\u001b[0m\n",
      "\u001b[34mhost      : algo-1\u001b[0m\n",
      "\u001b[34mrank      : 3 (local_rank: 3)\u001b[0m\n",
      "\u001b[34mexitcode  : 1 (pid: 107)\u001b[0m\n",
      "\u001b[34merror_file: <N/A>\u001b[0m\n",
      "\u001b[34mtraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[2]:\u001b[0m\n",
      "\u001b[34mtime      : 2022-07-28_21:23:04\u001b[0m\n",
      "\u001b[34mhost      : algo-1\u001b[0m\n",
      "\u001b[34mrank      : 4 (local_rank: 4)\u001b[0m\n",
      "\u001b[34mexitcode  : 1 (pid: 108)\u001b[0m\n",
      "\u001b[34merror_file: <N/A>\u001b[0m\n",
      "\u001b[34mtraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[3]:\u001b[0m\n",
      "\u001b[34mtime      : 2022-07-28_21:23:04\u001b[0m\n",
      "\u001b[34mhost      : algo-1\u001b[0m\n",
      "\u001b[34mrank      : 5 (local_rank: 5)\u001b[0m\n",
      "\u001b[34mexitcode  : 1 (pid: 109)\u001b[0m\n",
      "\u001b[34merror_file: <N/A>\u001b[0m\n",
      "\u001b[34mtraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[34m[0]:\u001b[0m\n",
      "\u001b[34mtime      : 2022-07-28_21:23:04\u001b[0m\n",
      "\u001b[34mhost      : algo-1\u001b[0m\n",
      "\u001b[34mrank      : 2 (local_rank: 2)\u001b[0m\n",
      "\u001b[34mexitcode  : 1 (pid: 106)\u001b[0m\n",
      "\u001b[34merror_file: <N/A>\u001b[0m\n",
      "\u001b[34mtraceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m2022-07-28 21:23:06,361 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-07-28 21:23:06,361 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python launch_ddp_lightning.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\"\u001b[0m\n",
      "\u001b[34m2022-07-28 21:23:06,361 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-07-28 21:23:26 Uploading - Uploading generated training model\n",
      "2022-07-28 21:23:26 Failed - Training job failed\n",
      "ProfilerReport-1659042256: Stopping\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python launch_ddp_lightning.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/estimator.py:1140\u001b[0m, in \u001b[0;36mEstimatorBase.logs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;124;03m\"\"\"Display the logs for Estimator's training job.\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \n\u001b[1;32m   1137\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded based\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    on which instance the log entry is from.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py:3842\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3839\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   3841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 3842\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   3844\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/sagemaker/session.py:3380\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   3375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   3376\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3377\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3378\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3379\u001b[0m     )\n\u001b[0;32m-> 3380\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   3381\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3382\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3383\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3384\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job lilianwa-mlperf-mrcnn-lightning-28-07-2022-21-04-14: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python launch_ddp_lightning.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x_1_node_test.yaml\", exit code: 1"
     ]
    }
   ],
   "source": [
    "estimator.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303ebc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
